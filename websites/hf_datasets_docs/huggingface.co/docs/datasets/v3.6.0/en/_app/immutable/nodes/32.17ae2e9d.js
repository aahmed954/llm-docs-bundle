import{s as Ts,n as Js,o as Us}from"../chunks/scheduler.bdbef820.js";import{S as Cs,i as ks,g as p,s as l,r as c,A as vs,h as r,f as n,c as t,j as xs,u as h,x as m,k as _s,y as Gs,a as e,v as u,d as i,t as b,w as j}from"../chunks/index.c0aea24a.js";import{C as Y}from"../chunks/CodeBlock.e814ab8d.js";import{H as hs,E as Ws}from"../chunks/index.89e522f3.js";function zs(us){let o,R,H,V,d,F,f,is="This guide shows specific methods for processing text datasets. Learn how to:",P,g,bs='<li>Tokenize a dataset with <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.map">map()</a>.</li> <li>Align dataset labels with label ids for NLI datasets.</li>',N,y,js='For a guide on how to process any type of dataset, take a look at the <a class="underline decoration-sky-400 decoration-2 font-semibold" href="./process">general process guide</a>.',Q,M,A,$,os='The <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.map">map()</a> function supports processing batches of examples at once which speeds up tokenization.',X,w,ds='Load a tokenizer from ðŸ¤— <a href="https://huggingface.co/transformers/" rel="nofollow">Transformers</a>:',E,x,S,_,fs='Set the <code>batched</code> parameter to <code>True</code> in the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.map">map()</a> function to apply the tokenizer to batches of examples:',B,T,D,J,gs='The <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.map">map()</a> function converts the returned values to a PyArrow-supported format. But explicitly returning the tensors as NumPy arrays is faster because it is a natively supported PyArrow format. Set <code>return_tensors=&quot;np&quot;</code> when you tokenize your text:',K,U,O,C,ss,k,ys='The <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping">align_labels_with_mapping()</a> function aligns a dataset label id with the label name. Not all ðŸ¤— Transformers models follow the prescribed label mapping of the original dataset, especially for NLI datasets. For example, the <a href="https://huggingface.co/datasets/glue" rel="nofollow">MNLI</a> dataset uses the following label mapping:',as,v,ns,G,Ms="To align the dataset label mapping with the mapping used by a model, create a dictionary of the label name and id to align on:",es,W,ls,z,$s='Pass the dictionary of the label mappings to the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping">align_labels_with_mapping()</a> function, and the column to align on:',ts,q,ps,Z,ws="You can also use this function to assign a custom mapping of labels to ids.",rs,I,ms,L,cs;return d=new hs({props:{title:"Process text data",local:"process-text-data",headingTag:"h1"}}),M=new hs({props:{title:"Map",local:"map",headingTag:"h2"}}),x=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJiZXJ0LWJhc2UtY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`,wrap:!1}}),T=new Y({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQubWFwKGxhbWJkYSUyMGV4YW1wbGVzJTNBJTIwdG9rZW5pemVyKGV4YW1wbGVzJTVCJTIydGV4dCUyMiU1RCklMkMlMjBiYXRjaGVkJTNEVHJ1ZSklMEFkYXRhc2V0JTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>]), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;the rock is destined to be the 21st century\\&#x27;s new &quot; conan &quot; and that he\\&#x27;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#x27;</span>, 
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>, 
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2600</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">16036</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2022</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">7398</span>, <span class="hljs-number">2301</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">2047</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">16608</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">1998</span>, <span class="hljs-number">2008</span>, <span class="hljs-number">2002</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">2183</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2191</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17624</span>, <span class="hljs-number">2130</span>, <span class="hljs-number">3618</span>, <span class="hljs-number">2084</span>, <span class="hljs-number">7779</span>, <span class="hljs-number">29058</span>, <span class="hljs-number">8625</span>, <span class="hljs-number">13327</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">3744</span>, <span class="hljs-number">1011</span>, <span class="hljs-number">18856</span>, <span class="hljs-number">19513</span>, <span class="hljs-number">3158</span>, <span class="hljs-number">5477</span>, <span class="hljs-number">4168</span>, <span class="hljs-number">2030</span>, <span class="hljs-number">7112</span>, <span class="hljs-number">16562</span>, <span class="hljs-number">2140</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`,wrap:!1}}),U=new Y({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQubWFwKGxhbWJkYSUyMGV4YW1wbGVzJTNBJTIwdG9rZW5pemVyKGV4YW1wbGVzJTVCJTIydGV4dCUyMiU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTJDJTIwYmF0Y2hlZCUzRFRydWUp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;np&quot;</span>), batched=<span class="hljs-literal">True</span>)',wrap:!1}}),C=new hs({props:{title:"Align",local:"align",headingTag:"h2"}}),v=new Y({props:{code:"bGFiZWwyaWQlMjAlM0QlMjAlN0IlMjJlbnRhaWxtZW50JTIyJTNBJTIwMCUyQyUyMCUyMm5ldXRyYWwlMjIlM0ElMjAxJTJDJTIwJTIyY29udHJhZGljdGlvbiUyMiUzQSUyMDIlN0Q=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;entailment&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;neutral&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;contradiction&quot;</span>: <span class="hljs-number">2</span>}',wrap:!1}}),W=new Y({props:{code:"bGFiZWwyaWQlMjAlM0QlMjAlN0IlMjJjb250cmFkaWN0aW9uJTIyJTNBJTIwMCUyQyUyMCUyMm5ldXRyYWwlMjIlM0ElMjAxJTJDJTIwJTIyZW50YWlsbWVudCUyMiUzQSUyMDIlN0Q=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;contradiction&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;neutral&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;entailment&quot;</span>: <span class="hljs-number">2</span>}',wrap:!1}}),q=new Y({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBbW5saSUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJueXUtbWxsJTJGZ2x1ZSUyMiUyQyUyMCUyMm1ubGklMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQW1ubGlfYWxpZ25lZCUyMCUzRCUyMG1ubGkuYWxpZ25fbGFiZWxzX3dpdGhfbWFwcGluZyhsYWJlbDJpZCUyQyUyMCUyMmxhYmVsJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>mnli = load_dataset(<span class="hljs-string">&quot;nyu-mll/glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mnli_aligned = mnli.align_labels_with_mapping(label2id, <span class="hljs-string">&quot;label&quot;</span>)`,wrap:!1}}),I=new Ws({props:{source:"https://github.com/huggingface/datasets/blob/main/docs/source/nlp_process.mdx"}}),{c(){o=p("meta"),R=l(),H=p("p"),V=l(),c(d.$$.fragment),F=l(),f=p("p"),f.textContent=is,P=l(),g=p("ul"),g.innerHTML=bs,N=l(),y=p("p"),y.innerHTML=js,Q=l(),c(M.$$.fragment),A=l(),$=p("p"),$.innerHTML=os,X=l(),w=p("p"),w.innerHTML=ds,E=l(),c(x.$$.fragment),S=l(),_=p("p"),_.innerHTML=fs,B=l(),c(T.$$.fragment),D=l(),J=p("p"),J.innerHTML=gs,K=l(),c(U.$$.fragment),O=l(),c(C.$$.fragment),ss=l(),k=p("p"),k.innerHTML=ys,as=l(),c(v.$$.fragment),ns=l(),G=p("p"),G.textContent=Ms,es=l(),c(W.$$.fragment),ls=l(),z=p("p"),z.innerHTML=$s,ts=l(),c(q.$$.fragment),ps=l(),Z=p("p"),Z.textContent=ws,rs=l(),c(I.$$.fragment),ms=l(),L=p("p"),this.h()},l(s){const a=vs("svelte-u9bgzb",document.head);o=r(a,"META",{name:!0,content:!0}),a.forEach(n),R=t(s),H=r(s,"P",{}),xs(H).forEach(n),V=t(s),h(d.$$.fragment,s),F=t(s),f=r(s,"P",{"data-svelte-h":!0}),m(f)!=="svelte-zxifil"&&(f.textContent=is),P=t(s),g=r(s,"UL",{"data-svelte-h":!0}),m(g)!=="svelte-x8e6n3"&&(g.innerHTML=bs),N=t(s),y=r(s,"P",{"data-svelte-h":!0}),m(y)!=="svelte-3s2bzp"&&(y.innerHTML=js),Q=t(s),h(M.$$.fragment,s),A=t(s),$=r(s,"P",{"data-svelte-h":!0}),m($)!=="svelte-1fy52j"&&($.innerHTML=os),X=t(s),w=r(s,"P",{"data-svelte-h":!0}),m(w)!=="svelte-b5bjp1"&&(w.innerHTML=ds),E=t(s),h(x.$$.fragment,s),S=t(s),_=r(s,"P",{"data-svelte-h":!0}),m(_)!=="svelte-j3ogai"&&(_.innerHTML=fs),B=t(s),h(T.$$.fragment,s),D=t(s),J=r(s,"P",{"data-svelte-h":!0}),m(J)!=="svelte-1n2cklk"&&(J.innerHTML=gs),K=t(s),h(U.$$.fragment,s),O=t(s),h(C.$$.fragment,s),ss=t(s),k=r(s,"P",{"data-svelte-h":!0}),m(k)!=="svelte-1qq06pm"&&(k.innerHTML=ys),as=t(s),h(v.$$.fragment,s),ns=t(s),G=r(s,"P",{"data-svelte-h":!0}),m(G)!=="svelte-tn6t6n"&&(G.textContent=Ms),es=t(s),h(W.$$.fragment,s),ls=t(s),z=r(s,"P",{"data-svelte-h":!0}),m(z)!=="svelte-zjvni6"&&(z.innerHTML=$s),ts=t(s),h(q.$$.fragment,s),ps=t(s),Z=r(s,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-18y4a1w"&&(Z.textContent=ws),rs=t(s),h(I.$$.fragment,s),ms=t(s),L=r(s,"P",{}),xs(L).forEach(n),this.h()},h(){_s(o,"name","hf:doc:metadata"),_s(o,"content",qs)},m(s,a){Gs(document.head,o),e(s,R,a),e(s,H,a),e(s,V,a),u(d,s,a),e(s,F,a),e(s,f,a),e(s,P,a),e(s,g,a),e(s,N,a),e(s,y,a),e(s,Q,a),u(M,s,a),e(s,A,a),e(s,$,a),e(s,X,a),e(s,w,a),e(s,E,a),u(x,s,a),e(s,S,a),e(s,_,a),e(s,B,a),u(T,s,a),e(s,D,a),e(s,J,a),e(s,K,a),u(U,s,a),e(s,O,a),u(C,s,a),e(s,ss,a),e(s,k,a),e(s,as,a),u(v,s,a),e(s,ns,a),e(s,G,a),e(s,es,a),u(W,s,a),e(s,ls,a),e(s,z,a),e(s,ts,a),u(q,s,a),e(s,ps,a),e(s,Z,a),e(s,rs,a),u(I,s,a),e(s,ms,a),e(s,L,a),cs=!0},p:Js,i(s){cs||(i(d.$$.fragment,s),i(M.$$.fragment,s),i(x.$$.fragment,s),i(T.$$.fragment,s),i(U.$$.fragment,s),i(C.$$.fragment,s),i(v.$$.fragment,s),i(W.$$.fragment,s),i(q.$$.fragment,s),i(I.$$.fragment,s),cs=!0)},o(s){b(d.$$.fragment,s),b(M.$$.fragment,s),b(x.$$.fragment,s),b(T.$$.fragment,s),b(U.$$.fragment,s),b(C.$$.fragment,s),b(v.$$.fragment,s),b(W.$$.fragment,s),b(q.$$.fragment,s),b(I.$$.fragment,s),cs=!1},d(s){s&&(n(R),n(H),n(V),n(F),n(f),n(P),n(g),n(N),n(y),n(Q),n(A),n($),n(X),n(w),n(E),n(S),n(_),n(B),n(D),n(J),n(K),n(O),n(ss),n(k),n(as),n(ns),n(G),n(es),n(ls),n(z),n(ts),n(ps),n(Z),n(rs),n(ms),n(L)),n(o),j(d,s),j(M,s),j(x,s),j(T,s),j(U,s),j(C,s),j(v,s),j(W,s),j(q,s),j(I,s)}}}const qs='{"title":"Process text data","local":"process-text-data","sections":[{"title":"Map","local":"map","sections":[],"depth":2},{"title":"Align","local":"align","sections":[],"depth":2}],"depth":1}';function Zs(us){return Us(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rs extends Cs{constructor(o){super(),ks(this,o,Zs,zs,Ts,{})}}export{Rs as component};
