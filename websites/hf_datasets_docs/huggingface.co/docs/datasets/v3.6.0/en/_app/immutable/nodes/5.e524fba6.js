import{s as yt,o as Tt,n as $t}from"../chunks/scheduler.bdbef820.js";import{S as kt,i as Lt,g as l,s as i,r as u,A as Ct,h as n,f as a,c as d,j as bt,u as h,x as r,k as ze,y as xt,a as s,v as p,d as m,t as g,w as _}from"../chunks/index.c0aea24a.js";import{T as wt}from"../chunks/Tip.31005f7d.js";import{H as ae,E as Mt}from"../chunks/index.89e522f3.js";function Ht(se){let o,c='Read the <a href="./upload_dataset">Share</a> section to learn more about how to share a dataset. This section also provides a step-by-step guide on how to write your own dataset loading script!';return{c(){o=l("p"),o.innerHTML=c},l(f){o=n(f,"P",{"data-svelte-h":!0}),r(o)!=="svelte-ktnbf3"&&(o.innerHTML=c)},m(f,v){s(f,o,v)},p:$t,d(f){f&&a(o)}}}function Bt(se){let o,c='If it is your own dataset, you’ll need to recompute the information above and update the <code>README.md</code> file in your dataset repository. Take a look at this <a href="dataset_script#optional-generate-dataset-metadata">section</a> to learn how to generate and update this metadata.';return{c(){o=l("p"),o.innerHTML=c},l(f){o=n(f,"P",{"data-svelte-h":!0}),r(o)!=="svelte-1njk00f"&&(o.innerHTML=c)},m(f,v){s(f,o,v)},p:$t,d(f){f&&a(o)}}}function Dt(se){let o,c,f,v,T,de,k,Ne='Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With 🤗 Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use.',le,L,ne,C,Oe="Let’s begin with a basic Explain Like I’m Five.",re,x,We="A dataset is a directory that contains:",oe,M,Je="<li>Some data files in generic formats (JSON, CSV, Parquet, text, etc.)</li> <li>A dataset card named <code>README.md</code> that contains documentation about the dataset as well as a YAML header to define the datasets tags and configurations</li> <li>An optional dataset script if it requires some code to read the data files. This is sometimes used to load files of specific formats and structures.</li>",fe,H,Ve=`The <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> function fetches the requested dataset locally or from the Hugging Face Hub.
The Hub is a central repository where all the Hugging Face datasets and models are stored.`,ce,B,Ze=`If the dataset only contains data files, then <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> automatically infers how to load the data files from their extensions (json, csv, parquet, txt, etc.).
Under the hood, 🤗 Datasets will use an appropriate <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a> based on the data files format. There exist one builder per data file format in 🤗 Datasets:`,ue,D,Ye='<li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.text.Text">datasets.packaged_modules.text.Text</a> for text</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.csv.Csv">datasets.packaged_modules.csv.Csv</a> for CSV and TSV</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.json.Json">datasets.packaged_modules.json.Json</a> for JSON and JSONL</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.parquet.Parquet">datasets.packaged_modules.parquet.Parquet</a> for Parquet</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.arrow.Arrow">datasets.packaged_modules.arrow.Arrow</a> for Arrow (streaming file format)</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.sql.Sql">datasets.packaged_modules.sql.Sql</a> for SQL databases</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.imagefolder.ImageFolder">datasets.packaged_modules.imagefolder.ImageFolder</a> for image folders</li> <li><a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.packaged_modules.audiofolder.AudioFolder">datasets.packaged_modules.audiofolder.AudioFolder</a> for audio folders</li>',he,P,Qe=`If the dataset has a dataset script, then it downloads and imports it from the Hugging Face Hub.
Code in the dataset script defines a custom <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a> the dataset information (description, features, URL to the original files, etc.), and tells 🤗 Datasets how to generate and display examples from it.`,pe,b,me,S,Ke=`🤗 Datasets downloads the dataset files from the original URL, generates the dataset and caches it in an Arrow table on your drive.
If you’ve downloaded the dataset before, then 🤗 Datasets will reload it from the cache to save you the trouble of downloading it again.`,ge,A,Xe="Now that you have a high-level understanding about how datasets are built, let’s take a closer look at the nuts and bolts of how all this works.",_e,I,ve,q,et='When you load a dataset for the first time, 🤗 Datasets takes the raw data file and builds it into a table of rows and typed columns. There are two main classes responsible for building a dataset: <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> and <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a>.',be,w,tt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/builderconfig.png"/>',we,E,$e,F,at='<a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> is the configuration class of <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a>. The <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> contains the following basic attributes about a dataset:',ye,R,st="<thead><tr><th>Attribute</th> <th>Description</th></tr></thead> <tbody><tr><td><code>name</code></td> <td>Short name of the dataset.</td></tr> <tr><td><code>version</code></td> <td>Dataset version identifier.</td></tr> <tr><td><code>data_dir</code></td> <td>Stores the path to a local folder containing the data files.</td></tr> <tr><td><code>data_files</code></td> <td>Stores paths to local data files.</td></tr> <tr><td><code>description</code></td> <td>Description of the dataset.</td></tr></tbody>",Te,U,it='If you want to add additional attributes to your dataset such as the class labels, you can subclass the base <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> class. There are two ways to populate the attributes of a <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> class or subclass:',ke,j,dt='<li><p>Provide a list of predefined <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> class (or subclass) instances in the datasets <code>DatasetBuilder.BUILDER_CONFIGS()</code> attribute.</p></li> <li><p>When you call <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a>, any keyword arguments that are not specific to the method will be used to set the associated attributes of the <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> class. This will override the predefined attributes if a specific configuration was selected.</p></li>',Le,G,lt='You can also set the <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">DatasetBuilder.BUILDER_CONFIG_CLASS</a> to any custom subclass of <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a>.',Ce,z,xe,N,nt='<a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a> accesses all the attributes inside <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a> to build the actual dataset.',Me,$,rt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/datasetbuilder.png"/>',He,O,ot='There are three main methods in <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a>:',Be,W,ft='<li><p><code>DatasetBuilder._info()</code> is in charge of defining the dataset attributes. When you call <code>dataset.info</code>, 🤗 Datasets returns the information stored here. Likewise, the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Features">Features</a> are also specified here. Remember, the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Features">Features</a> are like the skeleton of the dataset. It provides the names and types of each column.</p></li> <li><p><code>DatasetBuilder._split_generator</code> downloads or retrieves the requested data files, organizes them into splits, and defines specific arguments for the generation process. This method has a <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a> that downloads files or fetches them from your local filesystem. Within the <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, there is a <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract">DownloadManager.download_and_extract()</a> method that accepts a dictionary of URLs to the original data files, and downloads the requested files. Accepted inputs include: a single URL or path, or a list/dictionary of URLs or paths. Any compressed file types like TAR, GZIP and ZIP archives will be automatically extracted.</p> <p>Once the files are downloaded, <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.SplitGenerator">SplitGenerator</a> organizes them into splits. The <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.SplitGenerator">SplitGenerator</a> contains the name of the split, and any keyword arguments that are provided to the <code>DatasetBuilder._generate_examples</code> method. The keyword arguments can be specific to each split, and typically comprise at least the local path to the data files for each split.</p></li> <li><p><code>DatasetBuilder._generate_examples</code> reads and parses the data files for a split. Then it yields dataset examples according to the format specified in the <code>features</code> from <code>DatasetBuilder._info()</code>. The input of <code>DatasetBuilder._generate_examples</code> is actually the <code>filepath</code> provided in the keyword arguments of the last method.</p> <p>The dataset is generated with a Python generator, which doesn’t load all the data in memory. As a result, the generator can handle large datasets. However, before the generated samples are flushed to the dataset file on disk, they are stored in an <code>ArrowWriter</code> buffer. This means the generated samples are written by batch. If your dataset samples consumes a lot of memory (images or videos), then make sure to specify a low value for the <code>DEFAULT_WRITER_BATCH_SIZE</code> attribute in <a href="/docs/datasets/v3.6.0/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a>. We recommend not exceeding a size of 200 MB.</p></li>',De,J,Pe,V,ct='To ensure a dataset is complete, <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> will perform a series of tests on the downloaded files to make sure everything is there. This way, you don’t encounter any surprises when your requested dataset doesn’t get generated as expected. <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> verifies:',Se,Z,ut="<li>The number of splits in the generated <code>DatasetDict</code>.</li> <li>The number of samples in each split of the generated <code>DatasetDict</code>.</li> <li>The list of downloaded files.</li> <li>The SHA256 checksums of the downloaded files (disabled by default).</li>",Ae,Y,ht="If the dataset doesn’t pass the verifications, it is likely that the original host of the dataset made some changes in the data files.",Ie,y,qe,Q,pt=`In this case, an error is raised to alert that the dataset has changed.
To ignore the error, one needs to specify <code>verification_mode=&quot;no_checks&quot;</code> in <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a>.
Anytime you see a verification error, feel free to open a discussion or pull request in the corresponding dataset “Community” tab, so that the integrity checks for that dataset are updated.`,Ee,K,Fe,X,mt='The dataset repositories on the Hub are scanned for malware, see more information <a href="https://huggingface.co/docs/hub/security#malware-scanning" rel="nofollow">here</a>.',Re,ee,gt=`Moreover the datasets without a namespace (originally contributed on our GitHub repository) have all been reviewed by our maintainers.
The code of these datasets is considered <strong>safe</strong>.
It concerns datasets that are not under a namespace, e.g. “rajpurkar/squad” or “nyu-mll/glue”, unlike the other datasets that are named “username/dataset_name” or “org/dataset_name”.`,Ue,te,je,ie,Ge;return T=new ae({props:{title:"Build and load",local:"build-and-load",headingTag:"h1"}}),L=new ae({props:{title:"ELI5: load_dataset",local:"eli5-loaddataset",headingTag:"h2"}}),b=new wt({props:{$$slots:{default:[Ht]},$$scope:{ctx:se}}}),I=new ae({props:{title:"Building a dataset",local:"building-a-dataset",headingTag:"h2"}}),E=new ae({props:{title:"BuilderConfig",local:"datasets-builderconfig",headingTag:"h3"}}),z=new ae({props:{title:"DatasetBuilder",local:"datasets-datasetbuilder",headingTag:"h3"}}),J=new ae({props:{title:"Maintaining integrity",local:"maintaining-integrity",headingTag:"h2"}}),y=new wt({props:{$$slots:{default:[Bt]},$$scope:{ctx:se}}}),K=new ae({props:{title:"Security",local:"security",headingTag:"h2"}}),te=new Mt({props:{source:"https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_load.mdx"}}),{c(){o=l("meta"),c=i(),f=l("p"),v=i(),u(T.$$.fragment),de=i(),k=l("p"),k.innerHTML=Ne,le=i(),u(L.$$.fragment),ne=i(),C=l("p"),C.textContent=Oe,re=i(),x=l("p"),x.textContent=We,oe=i(),M=l("ul"),M.innerHTML=Je,fe=i(),H=l("p"),H.innerHTML=Ve,ce=i(),B=l("p"),B.innerHTML=Ze,ue=i(),D=l("ul"),D.innerHTML=Ye,he=i(),P=l("p"),P.innerHTML=Qe,pe=i(),u(b.$$.fragment),me=i(),S=l("p"),S.textContent=Ke,ge=i(),A=l("p"),A.textContent=Xe,_e=i(),u(I.$$.fragment),ve=i(),q=l("p"),q.innerHTML=et,be=i(),w=l("div"),w.innerHTML=tt,we=i(),u(E.$$.fragment),$e=i(),F=l("p"),F.innerHTML=at,ye=i(),R=l("table"),R.innerHTML=st,Te=i(),U=l("p"),U.innerHTML=it,ke=i(),j=l("ul"),j.innerHTML=dt,Le=i(),G=l("p"),G.innerHTML=lt,Ce=i(),u(z.$$.fragment),xe=i(),N=l("p"),N.innerHTML=nt,Me=i(),$=l("div"),$.innerHTML=rt,He=i(),O=l("p"),O.innerHTML=ot,Be=i(),W=l("ol"),W.innerHTML=ft,De=i(),u(J.$$.fragment),Pe=i(),V=l("p"),V.innerHTML=ct,Se=i(),Z=l("ul"),Z.innerHTML=ut,Ae=i(),Y=l("p"),Y.textContent=ht,Ie=i(),u(y.$$.fragment),qe=i(),Q=l("p"),Q.innerHTML=pt,Ee=i(),u(K.$$.fragment),Fe=i(),X=l("p"),X.innerHTML=mt,Re=i(),ee=l("p"),ee.innerHTML=gt,Ue=i(),u(te.$$.fragment),je=i(),ie=l("p"),this.h()},l(e){const t=Ct("svelte-u9bgzb",document.head);o=n(t,"META",{name:!0,content:!0}),t.forEach(a),c=d(e),f=n(e,"P",{}),bt(f).forEach(a),v=d(e),h(T.$$.fragment,e),de=d(e),k=n(e,"P",{"data-svelte-h":!0}),r(k)!=="svelte-clg19e"&&(k.innerHTML=Ne),le=d(e),h(L.$$.fragment,e),ne=d(e),C=n(e,"P",{"data-svelte-h":!0}),r(C)!=="svelte-1et77y"&&(C.textContent=Oe),re=d(e),x=n(e,"P",{"data-svelte-h":!0}),r(x)!=="svelte-gersvb"&&(x.textContent=We),oe=d(e),M=n(e,"UL",{"data-svelte-h":!0}),r(M)!=="svelte-mtskvn"&&(M.innerHTML=Je),fe=d(e),H=n(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-g99w1e"&&(H.innerHTML=Ve),ce=d(e),B=n(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-u7jit"&&(B.innerHTML=Ze),ue=d(e),D=n(e,"UL",{"data-svelte-h":!0}),r(D)!=="svelte-18rw3ue"&&(D.innerHTML=Ye),he=d(e),P=n(e,"P",{"data-svelte-h":!0}),r(P)!=="svelte-683kfy"&&(P.innerHTML=Qe),pe=d(e),h(b.$$.fragment,e),me=d(e),S=n(e,"P",{"data-svelte-h":!0}),r(S)!=="svelte-iduykv"&&(S.textContent=Ke),ge=d(e),A=n(e,"P",{"data-svelte-h":!0}),r(A)!=="svelte-1jl5e53"&&(A.textContent=Xe),_e=d(e),h(I.$$.fragment,e),ve=d(e),q=n(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-z37tlo"&&(q.innerHTML=et),be=d(e),w=n(e,"DIV",{class:!0,"data-svelte-h":!0}),r(w)!=="svelte-1adyav4"&&(w.innerHTML=tt),we=d(e),h(E.$$.fragment,e),$e=d(e),F=n(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-1fxlg7a"&&(F.innerHTML=at),ye=d(e),R=n(e,"TABLE",{"data-svelte-h":!0}),r(R)!=="svelte-fm8v6i"&&(R.innerHTML=st),Te=d(e),U=n(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-32tsve"&&(U.innerHTML=it),ke=d(e),j=n(e,"UL",{"data-svelte-h":!0}),r(j)!=="svelte-118rfnz"&&(j.innerHTML=dt),Le=d(e),G=n(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-yct187"&&(G.innerHTML=lt),Ce=d(e),h(z.$$.fragment,e),xe=d(e),N=n(e,"P",{"data-svelte-h":!0}),r(N)!=="svelte-czqgup"&&(N.innerHTML=nt),Me=d(e),$=n(e,"DIV",{class:!0,"data-svelte-h":!0}),r($)!=="svelte-11mt2nw"&&($.innerHTML=rt),He=d(e),O=n(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-8uq0sb"&&(O.innerHTML=ot),Be=d(e),W=n(e,"OL",{"data-svelte-h":!0}),r(W)!=="svelte-bndjom"&&(W.innerHTML=ft),De=d(e),h(J.$$.fragment,e),Pe=d(e),V=n(e,"P",{"data-svelte-h":!0}),r(V)!=="svelte-mvkz5h"&&(V.innerHTML=ct),Se=d(e),Z=n(e,"UL",{"data-svelte-h":!0}),r(Z)!=="svelte-1gxjcyf"&&(Z.innerHTML=ut),Ae=d(e),Y=n(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-1m6pq65"&&(Y.textContent=ht),Ie=d(e),h(y.$$.fragment,e),qe=d(e),Q=n(e,"P",{"data-svelte-h":!0}),r(Q)!=="svelte-p5wm09"&&(Q.innerHTML=pt),Ee=d(e),h(K.$$.fragment,e),Fe=d(e),X=n(e,"P",{"data-svelte-h":!0}),r(X)!=="svelte-p38c5d"&&(X.innerHTML=mt),Re=d(e),ee=n(e,"P",{"data-svelte-h":!0}),r(ee)!=="svelte-1nibobs"&&(ee.innerHTML=gt),Ue=d(e),h(te.$$.fragment,e),je=d(e),ie=n(e,"P",{}),bt(ie).forEach(a),this.h()},h(){ze(o,"name","hf:doc:metadata"),ze(o,"content",Pt),ze(w,"class","flex justify-center"),ze($,"class","flex justify-center")},m(e,t){xt(document.head,o),s(e,c,t),s(e,f,t),s(e,v,t),p(T,e,t),s(e,de,t),s(e,k,t),s(e,le,t),p(L,e,t),s(e,ne,t),s(e,C,t),s(e,re,t),s(e,x,t),s(e,oe,t),s(e,M,t),s(e,fe,t),s(e,H,t),s(e,ce,t),s(e,B,t),s(e,ue,t),s(e,D,t),s(e,he,t),s(e,P,t),s(e,pe,t),p(b,e,t),s(e,me,t),s(e,S,t),s(e,ge,t),s(e,A,t),s(e,_e,t),p(I,e,t),s(e,ve,t),s(e,q,t),s(e,be,t),s(e,w,t),s(e,we,t),p(E,e,t),s(e,$e,t),s(e,F,t),s(e,ye,t),s(e,R,t),s(e,Te,t),s(e,U,t),s(e,ke,t),s(e,j,t),s(e,Le,t),s(e,G,t),s(e,Ce,t),p(z,e,t),s(e,xe,t),s(e,N,t),s(e,Me,t),s(e,$,t),s(e,He,t),s(e,O,t),s(e,Be,t),s(e,W,t),s(e,De,t),p(J,e,t),s(e,Pe,t),s(e,V,t),s(e,Se,t),s(e,Z,t),s(e,Ae,t),s(e,Y,t),s(e,Ie,t),p(y,e,t),s(e,qe,t),s(e,Q,t),s(e,Ee,t),p(K,e,t),s(e,Fe,t),s(e,X,t),s(e,Re,t),s(e,ee,t),s(e,Ue,t),p(te,e,t),s(e,je,t),s(e,ie,t),Ge=!0},p(e,[t]){const _t={};t&2&&(_t.$$scope={dirty:t,ctx:e}),b.$set(_t);const vt={};t&2&&(vt.$$scope={dirty:t,ctx:e}),y.$set(vt)},i(e){Ge||(m(T.$$.fragment,e),m(L.$$.fragment,e),m(b.$$.fragment,e),m(I.$$.fragment,e),m(E.$$.fragment,e),m(z.$$.fragment,e),m(J.$$.fragment,e),m(y.$$.fragment,e),m(K.$$.fragment,e),m(te.$$.fragment,e),Ge=!0)},o(e){g(T.$$.fragment,e),g(L.$$.fragment,e),g(b.$$.fragment,e),g(I.$$.fragment,e),g(E.$$.fragment,e),g(z.$$.fragment,e),g(J.$$.fragment,e),g(y.$$.fragment,e),g(K.$$.fragment,e),g(te.$$.fragment,e),Ge=!1},d(e){e&&(a(c),a(f),a(v),a(de),a(k),a(le),a(ne),a(C),a(re),a(x),a(oe),a(M),a(fe),a(H),a(ce),a(B),a(ue),a(D),a(he),a(P),a(pe),a(me),a(S),a(ge),a(A),a(_e),a(ve),a(q),a(be),a(w),a(we),a($e),a(F),a(ye),a(R),a(Te),a(U),a(ke),a(j),a(Le),a(G),a(Ce),a(xe),a(N),a(Me),a($),a(He),a(O),a(Be),a(W),a(De),a(Pe),a(V),a(Se),a(Z),a(Ae),a(Y),a(Ie),a(qe),a(Q),a(Ee),a(Fe),a(X),a(Re),a(ee),a(Ue),a(je),a(ie)),a(o),_(T,e),_(L,e),_(b,e),_(I,e),_(E,e),_(z,e),_(J,e),_(y,e),_(K,e),_(te,e)}}}const Pt='{"title":"Build and load","local":"build-and-load","sections":[{"title":"ELI5: load_dataset","local":"eli5-loaddataset","sections":[],"depth":2},{"title":"Building a dataset","local":"building-a-dataset","sections":[{"title":"BuilderConfig","local":"datasets-builderconfig","sections":[],"depth":3},{"title":"DatasetBuilder","local":"datasets-datasetbuilder","sections":[],"depth":3}],"depth":2},{"title":"Maintaining integrity","local":"maintaining-integrity","sections":[],"depth":2},{"title":"Security","local":"security","sections":[],"depth":2}],"depth":1}';function St(se){return Tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ft extends kt{constructor(o){super(),Lt(this,o,St,Dt,yt,{})}}export{Ft as component};
