import{s as vl,o as Cl,n as Y}from"../chunks/scheduler.bdbef820.js";import{S as Xl,i as Rl,g as r,s as l,r as d,A as Yl,h as c,f as e,c as n,j as _t,u as o,x as h,k as $t,y as Fl,a as t,v as m,d as f,t as u,w as g}from"../chunks/index.c0aea24a.js";import{T as xa}from"../chunks/Tip.31005f7d.js";import{C as M}from"../chunks/CodeBlock.e814ab8d.js";import{F as Wl,M as Vl}from"../chunks/Markdown.1f17db59.js";import{H as J,E as Bl}from"../chunks/index.89e522f3.js";function zl(y){let p,j=`An <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> is useful for iterative jobs like training a model.
You shouldn‚Äôt use a <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples.
You can find more details in the <a href="./about_mapstyle_vs_iterable">Dataset vs. IterableDataset guide</a>.`;return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-13sij94"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function Nl(y){let p,j='<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle">IterableDataset.shuffle()</a> will also shuffle the order of the shards if the dataset is sharded into multiple files.';return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-adynww"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function Ql(y){let p,j="<code>take</code> and <code>skip</code> prevent future calls to <code>shuffle</code> because they lock in the order of the shards. You should <code>shuffle</code> your dataset before splitting it.";return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-kg825l"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function Hl(y){let p,j="Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type <code>Value(&#39;int32&#39;)</code> to <code>Value(&#39;bool&#39;)</code> if the original column only contains ones and zeros.";return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-19jja92"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function Dl(y){let p,j='See other examples of batch processing in the <a href="./process#batch-processing">batched map processing</a> documentation. They work the same for iterable datasets.';return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-csithm"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function El(y){let p,j='There is also a ‚ÄúBatch Processing‚Äù option when using the <code>map</code> function to apply a function to batches of data, which is discussed in the <a href="#map">Map section</a> above. The <code>batch</code> method described here is different and provides a more direct way to create batches from your dataset.';return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-f0d9pr"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function Sl(y){let p,j,i,b="Lastly, create a simple training loop and start training:",T,x,$;return p=new M({props:{code:"c2VlZCUyQyUyMGJ1ZmZlcl9zaXplJTIwJTNEJTIwNDIlMkMlMjAxMF8wMDAlMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zaHVmZmxlKHNlZWQlMkMlMjBidWZmZXJfc2l6ZSUzRGJ1ZmZlcl9zaXplKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>seed, buffer_size = <span class="hljs-number">42</span>, <span class="hljs-number">10_000</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(seed, buffer_size=buffer_size)`,wrap:!1}}),x=new M({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdG9yY2gudXRpbHMuZGF0YSUyMGltcG9ydCUyMERhdGFMb2FkZXIlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yTWFza2VkTE0lMkMlMjBEYXRhQ29sbGF0b3JGb3JMYW5ndWFnZU1vZGVsaW5nJTBBZnJvbSUyMHRxZG0lMjBpbXBvcnQlMjB0cWRtJTBBZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQud2l0aF9mb3JtYXQoJTIydG9yY2glMjIpJTBBZGF0YWxvYWRlciUyMCUzRCUyMERhdGFMb2FkZXIoZGF0YXNldCUyQyUyMGNvbGxhdGVfZm4lM0REYXRhQ29sbGF0b3JGb3JMYW5ndWFnZU1vZGVsaW5nKHRva2VuaXplcikpJTBBZGV2aWNlJTIwJTNEJTIwJ2N1ZGEnJTIwaWYlMjB0b3JjaC5jdWRhLmlzX2F2YWlsYWJsZSgpJTIwZWxzZSUyMCdjcHUnJTIwJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIpJTBBbW9kZWwudHJhaW4oKS50byhkZXZpY2UpJTBBb3B0aW1pemVyJTIwJTNEJTIwdG9yY2gub3B0aW0uQWRhbVcocGFyYW1zJTNEbW9kZWwucGFyYW1ldGVycygpJTJDJTIwbHIlM0QxZS01KSUwQWZvciUyMGVwb2NoJTIwaW4lMjByYW5nZSgzKSUzQSUwQSUyMCUyMCUyMCUyMGRhdGFzZXQuc2V0X2Vwb2NoKGVwb2NoKSUwQSUyMCUyMCUyMCUyMGZvciUyMGklMkMlMjBiYXRjaCUyMGluJTIwZW51bWVyYXRlKHRxZG0oZGF0YWxvYWRlciUyQyUyMHRvdGFsJTNENSkpJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaWYlMjBpJTIwJTNEJTNEJTIwNSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGJyZWFrJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYmF0Y2glMjAlM0QlMjAlN0JrJTNBJTIwdi50byhkZXZpY2UpJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMGJhdGNoLml0ZW1zKCklN0QlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKipiYXRjaCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb3NzJTIwJTNEJTIwb3V0cHV0cyU1QjAlNUQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb3NzLmJhY2t3YXJkKCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvcHRpbWl6ZXIuc3RlcCgpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb3B0aW1pemVyLnplcm9fZ3JhZCgpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaWYlMjBpJTIwJTI1JTIwMTAlMjAlM0QlM0QlMjAwJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcHJpbnQoZiUyMmxvc3MlM0ElMjAlN0Jsb3NzJTdEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`,wrap:!1}}),{c(){d(p.$$.fragment),j=l(),i=r("p"),i.textContent=b,T=l(),d(x.$$.fragment)},l(w){o(p.$$.fragment,w),j=n(w),i=c(w,"P",{"data-svelte-h":!0}),h(i)!=="svelte-uyzjf"&&(i.textContent=b),T=n(w),o(x.$$.fragment,w)},m(w,_){m(p,w,_),t(w,j,_),t(w,i,_),t(w,T,_),m(x,w,_),$=!0},p:Y,i(w){$||(f(p.$$.fragment,w),f(x.$$.fragment,w),$=!0)},o(w){u(p.$$.fragment,w),u(x.$$.fragment,w),$=!1},d(w){w&&(e(j),e(i),e(T)),g(p,w),g(x,w)}}}function Ll(y){let p,j;return p=new Vl({props:{$$slots:{default:[Sl]},$$scope:{ctx:y}}}),{c(){d(p.$$.fragment)},l(i){o(p.$$.fragment,i)},m(i,b){m(p,i,b),j=!0},p(i,b){const T={};b&2&&(T.$$scope={dirty:b,ctx:i}),p.$set(T)},i(i){j||(f(p.$$.fragment,i),j=!0)},o(i){u(p.$$.fragment,i),j=!1},d(i){g(p,i)}}}function Al(y){let p,j="Resuming returns exactly where the checkpoint was saved except if <code>.shuffle()</code> is used: examples from shuffle buffers are lost when resuming and the buffers are refilled with new data.";return{c(){p=r("p"),p.innerHTML=j},l(i){p=c(i,"P",{"data-svelte-h":!0}),h(p)!=="svelte-1aaqobe"&&(p.innerHTML=j)},m(i,b){t(i,p,b)},p:Y,d(i){i&&e(p)}}}function ql(y){let p,j,i,b,T,x,$,w=`Dataset streaming lets you work with a dataset without downloading it.
The data is streamed as you iterate over the dataset.
This is especially helpful when:`,_,F,kt="<li>You don‚Äôt want to wait for an extremely large dataset to download.</li> <li>The dataset size exceeds the amount of available disk space on your computer.</li> <li>You want to quickly explore just a few samples of a dataset.</li>",ka,k,Zt='<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif"/> <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif"/>',Za,W,Ut='For example, the English split of the <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb" rel="nofollow">HuggingFaceFW/fineweb</a> dataset is 45 terabytes, but you can use it instantly with streaming. Stream a dataset by setting <code>streaming=True</code> in <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> as shown below:',Ua,V,Ia,B,It=`Dataset streaming also lets you work with a dataset made of local files without doing any conversion.
In this case, the data is streamed from the local files as you iterate over the dataset.
This is especially helpful when:`,Ga,z,Gt="<li>You don‚Äôt want to wait for an extremely large local dataset to be converted to Arrow.</li> <li>The converted files size would exceed the amount of available disk space on your computer.</li> <li>You want to quickly explore just a few samples of a dataset.</li>",va,N,vt='For example, you can stream a local dataset of hundreds of compressed JSONL files like <a href="https://huggingface.co/datasets/oscar-corpus/OSCAR-2201" rel="nofollow">oscar-corpus/OSCAR-2201</a> to use it instantly:',Ca,Q,Xa,H,Ct=`Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset">Dataset</a> object), known as an <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a>.
This special type of dataset has its own set of processing methods shown below.`,Ra,Z,Ya,D,Fa,E,Xt='If you have an existing <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset">Dataset</a> object, you can convert it to an <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> with the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset">to_iterable_dataset()</a> function. This is actually faster than setting the <code>streaming=True</code> argument in <a href="/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset">load_dataset()</a> because the data is streamed from local files.',Wa,S,Va,L,Rt='The <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset">to_iterable_dataset()</a> function supports sharding when the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> is instantiated. This is useful when working with big datasets, and you‚Äôd like to shuffle the dataset or to enable fast parallel loading with a PyTorch DataLoader.',Ba,A,za,q,Na,K,Yt='Like a regular <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset">Dataset</a> object, you can also shuffle a <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> with <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle">IterableDataset.shuffle()</a>.',Qa,P,Ft='The <code>buffer_size</code> argument controls the size of the buffer to randomly sample examples from. Let‚Äôs say your dataset has one million examples, and you set the <code>buffer_size</code> to ten thousand. <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle">IterableDataset.shuffle()</a> will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000.',Ha,O,Da,U,Ea,ss,Sa,as,Wt="Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use <code>IterableDataset.set_epoch()</code> in between epochs to tell the dataset what epoch you‚Äôre on.",La,es,Vt="Your seed effectively becomes: <code>initial seed + current epoch</code>.",Aa,ts,qa,ls,Ka,ns,Bt="You can split your dataset one of two ways:",Pa,ps,zt='<li><a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.take">IterableDataset.take()</a> returns the first <code>n</code> examples in a dataset:</li>',Oa,is,se,rs,Nt='<li><a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.skip">IterableDataset.skip()</a> omits the first <code>n</code> examples in a dataset and returns the remaining examples:</li>',ae,cs,ee,I,te,$a,le,hs,ne,ds,Qt='ü§ó Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the <code>num_shards</code> parameter in <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.shard">shard()</a> to determine the number of shards to split the dataset into. You‚Äôll also need to provide the shard you want to return with the <code>index</code> parameter.',pe,os,Ht='For example, the <a href="https://huggingface.co/datasets/amazon_polarity" rel="nofollow">amazon_polarity</a> dataset has 4 shards (in this case they are 4 Parquet files):',ie,ms,re,fs,Dt="After sharding the dataset into two chunks, the first one will only have 2 shards:",ce,us,he,gs,Et='If your dataset has <code>dataset.num_shards==1</code>, you should chunk it using <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.skip">IterableDataset.skip()</a> and <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.take">IterableDataset.take()</a> instead.',de,js,oe,Ms,St='<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.interleave_datasets">interleave_datasets()</a> can combine an <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> with other datasets. The combined dataset returns alternating examples from each of the original datasets.',me,bs,fe,ws,Lt="Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the <code>probabilities</code> argument with your desired sampling probabilities:",ue,ys,ge,Js,At="Around 80% of the final dataset is made of the <code>en_dataset</code>, and 20% of the <code>fr_dataset</code>.",je,Ts,qt=`You can also specify the <code>stopping_strategy</code>. The default strategy, <code>first_exhausted</code>, is a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples.
You can specify <code>stopping_strategy=all_exhausted</code> to execute an oversampling strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached.
Note that if no sampling probabilities are specified, the new dataset will have <code>max_length_datasets*nb_dataset samples</code>.`,Me,xs,be,$s,Kt="The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns and changing columns to a new set of features.",we,_s,ye,ks,Pt='Use <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.rename_column">IterableDataset.rename_column()</a> when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place.',Je,Zs,Ot='Provide <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.rename_column">IterableDataset.rename_column()</a> with the name of the original column, and the new column name:',Te,Us,xe,Is,$e,Gs,sl='When you need to remove one or more columns, give <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.remove_columns">IterableDataset.remove_columns()</a> the name of the column to remove. Remove more than one column by providing a list of column names:',_e,vs,ke,Cs,Ze,Xs,al='<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.cast">IterableDataset.cast()</a> changes the feature type of one or more columns. This method takes your new <code>Features</code> as its argument. The following sample code shows how to change the feature types of <code>ClassLabel</code> and <code>Value</code>:',Ue,Rs,Ie,G,Ge,Ys,el='Use <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.cast_column">IterableDataset.cast_column()</a> to change the feature type of just one column. Pass the column name and its new feature type as arguments:',ve,Fs,Ce,Ws,Xe,Vs,tl=`Similar to the <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.map">Dataset.map()</a> function for a regular <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset">Dataset</a>, ü§ó  Datasets features <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.map">IterableDataset.map()</a> for processing an <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a>.
<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.map">IterableDataset.map()</a> applies processing on-the-fly when examples are streamed.`,Re,Bs,ll="It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns.",Ye,zs,nl='The following example demonstrates how to tokenize a <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a>. The function needs to accept and output a <code>dict</code>:',Fe,Ns,We,Qs,pl='Next, apply this function to the dataset with <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.map">IterableDataset.map()</a>:',Ve,Hs,Be,Ds,il='Let‚Äôs take a look at another example, except this time, you will remove columns with <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.map">IterableDataset.map()</a>. When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed.',ze,Es,rl='Specify the column to remove with the <code>remove_columns</code> argument in <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.map">IterableDataset.map()</a>:',Ne,Ss,Qe,Ls,He,As,cl='<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.map">IterableDataset.map()</a> also supports working with batches of examples. Operate on batches by setting <code>batched=True</code>. The default batch size is 1000, but you can adjust it with the <code>batch_size</code> argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation.',De,qs,Ee,Ks,Se,v,Le,Ps,Ae,Os,hl='You can filter rows in the dataset based on a predicate function using <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.filter">Dataset.filter()</a>. It returns rows that match a specified condition:',qe,sa,Ke,aa,dl='<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.filter">Dataset.filter()</a> can also filter by indices if you set <code>with_indices=True</code>:',Pe,ea,Oe,ta,st,la,ol="The <code>batch</code> method transforms your <code>IterableDataset</code> into an iterable of batches. This is particularly useful when you want to work with batches in your training loop or when using frameworks that expect batched inputs.",at,C,et,na,ml="You can use the <code>batch</code> method like this:",tt,pa,lt,ia,fl=`In this example, batched_dataset is still an IterableDataset, but each item yielded is now a batch of 32 samples instead of a single sample.
This batching is done on-the-fly as you iterate over the dataset, preserving the memory-efficient nature of IterableDataset.`,nt,ra,ul=`The batch method also provides a drop_last_batch parameter.
When set to True, it will discard the last batch if it‚Äôs smaller than the specified batch_size.
This can be useful in scenarios where your downstream processing requires all batches to be of the same size:`,pt,ca,it,ha,rt,da,gl='<a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a> can be integrated into a training loop. First, shuffle the dataset:',ct,X,ht,oa,dt,ma,jl="If your training loop stops, you may want to restart the training from where it was. To do so you can save a checkpoint of your model and optimizers, as well as your data loader.",ot,fa,Ml='Iterable datasets don‚Äôt provide random access to a specific example index to resume from, but you can use <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.state_dict">IterableDataset.state_dict()</a> and <a href="/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.IterableDataset.load_state_dict">IterableDataset.load_state_dict()</a> to resume from a checkpoint instead, similarly to what you can do for models and optimizers:',mt,ua,ft,ga,bl="Returns:",ut,ja,gt,Ma,wl="Under the hood, the iterable dataset keeps track of the current shard being read and the example index in the current shard and it stores this info in the <code>state_dict</code>.",jt,ba,yl=`To resume from a checkpoint, the dataset skips all the shards that were previously read to restart from the current shard.
Then it reads the shard and skips examples until it reaches the exact example from the checkpoint.`,Mt,wa,Jl="Therefore restarting a dataset is quite fast, since it will not re-read the shards that have already been iterated on. Still, resuming a dataset is generally not instantaneous since it has to restart reading from the beginning of the current shard and skip examples until it reaches the checkpoint location.",bt,ya,Tl="This can be used with the <code>StatefulDataLoader</code> from <code>torchdata</code>:",wt,Ja,yt,R,Jt,Ta,Tt,_a,xt;return T=new J({props:{title:"Stream",local:"stream",headingTag:"h1"}}),V=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnSHVnZ2luZ0ZhY2VGVyUyRmZpbmV3ZWInJTJDJTIwc3BsaXQlM0QndHJhaW4nJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFwcmludChuZXh0KGl0ZXIoZGF0YXNldCkpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;HuggingFaceFW/fineweb&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;How AP reported in all formats from tornado-stricken regionsMarch 8, 2012\\nWhen the first serious bout of tornadoes of 2012 blew through middle America in the middle of the night, they touched down in places hours from any AP bureau...</span>`,wrap:!1}}),Q=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YV9maWxlcyUyMCUzRCUyMCU3Qid0cmFpbiclM0ElMjAncGF0aCUyRnRvJTJGT1NDQVItMjIwMSUyRmNvbXByZXNzZWQlMkZlbl9tZXRhJTJGKi5qc29ubC5neiclN0QlMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCdqc29uJyUyQyUyMGRhdGFfZmlsZXMlM0RkYXRhX2ZpbGVzJTJDJTIwc3BsaXQlM0QndHJhaW4nJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFwcmludChuZXh0KGl0ZXIoZGF0YXNldCkpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz&#x27;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=data_files, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.&#x27;</span>, ...`,wrap:!1}}),Z=new xa({props:{$$slots:{default:[zl]},$$scope:{ctx:y}}}),D=new J({props:{title:"Convert from a Dataset",local:"convert-from-a-dataset",headingTag:"h2"}}),S=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJldGh6JTJGZm9vZDEwMSUyMiklMEFpdGVyYWJsZV9kYXRhc2V0JTIwJTNEJTIwZGF0YXNldC50b19pdGVyYWJsZV9kYXRhc2V0KCklMEElMEFpdGVyYWJsZV9kYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmV0aHolMkZmb29kMTAxJTIyJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># faster üêá</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;ethz/food101&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset = dataset.to_iterable_dataset()

<span class="hljs-comment"># slower üê¢</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset = load_dataset(<span class="hljs-string">&quot;ethz/food101&quot;</span>, streaming=<span class="hljs-literal">True</span>)`,wrap:!1}}),A=new M({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmV0aHolMkZmb29kMTAxJTIyKSUwQWl0ZXJhYmxlX2RhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LnRvX2l0ZXJhYmxlX2RhdGFzZXQobnVtX3NoYXJkcyUzRDY0KSUyMCUyMyUyMHNoYXJkJTIwdGhlJTIwZGF0YXNldCUwQWl0ZXJhYmxlX2RhdGFzZXQlMjAlM0QlMjBpdGVyYWJsZV9kYXRhc2V0LnNodWZmbGUoYnVmZmVyX3NpemUlM0QxMF8wMDApJTIwJTIwJTIzJTIwc2h1ZmZsZXMlMjB0aGUlMjBzaGFyZHMlMjBvcmRlciUyMGFuZCUyMHVzZSUyMGElMjBzaHVmZmxlJTIwYnVmZmVyJTIwd2hlbiUyMHlvdSUyMHN0YXJ0JTIwaXRlcmF0aW5n",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;ethz/food101&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset = dataset.to_iterable_dataset(num_shards=<span class="hljs-number">64</span>) <span class="hljs-comment"># shard the dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset = iterable_dataset.shuffle(buffer_size=<span class="hljs-number">10_000</span>)  <span class="hljs-comment"># shuffles the shards order and use a shuffle buffer when you start iterating</span>
dataloader = torch.utils.data.DataLoader(iterable_dataset, num_workers=<span class="hljs-number">4</span>)  <span class="hljs-comment"># assigns 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating</span>`,wrap:!1}}),q=new J({props:{title:"Shuffle",local:"shuffle",headingTag:"h2"}}),O=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnSHVnZ2luZ0ZhY2VGVyUyRmZpbmV3ZWInJTJDJTIwc3BsaXQlM0QndHJhaW4nJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFzaHVmZmxlZF9kYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zaHVmZmxlKHNlZWQlM0Q0MiUyQyUyMGJ1ZmZlcl9zaXplJTNEMTBfMDAwKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;HuggingFaceFW/fineweb&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`,wrap:!1}}),U=new xa({props:{$$slots:{default:[Nl]},$$scope:{ctx:y}}}),ss=new J({props:{title:"Reshuffle",local:"reshuffle",headingTag:"h2"}}),ts=new M({props:{code:"Zm9yJTIwZXBvY2glMjBpbiUyMHJhbmdlKGVwb2NocyklM0ElMEElMjAlMjAlMjAlMjBzaHVmZmxlZF9kYXRhc2V0LnNldF9lcG9jaChlcG9jaCklMEElMjAlMjAlMjAlMjBmb3IlMjBleGFtcGxlJTIwaW4lMjBzaHVmZmxlZF9kYXRhc2V0JTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLi4u",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`,wrap:!1}}),ls=new J({props:{title:"Split dataset",local:"split-dataset",headingTag:"h2"}}),is=new M({props:{code:"ZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnSHVnZ2luZ0ZhY2VGVyUyRmZpbmV3ZWInJTJDJTIwc3BsaXQlM0QndHJhaW4nJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFkYXRhc2V0X2hlYWQlMjAlM0QlMjBkYXRhc2V0LnRha2UoMiklMEFsaXN0KGRhdGFzZXRfaGVhZCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;HuggingFaceFW/fineweb&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;How AP reported in all formats from tor...},
 {&#x27;text&#x27;: &#x27;Did you know you have two little yellow...}]</span>`,wrap:!1}}),cs=new M({props:{code:"dHJhaW5fZGF0YXNldCUyMCUzRCUyMHNodWZmbGVkX2RhdGFzZXQuc2tpcCgxMDAwKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)',wrap:!1}}),I=new xa({props:{warning:!0,$$slots:{default:[Ql]},$$scope:{ctx:y}}}),hs=new J({props:{title:"Shard",local:"shard",headingTag:"h3"}}),ms=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJhbWF6b25fcG9sYXJpdHklMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFwcmludChkYXRhc2V0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;amazon_polarity&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(dataset)
IterableDataset({
    features: [<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>],
    num_shards: <span class="hljs-number">4</span>
})`,wrap:!1}}),us=new M({props:{code:"ZGF0YXNldC5zaGFyZChudW1fc2hhcmRzJTNEMiUyQyUyMGluZGV4JTNEMCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.shard(num_shards=<span class="hljs-number">2</span>, index=<span class="hljs-number">0</span>)
IterableDataset({
    features: [<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>],
    num_shards: <span class="hljs-number">2</span>
})`,wrap:!1}}),js=new J({props:{title:"Interleave",local:"interleave",headingTag:"h2"}}),bs=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwaW50ZXJsZWF2ZV9kYXRhc2V0cyUwQWVzX2RhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJ2FsbGVuYWklMkZjNCclMkMlMjAnZXMnJTJDJTIwc3BsaXQlM0QndHJhaW4nJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFmcl9kYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCdhbGxlbmFpJTJGYzQnJTJDJTIwJ2ZyJyUyQyUyMHNwbGl0JTNEJ3RyYWluJyUyQyUyMHN0cmVhbWluZyUzRFRydWUpJTBBJTBBbXVsdGlsaW5ndWFsX2RhdGFzZXQlMjAlM0QlMjBpbnRlcmxlYXZlX2RhdGFzZXRzKCU1QmVzX2RhdGFzZXQlMkMlMjBmcl9kYXRhc2V0JTVEKSUwQWxpc3QobXVsdGlsaW5ndWFsX2RhdGFzZXQudGFrZSgyKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>es_dataset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, <span class="hljs-string">&#x27;es&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, <span class="hljs-string">&#x27;fr&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([es_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(multilingual_dataset.take(<span class="hljs-number">2</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Comprar Zapatillas para ni√±a en chancla con goma por...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Le sacre de philippe ier, 23 mai 1059 - Compte Rendu...&#x27;</span>}]`,wrap:!1}}),ys=new M({props:{code:"bXVsdGlsaW5ndWFsX2RhdGFzZXRfd2l0aF9vdmVyc2FtcGxpbmclMjAlM0QlMjBpbnRlcmxlYXZlX2RhdGFzZXRzKCU1QmVzX2RhdGFzZXQlMkMlMjBmcl9kYXRhc2V0JTVEJTJDJTIwcHJvYmFiaWxpdGllcyUzRCU1QjAuOCUyQyUyMDAuMiU1RCUyQyUyMHNlZWQlM0Q0MiklMEFsaXN0KG11bHRpbGluZ3VhbF9kYXRhc2V0X3dpdGhfb3ZlcnNhbXBsaW5nLnRha2UoMikp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([es_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(multilingual_dataset_with_oversampling.take(<span class="hljs-number">2</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Comprar Zapatillas para ni√±a en chancla con goma por...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Chevrolet Cavalier Usados en Bogota - Carros en Vent...&#x27;</span>}]`,wrap:!1}}),xs=new J({props:{title:"Rename, remove, and cast",local:"rename-remove-and-cast",headingTag:"h2"}}),_s=new J({props:{title:"Rename",local:"rename",headingTag:"h3"}}),Us=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnYWxsZW5haSUyRmM0JyUyQyUyMCdlbiclMkMlMjBzdHJlYW1pbmclM0RUcnVlJTJDJTIwc3BsaXQlM0QndHJhaW4nKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LnJlbmFtZV9jb2x1bW4oJTIydGV4dCUyMiUyQyUyMCUyMmNvbnRlbnQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>)`,wrap:!1}}),Is=new J({props:{title:"Remove",local:"remove",headingTag:"h3"}}),vs=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnYWxsZW5haSUyRmM0JyUyQyUyMCdlbiclMkMlMjBzdHJlYW1pbmclM0RUcnVlJTJDJTIwc3BsaXQlM0QndHJhaW4nKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LnJlbW92ZV9jb2x1bW5zKCd0aW1lc3RhbXAnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`,wrap:!1}}),Cs=new J({props:{title:"Cast",local:"cast",headingTag:"h3"}}),Rs=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnbnl1LW1sbCUyRmdsdWUnJTJDJTIwJ21ycGMnJTJDJTIwc3BsaXQlM0QndHJhaW4nJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEFkYXRhc2V0LmZlYXR1cmVzJTBBJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwQ2xhc3NMYWJlbCUyQyUyMFZhbHVlJTBBbmV3X2ZlYXR1cmVzJTIwJTNEJTIwZGF0YXNldC5mZWF0dXJlcy5jb3B5KCklMEFuZXdfZmVhdHVyZXMlNUIlMjJsYWJlbCUyMiU1RCUyMCUzRCUyMENsYXNzTGFiZWwobmFtZXMlM0QlNUInbmVnYXRpdmUnJTJDJTIwJ3Bvc2l0aXZlJyU1RCklMEFuZXdfZmVhdHVyZXMlNUIlMjJpZHglMjIlNUQlMjAlM0QlMjBWYWx1ZSgnaW50NjQnKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LmNhc3QobmV3X2ZlYXR1cmVzKSUwQWRhdGFzZXQuZmVhdHVyZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;nyu-mll/glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features = dataset.features.copy()
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;label&quot;</span>] = ClassLabel(names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;idx&quot;</span>] = Value(<span class="hljs-string">&#x27;int64&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast(new_features)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>], <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`,wrap:!1}}),G=new xa({props:{$$slots:{default:[Hl]},$$scope:{ctx:y}}}),Fs=new M({props:{code:"ZGF0YXNldC5mZWF0dXJlcyUwQSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LmNhc3RfY29sdW1uKCUyMmF1ZGlvJTIyJTJDJTIwQXVkaW8oc2FtcGxpbmdfcmF0ZSUzRDE2MDAwKSklMEFkYXRhc2V0LmZlYXR1cmVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">44100</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">16000</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`,wrap:!1}}),Ws=new J({props:{title:"Map",local:"map",headingTag:"h2"}}),Ns=new M({props:{code:"ZGVmJTIwYWRkX3ByZWZpeChleGFtcGxlKSUzQSUwQSUyMCUyMCUyMCUyMGV4YW1wbGUlNUIndGV4dCclNUQlMjAlM0QlMjAnTXklMjB0ZXh0JTNBJTIwJyUyMCUyQiUyMGV4YW1wbGUlNUIndGV4dCclNUQlMEElMjAlMjAlMjAlMjByZXR1cm4lMjBleGFtcGxl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`,wrap:!1}}),Hs=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnYWxsZW5haSUyRmM0JyUyQyUyMCdlbiclMkMlMjBzdHJlYW1pbmclM0RUcnVlJTJDJTIwc3BsaXQlM0QndHJhaW4nKSUwQXVwZGF0ZWRfZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQubWFwKGFkZF9wcmVmaXgpJTBBbGlzdCh1cGRhdGVkX2RhdGFzZXQudGFrZSgzKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making...&#x27;</span>,
  <span class="hljs-string">&#x27;timestamp&#x27;</span>: <span class="hljs-string">&#x27;2019-04-25 12:57:54&#x27;</span>,
  <span class="hljs-string">&#x27;url&#x27;</span>: <span class="hljs-string">&#x27;https://klyq.com/beginners-bbq-class-taking-place-in-missoula/&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Discussion in \\&#x27;Mac OS X Lion (10.7)\\&#x27; started by axboi87, Jan 20, 2012.\\nI\\&#x27;ve go...&#x27;</span>,
  <span class="hljs-string">&#x27;timestamp&#x27;</span>: <span class="hljs-string">&#x27;2019-04-21 10:07:13&#x27;</span>,
  <span class="hljs-string">&#x27;url&#x27;</span>: <span class="hljs-string">&#x27;https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metall...&#x27;</span>,
  <span class="hljs-string">&#x27;timestamp&#x27;</span>: <span class="hljs-string">&#x27;2019-04-25 10:40:23&#x27;</span>,
  <span class="hljs-string">&#x27;url&#x27;</span>: <span class="hljs-string">&#x27;https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way&#x27;</span>}]`,wrap:!1}}),Ss=new M({props:{code:"dXBkYXRlZF9kYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5tYXAoYWRkX3ByZWZpeCUyQyUyMHJlbW92ZV9jb2x1bW5zJTNEJTVCJTIydGltZXN0YW1wJTIyJTJDJTIwJTIydXJsJTIyJTVEKSUwQWxpc3QodXBkYXRlZF9kYXRhc2V0LnRha2UoMykp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Discussion in \\&#x27;Mac OS X Lion (10.7)\\&#x27; started by axboi87, Jan 20, 2012.\\nI\\&#x27;ve go...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metall...&#x27;</span>}]`,wrap:!1}}),Ls=new J({props:{title:"Batch processing",local:"batch-processing",headingTag:"h3"}}),qs=new J({props:{title:"Tokenization",local:"tokenization",headingTag:"h4"}}),Ks=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmFsbGVuYWklMkZjNCUyMiUyQyUyMCUyMmVuJTIyJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSUyQyUyMHNwbGl0JTNEJTIydHJhaW4lMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJ2Rpc3RpbGJlcnQtYmFzZS11bmNhc2VkJyklMEFkZWYlMjBlbmNvZGUoZXhhbXBsZXMpJTNBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwdG9rZW5pemVyKGV4YW1wbGVzJTVCJ3RleHQnJTVEJTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMjBwYWRkaW5nJTNEJ21heF9sZW5ndGgnKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0Lm1hcChlbmNvZGUlMkMlMjBiYXRjaGVkJTNEVHJ1ZSUyQyUyMHJlbW92ZV9jb2x1bW5zJTNEJTVCJTIydGV4dCUyMiUyQyUyMCUyMnRpbWVzdGFtcCUyMiUyQyUyMCUyMnVybCUyMiU1RCklMEFuZXh0KGl0ZXIoZGF0YXNldCkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">4088</span>, <span class="hljs-number">16912</span>, <span class="hljs-number">22861</span>, <span class="hljs-number">4160</span>, <span class="hljs-number">2465</span>, <span class="hljs-number">2635</span>, <span class="hljs-number">2173</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">3335</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]}`,wrap:!1}}),v=new xa({props:{$$slots:{default:[Dl]},$$scope:{ctx:y}}}),Ps=new J({props:{title:"Filter",local:"filter",headingTag:"h3"}}),sa=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCgnSHVnZ2luZ0ZhY2VGVyUyRmZpbmV3ZWInJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSUyQyUyMHNwbGl0JTNEJ3RyYWluJyklMEFzdGFydF93aXRoX2FyJTIwJTNEJTIwZGF0YXNldC5maWx0ZXIobGFtYmRhJTIwZXhhbXBsZSUzQSUyMGV4YW1wbGUlNUIndGV4dCclNUQuc3RhcnRzd2l0aCgnU2FuJTIwRnJhbmNpc2NvJykpJTBBbmV4dChpdGVyKHN0YXJ0X3dpdGhfYXIpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;HuggingFaceFW/fineweb&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;text&#x27;</span>].startswith(<span class="hljs-string">&#x27;San Francisco&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(start_with_ar))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the sea...}</span>`,wrap:!1}}),ea=new M({props:{code:"ZXZlbl9kYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5maWx0ZXIobGFtYmRhJTIwZXhhbXBsZSUyQyUyMGlkeCUzQSUyMGlkeCUyMCUyNSUyMDIlMjAlM0QlM0QlMjAwJTJDJTIwd2l0aF9pbmRpY2VzJTNEVHJ1ZSklMEFsaXN0KGV2ZW5fZGF0YXNldC50YWtlKDMpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(even_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;How AP reported in all formats from tornado-stricken regionsMarch 8, 2012 Whe...},
 {&#x27;</span>text<span class="hljs-string">&#x27;: &#x27;</span>Car Wash For Clara! Now <span class="hljs-keyword">is</span> your chance to <span class="hljs-built_in">help</span>! <span class="hljs-number">2</span> year old Clara Woodward has...},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Log In Please enter your ECode to log in. Forgotten your eCode? If you create...}]</span>`,wrap:!1}}),ta=new J({props:{title:"Batch",local:"batch",headingTag:"h2"}}),C=new xa({props:{$$slots:{default:[El]},$$scope:{ctx:y}}}),pa=new M({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBJTIzJTIwTG9hZCUyMGElMjBkYXRhc2V0JTIwaW4lMjBzdHJlYW1pbmclMjBtb2RlJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJzb21lX2RhdGFzZXQlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyJTJDJTIwc3RyZWFtaW5nJTNEVHJ1ZSklMEElMEElMjMlMjBDcmVhdGUlMjBiYXRjaGVzJTIwb2YlMjAzMiUyMHNhbXBsZXMlMEFiYXRjaGVkX2RhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LmJhdGNoKGJhdGNoX3NpemUlM0QzMiklMEElMEElMjMlMjBJdGVyYXRlJTIwb3ZlciUyMHRoZSUyMGJhdGNoZWQlMjBkYXRhc2V0JTBBZm9yJTIwYmF0Y2glMjBpbiUyMGJhdGNoZWRfZGF0YXNldCUzQSUwQSUyMCUyMCUyMCUyMHByaW50KGJhdGNoKSUwQSUyMCUyMCUyMCUyMGJyZWFr",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># Load a dataset in streaming mode</span>
dataset = load_dataset(<span class="hljs-string">&quot;some_dataset&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Create batches of 32 samples</span>
batched_dataset = dataset.batch(batch_size=<span class="hljs-number">32</span>)

<span class="hljs-comment"># Iterate over the batched dataset</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> batched_dataset:
    <span class="hljs-built_in">print</span>(batch)
    <span class="hljs-keyword">break</span>`,wrap:!1}}),ca=new M({props:{code:"YmF0Y2hlZF9kYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5iYXRjaChiYXRjaF9zaXplJTNEMzIlMkMlMjBkcm9wX2xhc3RfYmF0Y2glM0RUcnVlKQ==",highlighted:'batched_dataset = dataset.batch(batch_size=<span class="hljs-number">32</span>, drop_last_batch=<span class="hljs-literal">True</span>)',wrap:!1}}),ha=new J({props:{title:"Stream in a training loop",local:"stream-in-a-training-loop",headingTag:"h2"}}),X=new Wl({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Ll]},$$scope:{ctx:y}}}),oa=new J({props:{title:"Save a dataset checkpoint and resume iteration",local:"save-a-dataset-checkpoint-and-resume-iteration",headingTag:"h3"}}),ua=new M({props:{code:"aXRlcmFibGVfZGF0YXNldCUyMCUzRCUyMERhdGFzZXQuZnJvbV9kaWN0KCU3QiUyMmElMjIlM0ElMjByYW5nZSg2KSU3RCkudG9faXRlcmFibGVfZGF0YXNldChudW1fc2hhcmRzJTNEMyklMEFmb3IlMjBpZHglMkMlMjBleGFtcGxlJTIwaW4lMjBlbnVtZXJhdGUoaXRlcmFibGVfZGF0YXNldCklM0ElMEElMjAlMjAlMjAlMjBwcmludChleGFtcGxlKSUwQSUyMCUyMCUyMCUyMGlmJTIwaWR4JTIwJTNEJTNEJTIwMiUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHN0YXRlX2RpY3QlMjAlM0QlMjBpdGVyYWJsZV9kYXRhc2V0LnN0YXRlX2RpY3QoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByaW50KCUyMmNoZWNrcG9pbnQlMjIpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYnJlYWslMEFpdGVyYWJsZV9kYXRhc2V0LmxvYWRfc3RhdGVfZGljdChzdGF0ZV9kaWN0KSUwQXByaW50KGYlMjJyZXN0YXJ0JTIwZnJvbSUyMGNoZWNrcG9pbnQlMjIpJTBBZm9yJTIwZXhhbXBsZSUyMGluJTIwaXRlcmFibGVfZGF0YXNldCUzQSUwQSUyMCUyMCUyMCUyMHByaW50KGV4YW1wbGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>)}).to_iterable_dataset(num_shards=<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> idx, example <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(iterable_dataset):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(example)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> idx == <span class="hljs-number">2</span>:
<span class="hljs-meta">... </span>        state_dict = iterable_dataset.state_dict()
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;checkpoint&quot;</span>)
<span class="hljs-meta">... </span>        <span class="hljs-keyword">break</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset.load_state_dict(state_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;restart from checkpoint&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> iterable_dataset:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(example)`,wrap:!1}}),ja=new M({props:{code:"JTdCJ2EnJTNBJTIwMCU3RCUwQSU3QidhJyUzQSUyMDElN0QlMEElN0InYSclM0ElMjAyJTdEJTBBY2hlY2twb2ludCUwQXJlc3RhcnQlMjBmcm9tJTIwY2hlY2twb2ludCUwQSU3QidhJyUzQSUyMDMlN0QlMEElN0InYSclM0ElMjA0JTdEJTBBJTdCJ2EnJTNBJTIwNSU3RA==",highlighted:`{<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">0</span>}
{<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">1</span>}
{<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">2</span>}
<span class="hljs-keyword">checkpoint</span>
<span class="hljs-keyword">restart</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">checkpoint</span>
{<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">3</span>}
{<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">4</span>}
{<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">5</span>}`,wrap:!1}}),Ja=new M({props:{code:"ZnJvbSUyMHRvcmNoZGF0YS5zdGF0ZWZ1bF9kYXRhbG9hZGVyJTIwaW1wb3J0JTIwU3RhdGVmdWxEYXRhTG9hZGVyJTBBaXRlcmFibGVfZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJkZWVwbWluZCUyRmNvZGVfY29udGVzdHMlMjIlMkMlMjBzdHJlYW1pbmclM0RUcnVlJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiUyMiklMEFkYXRhbG9hZGVyJTIwJTNEJTIwU3RhdGVmdWxEYXRhTG9hZGVyKGl0ZXJhYmxlX2RhdGFzZXQlMkMlMjBiYXRjaF9zaXplJTNEMzIlMkMlMjBudW1fd29ya2VycyUzRDQpJTBBJTIzJTIwY2hlY2twb2ludCUwQXN0YXRlX2RpY3QlMjAlM0QlMjBkYXRhbG9hZGVyLnN0YXRlX2RpY3QoKSUyMCUyMCUyMyUyMHVzZXMlMjBpdGVyYWJsZV9kYXRhc2V0LnN0YXRlX2RpY3QoKSUyMHVuZGVyJTIwdGhlJTIwaG9vZCUwQSUyMyUyMHJlc3VtZSUyMGZyb20lMjBjaGVja3BvaW50JTBBZGF0YWxvYWRlci5sb2FkX3N0YXRlX2RpY3Qoc3RhdGVfZGljdCklMjAlMjAlMjMlMjB1c2VzJTIwaXRlcmFibGVfZGF0YXNldC5sb2FkX3N0YXRlX2RpY3QoKSUyMHVuZGVyJTIwdGhlJTIwaG9vZA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchdata.stateful_dataloader <span class="hljs-keyword">import</span> StatefulDataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span>iterable_dataset = load_dataset(<span class="hljs-string">&quot;deepmind/code_contests&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = StatefulDataLoader(iterable_dataset, batch_size=<span class="hljs-number">32</span>, num_workers=<span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># checkpoint</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>state_dict = dataloader.state_dict()  <span class="hljs-comment"># uses iterable_dataset.state_dict() under the hood</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># resume from checkpoint</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader.load_state_dict(state_dict)  <span class="hljs-comment"># uses iterable_dataset.load_state_dict() under the hood</span>`,wrap:!1}}),R=new xa({props:{$$slots:{default:[Al]},$$scope:{ctx:y}}}),Ta=new Bl({props:{source:"https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx"}}),{c(){p=r("meta"),j=l(),i=r("p"),b=l(),d(T.$$.fragment),x=l(),$=r("p"),$.textContent=w,_=l(),F=r("ul"),F.innerHTML=kt,ka=l(),k=r("div"),k.innerHTML=Zt,Za=l(),W=r("p"),W.innerHTML=Ut,Ua=l(),d(V.$$.fragment),Ia=l(),B=r("p"),B.textContent=It,Ga=l(),z=r("ul"),z.innerHTML=Gt,va=l(),N=r("p"),N.innerHTML=vt,Ca=l(),d(Q.$$.fragment),Xa=l(),H=r("p"),H.innerHTML=Ct,Ra=l(),d(Z.$$.fragment),Ya=l(),d(D.$$.fragment),Fa=l(),E=r("p"),E.innerHTML=Xt,Wa=l(),d(S.$$.fragment),Va=l(),L=r("p"),L.innerHTML=Rt,Ba=l(),d(A.$$.fragment),za=l(),d(q.$$.fragment),Na=l(),K=r("p"),K.innerHTML=Yt,Qa=l(),P=r("p"),P.innerHTML=Ft,Ha=l(),d(O.$$.fragment),Da=l(),d(U.$$.fragment),Ea=l(),d(ss.$$.fragment),Sa=l(),as=r("p"),as.innerHTML=Wt,La=l(),es=r("p"),es.innerHTML=Vt,Aa=l(),d(ts.$$.fragment),qa=l(),d(ls.$$.fragment),Ka=l(),ns=r("p"),ns.textContent=Bt,Pa=l(),ps=r("ul"),ps.innerHTML=zt,Oa=l(),d(is.$$.fragment),se=l(),rs=r("ul"),rs.innerHTML=Nt,ae=l(),d(cs.$$.fragment),ee=l(),d(I.$$.fragment),te=l(),$a=r("a"),le=l(),d(hs.$$.fragment),ne=l(),ds=r("p"),ds.innerHTML=Qt,pe=l(),os=r("p"),os.innerHTML=Ht,ie=l(),d(ms.$$.fragment),re=l(),fs=r("p"),fs.textContent=Dt,ce=l(),d(us.$$.fragment),he=l(),gs=r("p"),gs.innerHTML=Et,de=l(),d(js.$$.fragment),oe=l(),Ms=r("p"),Ms.innerHTML=St,me=l(),d(bs.$$.fragment),fe=l(),ws=r("p"),ws.innerHTML=Lt,ue=l(),d(ys.$$.fragment),ge=l(),Js=r("p"),Js.innerHTML=At,je=l(),Ts=r("p"),Ts.innerHTML=qt,Me=l(),d(xs.$$.fragment),be=l(),$s=r("p"),$s.textContent=Kt,we=l(),d(_s.$$.fragment),ye=l(),ks=r("p"),ks.innerHTML=Pt,Je=l(),Zs=r("p"),Zs.innerHTML=Ot,Te=l(),d(Us.$$.fragment),xe=l(),d(Is.$$.fragment),$e=l(),Gs=r("p"),Gs.innerHTML=sl,_e=l(),d(vs.$$.fragment),ke=l(),d(Cs.$$.fragment),Ze=l(),Xs=r("p"),Xs.innerHTML=al,Ue=l(),d(Rs.$$.fragment),Ie=l(),d(G.$$.fragment),Ge=l(),Ys=r("p"),Ys.innerHTML=el,ve=l(),d(Fs.$$.fragment),Ce=l(),d(Ws.$$.fragment),Xe=l(),Vs=r("p"),Vs.innerHTML=tl,Re=l(),Bs=r("p"),Bs.textContent=ll,Ye=l(),zs=r("p"),zs.innerHTML=nl,Fe=l(),d(Ns.$$.fragment),We=l(),Qs=r("p"),Qs.innerHTML=pl,Ve=l(),d(Hs.$$.fragment),Be=l(),Ds=r("p"),Ds.innerHTML=il,ze=l(),Es=r("p"),Es.innerHTML=rl,Ne=l(),d(Ss.$$.fragment),Qe=l(),d(Ls.$$.fragment),He=l(),As=r("p"),As.innerHTML=cl,De=l(),d(qs.$$.fragment),Ee=l(),d(Ks.$$.fragment),Se=l(),d(v.$$.fragment),Le=l(),d(Ps.$$.fragment),Ae=l(),Os=r("p"),Os.innerHTML=hl,qe=l(),d(sa.$$.fragment),Ke=l(),aa=r("p"),aa.innerHTML=dl,Pe=l(),d(ea.$$.fragment),Oe=l(),d(ta.$$.fragment),st=l(),la=r("p"),la.innerHTML=ol,at=l(),d(C.$$.fragment),et=l(),na=r("p"),na.innerHTML=ml,tt=l(),d(pa.$$.fragment),lt=l(),ia=r("p"),ia.textContent=fl,nt=l(),ra=r("p"),ra.textContent=ul,pt=l(),d(ca.$$.fragment),it=l(),d(ha.$$.fragment),rt=l(),da=r("p"),da.innerHTML=gl,ct=l(),d(X.$$.fragment),ht=l(),d(oa.$$.fragment),dt=l(),ma=r("p"),ma.textContent=jl,ot=l(),fa=r("p"),fa.innerHTML=Ml,mt=l(),d(ua.$$.fragment),ft=l(),ga=r("p"),ga.textContent=bl,ut=l(),d(ja.$$.fragment),gt=l(),Ma=r("p"),Ma.innerHTML=wl,jt=l(),ba=r("p"),ba.textContent=yl,Mt=l(),wa=r("p"),wa.textContent=Jl,bt=l(),ya=r("p"),ya.innerHTML=Tl,wt=l(),d(Ja.$$.fragment),yt=l(),d(R.$$.fragment),Jt=l(),d(Ta.$$.fragment),Tt=l(),_a=r("p"),this.h()},l(s){const a=Yl("svelte-u9bgzb",document.head);p=c(a,"META",{name:!0,content:!0}),a.forEach(e),j=n(s),i=c(s,"P",{}),_t(i).forEach(e),b=n(s),o(T.$$.fragment,s),x=n(s),$=c(s,"P",{"data-svelte-h":!0}),h($)!=="svelte-1sf84jt"&&($.textContent=w),_=n(s),F=c(s,"UL",{"data-svelte-h":!0}),h(F)!=="svelte-1y6qvic"&&(F.innerHTML=kt),ka=n(s),k=c(s,"DIV",{class:!0,"data-svelte-h":!0}),h(k)!=="svelte-p91s8d"&&(k.innerHTML=Zt),Za=n(s),W=c(s,"P",{"data-svelte-h":!0}),h(W)!=="svelte-j07qx7"&&(W.innerHTML=Ut),Ua=n(s),o(V.$$.fragment,s),Ia=n(s),B=c(s,"P",{"data-svelte-h":!0}),h(B)!=="svelte-xdwdpa"&&(B.textContent=It),Ga=n(s),z=c(s,"UL",{"data-svelte-h":!0}),h(z)!=="svelte-lffs3"&&(z.innerHTML=Gt),va=n(s),N=c(s,"P",{"data-svelte-h":!0}),h(N)!=="svelte-1newt0m"&&(N.innerHTML=vt),Ca=n(s),o(Q.$$.fragment,s),Xa=n(s),H=c(s,"P",{"data-svelte-h":!0}),h(H)!=="svelte-1k8lr8m"&&(H.innerHTML=Ct),Ra=n(s),o(Z.$$.fragment,s),Ya=n(s),o(D.$$.fragment,s),Fa=n(s),E=c(s,"P",{"data-svelte-h":!0}),h(E)!=="svelte-lssg0c"&&(E.innerHTML=Xt),Wa=n(s),o(S.$$.fragment,s),Va=n(s),L=c(s,"P",{"data-svelte-h":!0}),h(L)!=="svelte-1uxjx4j"&&(L.innerHTML=Rt),Ba=n(s),o(A.$$.fragment,s),za=n(s),o(q.$$.fragment,s),Na=n(s),K=c(s,"P",{"data-svelte-h":!0}),h(K)!=="svelte-1b7wpa9"&&(K.innerHTML=Yt),Qa=n(s),P=c(s,"P",{"data-svelte-h":!0}),h(P)!=="svelte-rs1xkt"&&(P.innerHTML=Ft),Ha=n(s),o(O.$$.fragment,s),Da=n(s),o(U.$$.fragment,s),Ea=n(s),o(ss.$$.fragment,s),Sa=n(s),as=c(s,"P",{"data-svelte-h":!0}),h(as)!=="svelte-l7znh1"&&(as.innerHTML=Wt),La=n(s),es=c(s,"P",{"data-svelte-h":!0}),h(es)!=="svelte-flznjz"&&(es.innerHTML=Vt),Aa=n(s),o(ts.$$.fragment,s),qa=n(s),o(ls.$$.fragment,s),Ka=n(s),ns=c(s,"P",{"data-svelte-h":!0}),h(ns)!=="svelte-1y9p9l9"&&(ns.textContent=Bt),Pa=n(s),ps=c(s,"UL",{"data-svelte-h":!0}),h(ps)!=="svelte-c145tb"&&(ps.innerHTML=zt),Oa=n(s),o(is.$$.fragment,s),se=n(s),rs=c(s,"UL",{"data-svelte-h":!0}),h(rs)!=="svelte-66n5w"&&(rs.innerHTML=Nt),ae=n(s),o(cs.$$.fragment,s),ee=n(s),o(I.$$.fragment,s),te=n(s),$a=c(s,"A",{id:!0}),_t($a).forEach(e),le=n(s),o(hs.$$.fragment,s),ne=n(s),ds=c(s,"P",{"data-svelte-h":!0}),h(ds)!=="svelte-1aegrxb"&&(ds.innerHTML=Qt),pe=n(s),os=c(s,"P",{"data-svelte-h":!0}),h(os)!=="svelte-53o0s5"&&(os.innerHTML=Ht),ie=n(s),o(ms.$$.fragment,s),re=n(s),fs=c(s,"P",{"data-svelte-h":!0}),h(fs)!=="svelte-bcezc5"&&(fs.textContent=Dt),ce=n(s),o(us.$$.fragment,s),he=n(s),gs=c(s,"P",{"data-svelte-h":!0}),h(gs)!=="svelte-86ubmp"&&(gs.innerHTML=Et),de=n(s),o(js.$$.fragment,s),oe=n(s),Ms=c(s,"P",{"data-svelte-h":!0}),h(Ms)!=="svelte-1d6uyc2"&&(Ms.innerHTML=St),me=n(s),o(bs.$$.fragment,s),fe=n(s),ws=c(s,"P",{"data-svelte-h":!0}),h(ws)!=="svelte-1t5oa18"&&(ws.innerHTML=Lt),ue=n(s),o(ys.$$.fragment,s),ge=n(s),Js=c(s,"P",{"data-svelte-h":!0}),h(Js)!=="svelte-1oi4bk"&&(Js.innerHTML=At),je=n(s),Ts=c(s,"P",{"data-svelte-h":!0}),h(Ts)!=="svelte-13euv56"&&(Ts.innerHTML=qt),Me=n(s),o(xs.$$.fragment,s),be=n(s),$s=c(s,"P",{"data-svelte-h":!0}),h($s)!=="svelte-mlzv10"&&($s.textContent=Kt),we=n(s),o(_s.$$.fragment,s),ye=n(s),ks=c(s,"P",{"data-svelte-h":!0}),h(ks)!=="svelte-1dbp7vc"&&(ks.innerHTML=Pt),Je=n(s),Zs=c(s,"P",{"data-svelte-h":!0}),h(Zs)!=="svelte-t4o82r"&&(Zs.innerHTML=Ot),Te=n(s),o(Us.$$.fragment,s),xe=n(s),o(Is.$$.fragment,s),$e=n(s),Gs=c(s,"P",{"data-svelte-h":!0}),h(Gs)!=="svelte-1b8kdfa"&&(Gs.innerHTML=sl),_e=n(s),o(vs.$$.fragment,s),ke=n(s),o(Cs.$$.fragment,s),Ze=n(s),Xs=c(s,"P",{"data-svelte-h":!0}),h(Xs)!=="svelte-117l4x4"&&(Xs.innerHTML=al),Ue=n(s),o(Rs.$$.fragment,s),Ie=n(s),o(G.$$.fragment,s),Ge=n(s),Ys=c(s,"P",{"data-svelte-h":!0}),h(Ys)!=="svelte-pb8rvg"&&(Ys.innerHTML=el),ve=n(s),o(Fs.$$.fragment,s),Ce=n(s),o(Ws.$$.fragment,s),Xe=n(s),Vs=c(s,"P",{"data-svelte-h":!0}),h(Vs)!=="svelte-crpwf3"&&(Vs.innerHTML=tl),Re=n(s),Bs=c(s,"P",{"data-svelte-h":!0}),h(Bs)!=="svelte-1rfoeeh"&&(Bs.textContent=ll),Ye=n(s),zs=c(s,"P",{"data-svelte-h":!0}),h(zs)!=="svelte-1quswlu"&&(zs.innerHTML=nl),Fe=n(s),o(Ns.$$.fragment,s),We=n(s),Qs=c(s,"P",{"data-svelte-h":!0}),h(Qs)!=="svelte-ek1o2b"&&(Qs.innerHTML=pl),Ve=n(s),o(Hs.$$.fragment,s),Be=n(s),Ds=c(s,"P",{"data-svelte-h":!0}),h(Ds)!=="svelte-vjqmss"&&(Ds.innerHTML=il),ze=n(s),Es=c(s,"P",{"data-svelte-h":!0}),h(Es)!=="svelte-ffq351"&&(Es.innerHTML=rl),Ne=n(s),o(Ss.$$.fragment,s),Qe=n(s),o(Ls.$$.fragment,s),He=n(s),As=c(s,"P",{"data-svelte-h":!0}),h(As)!=="svelte-xoequz"&&(As.innerHTML=cl),De=n(s),o(qs.$$.fragment,s),Ee=n(s),o(Ks.$$.fragment,s),Se=n(s),o(v.$$.fragment,s),Le=n(s),o(Ps.$$.fragment,s),Ae=n(s),Os=c(s,"P",{"data-svelte-h":!0}),h(Os)!=="svelte-quqsuf"&&(Os.innerHTML=hl),qe=n(s),o(sa.$$.fragment,s),Ke=n(s),aa=c(s,"P",{"data-svelte-h":!0}),h(aa)!=="svelte-th1aer"&&(aa.innerHTML=dl),Pe=n(s),o(ea.$$.fragment,s),Oe=n(s),o(ta.$$.fragment,s),st=n(s),la=c(s,"P",{"data-svelte-h":!0}),h(la)!=="svelte-144pwpf"&&(la.innerHTML=ol),at=n(s),o(C.$$.fragment,s),et=n(s),na=c(s,"P",{"data-svelte-h":!0}),h(na)!=="svelte-la94za"&&(na.innerHTML=ml),tt=n(s),o(pa.$$.fragment,s),lt=n(s),ia=c(s,"P",{"data-svelte-h":!0}),h(ia)!=="svelte-z19uc5"&&(ia.textContent=fl),nt=n(s),ra=c(s,"P",{"data-svelte-h":!0}),h(ra)!=="svelte-1srxqyd"&&(ra.textContent=ul),pt=n(s),o(ca.$$.fragment,s),it=n(s),o(ha.$$.fragment,s),rt=n(s),da=c(s,"P",{"data-svelte-h":!0}),h(da)!=="svelte-1bwm9xg"&&(da.innerHTML=gl),ct=n(s),o(X.$$.fragment,s),ht=n(s),o(oa.$$.fragment,s),dt=n(s),ma=c(s,"P",{"data-svelte-h":!0}),h(ma)!=="svelte-qoizba"&&(ma.textContent=jl),ot=n(s),fa=c(s,"P",{"data-svelte-h":!0}),h(fa)!=="svelte-29qfvj"&&(fa.innerHTML=Ml),mt=n(s),o(ua.$$.fragment,s),ft=n(s),ga=c(s,"P",{"data-svelte-h":!0}),h(ga)!=="svelte-f816x"&&(ga.textContent=bl),ut=n(s),o(ja.$$.fragment,s),gt=n(s),Ma=c(s,"P",{"data-svelte-h":!0}),h(Ma)!=="svelte-ul64r3"&&(Ma.innerHTML=wl),jt=n(s),ba=c(s,"P",{"data-svelte-h":!0}),h(ba)!=="svelte-fghrva"&&(ba.textContent=yl),Mt=n(s),wa=c(s,"P",{"data-svelte-h":!0}),h(wa)!=="svelte-1i92o70"&&(wa.textContent=Jl),bt=n(s),ya=c(s,"P",{"data-svelte-h":!0}),h(ya)!=="svelte-10pdp7c"&&(ya.innerHTML=Tl),wt=n(s),o(Ja.$$.fragment,s),yt=n(s),o(R.$$.fragment,s),Jt=n(s),o(Ta.$$.fragment,s),Tt=n(s),_a=c(s,"P",{}),_t(_a).forEach(e),this.h()},h(){$t(p,"name","hf:doc:metadata"),$t(p,"content",Kl),$t(k,"class","flex justify-center"),$t($a,"id","interleave_datasets")},m(s,a){Fl(document.head,p),t(s,j,a),t(s,i,a),t(s,b,a),m(T,s,a),t(s,x,a),t(s,$,a),t(s,_,a),t(s,F,a),t(s,ka,a),t(s,k,a),t(s,Za,a),t(s,W,a),t(s,Ua,a),m(V,s,a),t(s,Ia,a),t(s,B,a),t(s,Ga,a),t(s,z,a),t(s,va,a),t(s,N,a),t(s,Ca,a),m(Q,s,a),t(s,Xa,a),t(s,H,a),t(s,Ra,a),m(Z,s,a),t(s,Ya,a),m(D,s,a),t(s,Fa,a),t(s,E,a),t(s,Wa,a),m(S,s,a),t(s,Va,a),t(s,L,a),t(s,Ba,a),m(A,s,a),t(s,za,a),m(q,s,a),t(s,Na,a),t(s,K,a),t(s,Qa,a),t(s,P,a),t(s,Ha,a),m(O,s,a),t(s,Da,a),m(U,s,a),t(s,Ea,a),m(ss,s,a),t(s,Sa,a),t(s,as,a),t(s,La,a),t(s,es,a),t(s,Aa,a),m(ts,s,a),t(s,qa,a),m(ls,s,a),t(s,Ka,a),t(s,ns,a),t(s,Pa,a),t(s,ps,a),t(s,Oa,a),m(is,s,a),t(s,se,a),t(s,rs,a),t(s,ae,a),m(cs,s,a),t(s,ee,a),m(I,s,a),t(s,te,a),t(s,$a,a),t(s,le,a),m(hs,s,a),t(s,ne,a),t(s,ds,a),t(s,pe,a),t(s,os,a),t(s,ie,a),m(ms,s,a),t(s,re,a),t(s,fs,a),t(s,ce,a),m(us,s,a),t(s,he,a),t(s,gs,a),t(s,de,a),m(js,s,a),t(s,oe,a),t(s,Ms,a),t(s,me,a),m(bs,s,a),t(s,fe,a),t(s,ws,a),t(s,ue,a),m(ys,s,a),t(s,ge,a),t(s,Js,a),t(s,je,a),t(s,Ts,a),t(s,Me,a),m(xs,s,a),t(s,be,a),t(s,$s,a),t(s,we,a),m(_s,s,a),t(s,ye,a),t(s,ks,a),t(s,Je,a),t(s,Zs,a),t(s,Te,a),m(Us,s,a),t(s,xe,a),m(Is,s,a),t(s,$e,a),t(s,Gs,a),t(s,_e,a),m(vs,s,a),t(s,ke,a),m(Cs,s,a),t(s,Ze,a),t(s,Xs,a),t(s,Ue,a),m(Rs,s,a),t(s,Ie,a),m(G,s,a),t(s,Ge,a),t(s,Ys,a),t(s,ve,a),m(Fs,s,a),t(s,Ce,a),m(Ws,s,a),t(s,Xe,a),t(s,Vs,a),t(s,Re,a),t(s,Bs,a),t(s,Ye,a),t(s,zs,a),t(s,Fe,a),m(Ns,s,a),t(s,We,a),t(s,Qs,a),t(s,Ve,a),m(Hs,s,a),t(s,Be,a),t(s,Ds,a),t(s,ze,a),t(s,Es,a),t(s,Ne,a),m(Ss,s,a),t(s,Qe,a),m(Ls,s,a),t(s,He,a),t(s,As,a),t(s,De,a),m(qs,s,a),t(s,Ee,a),m(Ks,s,a),t(s,Se,a),m(v,s,a),t(s,Le,a),m(Ps,s,a),t(s,Ae,a),t(s,Os,a),t(s,qe,a),m(sa,s,a),t(s,Ke,a),t(s,aa,a),t(s,Pe,a),m(ea,s,a),t(s,Oe,a),m(ta,s,a),t(s,st,a),t(s,la,a),t(s,at,a),m(C,s,a),t(s,et,a),t(s,na,a),t(s,tt,a),m(pa,s,a),t(s,lt,a),t(s,ia,a),t(s,nt,a),t(s,ra,a),t(s,pt,a),m(ca,s,a),t(s,it,a),m(ha,s,a),t(s,rt,a),t(s,da,a),t(s,ct,a),m(X,s,a),t(s,ht,a),m(oa,s,a),t(s,dt,a),t(s,ma,a),t(s,ot,a),t(s,fa,a),t(s,mt,a),m(ua,s,a),t(s,ft,a),t(s,ga,a),t(s,ut,a),m(ja,s,a),t(s,gt,a),t(s,Ma,a),t(s,jt,a),t(s,ba,a),t(s,Mt,a),t(s,wa,a),t(s,bt,a),t(s,ya,a),t(s,wt,a),m(Ja,s,a),t(s,yt,a),m(R,s,a),t(s,Jt,a),m(Ta,s,a),t(s,Tt,a),t(s,_a,a),xt=!0},p(s,[a]){const xl={};a&2&&(xl.$$scope={dirty:a,ctx:s}),Z.$set(xl);const $l={};a&2&&($l.$$scope={dirty:a,ctx:s}),U.$set($l);const _l={};a&2&&(_l.$$scope={dirty:a,ctx:s}),I.$set(_l);const kl={};a&2&&(kl.$$scope={dirty:a,ctx:s}),G.$set(kl);const Zl={};a&2&&(Zl.$$scope={dirty:a,ctx:s}),v.$set(Zl);const Ul={};a&2&&(Ul.$$scope={dirty:a,ctx:s}),C.$set(Ul);const Il={};a&2&&(Il.$$scope={dirty:a,ctx:s}),X.$set(Il);const Gl={};a&2&&(Gl.$$scope={dirty:a,ctx:s}),R.$set(Gl)},i(s){xt||(f(T.$$.fragment,s),f(V.$$.fragment,s),f(Q.$$.fragment,s),f(Z.$$.fragment,s),f(D.$$.fragment,s),f(S.$$.fragment,s),f(A.$$.fragment,s),f(q.$$.fragment,s),f(O.$$.fragment,s),f(U.$$.fragment,s),f(ss.$$.fragment,s),f(ts.$$.fragment,s),f(ls.$$.fragment,s),f(is.$$.fragment,s),f(cs.$$.fragment,s),f(I.$$.fragment,s),f(hs.$$.fragment,s),f(ms.$$.fragment,s),f(us.$$.fragment,s),f(js.$$.fragment,s),f(bs.$$.fragment,s),f(ys.$$.fragment,s),f(xs.$$.fragment,s),f(_s.$$.fragment,s),f(Us.$$.fragment,s),f(Is.$$.fragment,s),f(vs.$$.fragment,s),f(Cs.$$.fragment,s),f(Rs.$$.fragment,s),f(G.$$.fragment,s),f(Fs.$$.fragment,s),f(Ws.$$.fragment,s),f(Ns.$$.fragment,s),f(Hs.$$.fragment,s),f(Ss.$$.fragment,s),f(Ls.$$.fragment,s),f(qs.$$.fragment,s),f(Ks.$$.fragment,s),f(v.$$.fragment,s),f(Ps.$$.fragment,s),f(sa.$$.fragment,s),f(ea.$$.fragment,s),f(ta.$$.fragment,s),f(C.$$.fragment,s),f(pa.$$.fragment,s),f(ca.$$.fragment,s),f(ha.$$.fragment,s),f(X.$$.fragment,s),f(oa.$$.fragment,s),f(ua.$$.fragment,s),f(ja.$$.fragment,s),f(Ja.$$.fragment,s),f(R.$$.fragment,s),f(Ta.$$.fragment,s),xt=!0)},o(s){u(T.$$.fragment,s),u(V.$$.fragment,s),u(Q.$$.fragment,s),u(Z.$$.fragment,s),u(D.$$.fragment,s),u(S.$$.fragment,s),u(A.$$.fragment,s),u(q.$$.fragment,s),u(O.$$.fragment,s),u(U.$$.fragment,s),u(ss.$$.fragment,s),u(ts.$$.fragment,s),u(ls.$$.fragment,s),u(is.$$.fragment,s),u(cs.$$.fragment,s),u(I.$$.fragment,s),u(hs.$$.fragment,s),u(ms.$$.fragment,s),u(us.$$.fragment,s),u(js.$$.fragment,s),u(bs.$$.fragment,s),u(ys.$$.fragment,s),u(xs.$$.fragment,s),u(_s.$$.fragment,s),u(Us.$$.fragment,s),u(Is.$$.fragment,s),u(vs.$$.fragment,s),u(Cs.$$.fragment,s),u(Rs.$$.fragment,s),u(G.$$.fragment,s),u(Fs.$$.fragment,s),u(Ws.$$.fragment,s),u(Ns.$$.fragment,s),u(Hs.$$.fragment,s),u(Ss.$$.fragment,s),u(Ls.$$.fragment,s),u(qs.$$.fragment,s),u(Ks.$$.fragment,s),u(v.$$.fragment,s),u(Ps.$$.fragment,s),u(sa.$$.fragment,s),u(ea.$$.fragment,s),u(ta.$$.fragment,s),u(C.$$.fragment,s),u(pa.$$.fragment,s),u(ca.$$.fragment,s),u(ha.$$.fragment,s),u(X.$$.fragment,s),u(oa.$$.fragment,s),u(ua.$$.fragment,s),u(ja.$$.fragment,s),u(Ja.$$.fragment,s),u(R.$$.fragment,s),u(Ta.$$.fragment,s),xt=!1},d(s){s&&(e(j),e(i),e(b),e(x),e($),e(_),e(F),e(ka),e(k),e(Za),e(W),e(Ua),e(Ia),e(B),e(Ga),e(z),e(va),e(N),e(Ca),e(Xa),e(H),e(Ra),e(Ya),e(Fa),e(E),e(Wa),e(Va),e(L),e(Ba),e(za),e(Na),e(K),e(Qa),e(P),e(Ha),e(Da),e(Ea),e(Sa),e(as),e(La),e(es),e(Aa),e(qa),e(Ka),e(ns),e(Pa),e(ps),e(Oa),e(se),e(rs),e(ae),e(ee),e(te),e($a),e(le),e(ne),e(ds),e(pe),e(os),e(ie),e(re),e(fs),e(ce),e(he),e(gs),e(de),e(oe),e(Ms),e(me),e(fe),e(ws),e(ue),e(ge),e(Js),e(je),e(Ts),e(Me),e(be),e($s),e(we),e(ye),e(ks),e(Je),e(Zs),e(Te),e(xe),e($e),e(Gs),e(_e),e(ke),e(Ze),e(Xs),e(Ue),e(Ie),e(Ge),e(Ys),e(ve),e(Ce),e(Xe),e(Vs),e(Re),e(Bs),e(Ye),e(zs),e(Fe),e(We),e(Qs),e(Ve),e(Be),e(Ds),e(ze),e(Es),e(Ne),e(Qe),e(He),e(As),e(De),e(Ee),e(Se),e(Le),e(Ae),e(Os),e(qe),e(Ke),e(aa),e(Pe),e(Oe),e(st),e(la),e(at),e(et),e(na),e(tt),e(lt),e(ia),e(nt),e(ra),e(pt),e(it),e(rt),e(da),e(ct),e(ht),e(dt),e(ma),e(ot),e(fa),e(mt),e(ft),e(ga),e(ut),e(gt),e(Ma),e(jt),e(ba),e(Mt),e(wa),e(bt),e(ya),e(wt),e(yt),e(Jt),e(Tt),e(_a)),e(p),g(T,s),g(V,s),g(Q,s),g(Z,s),g(D,s),g(S,s),g(A,s),g(q,s),g(O,s),g(U,s),g(ss,s),g(ts,s),g(ls,s),g(is,s),g(cs,s),g(I,s),g(hs,s),g(ms,s),g(us,s),g(js,s),g(bs,s),g(ys,s),g(xs,s),g(_s,s),g(Us,s),g(Is,s),g(vs,s),g(Cs,s),g(Rs,s),g(G,s),g(Fs,s),g(Ws,s),g(Ns,s),g(Hs,s),g(Ss,s),g(Ls,s),g(qs,s),g(Ks,s),g(v,s),g(Ps,s),g(sa,s),g(ea,s),g(ta,s),g(C,s),g(pa,s),g(ca,s),g(ha,s),g(X,s),g(oa,s),g(ua,s),g(ja,s),g(Ja,s),g(R,s),g(Ta,s)}}}const Kl='{"title":"Stream","local":"stream","sections":[{"title":"Convert from a Dataset","local":"convert-from-a-dataset","sections":[],"depth":2},{"title":"Shuffle","local":"shuffle","sections":[],"depth":2},{"title":"Reshuffle","local":"reshuffle","sections":[],"depth":2},{"title":"Split dataset","local":"split-dataset","sections":[{"title":"Shard","local":"shard","sections":[],"depth":3}],"depth":2},{"title":"Interleave","local":"interleave","sections":[],"depth":2},{"title":"Rename, remove, and cast","local":"rename-remove-and-cast","sections":[{"title":"Rename","local":"rename","sections":[],"depth":3},{"title":"Remove","local":"remove","sections":[],"depth":3},{"title":"Cast","local":"cast","sections":[],"depth":3}],"depth":2},{"title":"Map","local":"map","sections":[{"title":"Batch processing","local":"batch-processing","sections":[{"title":"Tokenization","local":"tokenization","sections":[],"depth":4}],"depth":3},{"title":"Filter","local":"filter","sections":[],"depth":3}],"depth":2},{"title":"Batch","local":"batch","sections":[],"depth":2},{"title":"Stream in a training loop","local":"stream-in-a-training-loop","sections":[{"title":"Save a dataset checkpoint and resume iteration","local":"save-a-dataset-checkpoint-and-resume-iteration","sections":[],"depth":3}],"depth":2}],"depth":1}';function Pl(y){return Cl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nn extends Xl{constructor(p){super(),Rl(this,p,Pl,ql,vl,{})}}export{nn as component};
