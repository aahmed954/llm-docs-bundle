import{s as O,n as Q,o as q}from"../chunks/scheduler.362310b7.js";import{S as K,i as ee,g as i,s as a,r as M,A as te,h as m,f as l,c as s,j as F,u as $,x as v,k as Y,y as le,a as n,v as J,d as Z,t as U,w as I}from"../chunks/index.57dfc70d.js";import{C as A}from"../chunks/CodeBlock.5d40996c.js";import{H as ne,E as ae}from"../chunks/index.fa158b42.js";function se(P){let o,L,y,G,r,_,c,X='TGI supports various LLM architectures (see full list <a href="../supported_models">here</a>). If you wish to serve a model that is not one of the supported models, TGI will fallback to the <code>transformers</code> implementation of that model. This means you will be unable to use some of the features introduced by TGI, such as tensor-parallel sharding or flash attention. However, you can still get many benefits of TGI, such as continuous batching or streaming outputs.',C,u,B="You can serve these models using the same Docker command-line invocation as with fully supported models ðŸ‘‡",H,d,N,p,D="If the model you wish to serve is a custom transformers model, and its weights and implementation are available in the Hub, you can still serve the model by passing the <code>--trust-remote-code</code> flag to the <code>docker run</code> command like below ðŸ‘‡",R,f,S,h,W="Finally, if the model is not on Hugging Face Hub but on your local, you can pass the path to the folder that contains your model like below ðŸ‘‡",V,g,k,b,E='You can refer to <a href="https://huggingface.co/docs/transformers/main/en/custom_models" rel="nofollow">transformers docs on custom models</a> for more information.',j,T,x,w,z;return r=new ne({props:{title:"Non-core Model Serving",local:"non-core-model-serving",headingTag:"h1"}}),d=new A({props:{code:"ZG9ja2VyJTIwcnVuJTIwLS1ncHVzJTIwYWxsJTIwLS1zaG0tc2l6ZSUyMDFnJTIwLXAlMjA4MDgwJTNBODAlMjAtdiUyMCUyNHZvbHVtZSUzQSUyRmRhdGElMjBnaGNyLmlvJTJGaHVnZ2luZ2ZhY2UlMkZ0ZXh0LWdlbmVyYXRpb24taW5mZXJlbmNlJTNBbGF0ZXN0JTIwLS1tb2RlbC1pZCUyMGdwdDI=",highlighted:'docker run --gpus all --shm-size 1g -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data ghcr.io/huggingface/text-generation-inference:latest --model-id gpt2',wrap:!1}}),f=new A({props:{code:"ZG9ja2VyJTIwcnVuJTIwLS1ncHVzJTIwYWxsJTIwLS1zaG0tc2l6ZSUyMDFnJTIwLXAlMjA4MDgwJTNBODAlMjAtdiUyMCUyNHZvbHVtZSUzQSUyRmRhdGElMjBnaGNyLmlvJTJGaHVnZ2luZ2ZhY2UlMkZ0ZXh0LWdlbmVyYXRpb24taW5mZXJlbmNlJTNBbGF0ZXN0JTIwLS1tb2RlbC1pZCUyMCUzQ0NVU1RPTV9NT0RFTF9JRCUzRSUyMC0tdHJ1c3QtcmVtb3RlLWNvZGU=",highlighted:'docker run --gpus all --shm-size 1g -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data ghcr.io/huggingface/text-generation-inference:latest --model-id &lt;CUSTOM_MODEL_ID&gt; --trust-remote-code',wrap:!1}}),g=new A({props:{code:"JTIzJTIwTWFrZSUyMHN1cmUlMjB5b3VyJTIwbW9kZWwlMjBpcyUyMGluJTIwdGhlJTIwJTI0dm9sdW1lJTIwZGlyZWN0b3J5JTBBZG9ja2VyJTIwcnVuJTIwLS1zaG0tc2l6ZSUyMDFnJTIwLXAlMjA4MDgwJTNBODAlMjAtdiUyMCUyNHZvbHVtZSUzQSUyRmRhdGElMjAlMjBnaGNyLmlvJTJGaHVnZ2luZ2ZhY2UlMkZ0ZXh0LWdlbmVyYXRpb24taW5mZXJlbmNlJTNBbGF0ZXN0JTIwLS1tb2RlbC1pZCUyMCUyRmRhdGElMkYlM0NQQVRILVRPLUZPTERFUiUzRQ==",highlighted:`<span class="hljs-comment"># Make sure your model is in the $volume directory</span>
docker run --shm-size 1g -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data  ghcr.io/huggingface/text-generation-inference:latest --model-id /data/&lt;PATH-TO-FOLDER&gt;`,wrap:!1}}),T=new ae({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/basic_tutorials/non_core_models.md"}}),{c(){o=i("meta"),L=a(),y=i("p"),G=a(),M(r.$$.fragment),_=a(),c=i("p"),c.innerHTML=X,C=a(),u=i("p"),u.textContent=B,H=a(),M(d.$$.fragment),N=a(),p=i("p"),p.innerHTML=D,R=a(),M(f.$$.fragment),S=a(),h=i("p"),h.textContent=W,V=a(),M(g.$$.fragment),k=a(),b=i("p"),b.innerHTML=E,j=a(),M(T.$$.fragment),x=a(),w=i("p"),this.h()},l(e){const t=te("svelte-u9bgzb",document.head);o=m(t,"META",{name:!0,content:!0}),t.forEach(l),L=s(e),y=m(e,"P",{}),F(y).forEach(l),G=s(e),$(r.$$.fragment,e),_=s(e),c=m(e,"P",{"data-svelte-h":!0}),v(c)!=="svelte-1kygkxg"&&(c.innerHTML=X),C=s(e),u=m(e,"P",{"data-svelte-h":!0}),v(u)!=="svelte-1ytiiws"&&(u.textContent=B),H=s(e),$(d.$$.fragment,e),N=s(e),p=m(e,"P",{"data-svelte-h":!0}),v(p)!=="svelte-13d3ogn"&&(p.innerHTML=D),R=s(e),$(f.$$.fragment,e),S=s(e),h=m(e,"P",{"data-svelte-h":!0}),v(h)!=="svelte-1abpc8h"&&(h.textContent=W),V=s(e),$(g.$$.fragment,e),k=s(e),b=m(e,"P",{"data-svelte-h":!0}),v(b)!=="svelte-51bagv"&&(b.innerHTML=E),j=s(e),$(T.$$.fragment,e),x=s(e),w=m(e,"P",{}),F(w).forEach(l),this.h()},h(){Y(o,"name","hf:doc:metadata"),Y(o,"content",oe)},m(e,t){le(document.head,o),n(e,L,t),n(e,y,t),n(e,G,t),J(r,e,t),n(e,_,t),n(e,c,t),n(e,C,t),n(e,u,t),n(e,H,t),J(d,e,t),n(e,N,t),n(e,p,t),n(e,R,t),J(f,e,t),n(e,S,t),n(e,h,t),n(e,V,t),J(g,e,t),n(e,k,t),n(e,b,t),n(e,j,t),J(T,e,t),n(e,x,t),n(e,w,t),z=!0},p:Q,i(e){z||(Z(r.$$.fragment,e),Z(d.$$.fragment,e),Z(f.$$.fragment,e),Z(g.$$.fragment,e),Z(T.$$.fragment,e),z=!0)},o(e){U(r.$$.fragment,e),U(d.$$.fragment,e),U(f.$$.fragment,e),U(g.$$.fragment,e),U(T.$$.fragment,e),z=!1},d(e){e&&(l(L),l(y),l(G),l(_),l(c),l(C),l(u),l(H),l(N),l(p),l(R),l(S),l(h),l(V),l(k),l(b),l(j),l(x),l(w)),l(o),I(r,e),I(d,e),I(f,e),I(g,e),I(T,e)}}}const oe='{"title":"Non-core Model Serving","local":"non-core-model-serving","sections":[],"depth":1}';function ie(P){return q(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class de extends K{constructor(o){super(),ee(this,o,ie,se,O,{})}}export{de as component};
