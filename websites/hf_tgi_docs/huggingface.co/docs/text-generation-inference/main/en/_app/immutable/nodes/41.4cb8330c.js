import{s as H,n as R,o as A}from"../chunks/scheduler.362310b7.js";import{S as N,i as X,g as u,s as o,r as G,A as D,h as d,f as t,c as i,j as W,u as J,x as k,k as S,y as O,a,v as B,d as j,t as F,w as P}from"../chunks/index.57dfc70d.js";import{C as q}from"../chunks/CodeBlock.5d40996c.js";import{H as K,E as Y}from"../chunks/index.fa158b42.js";function ee(Q){let n,M,w,x,s,v,c,Z="Text Generation Inference enables serving optimized models. The following sections list which models (VLMs & LLMs) are supported.",$,f,z='<li><a href="https://huggingface.co/deepseek-ai/DeepSeek-V2" rel="nofollow">Deepseek V2</a></li> <li><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3" rel="nofollow">Deepseek V3</a></li> <li><a href="https://huggingface.co/HuggingFaceM4/idefics2-8b" rel="nofollow">Idefics 2</a> (Multimodal)</li> <li><a href="https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3" rel="nofollow">Idefics 3</a> (Multimodal)</li> <li><a href="https://huggingface.co/llava-hf/llava-v1.6-vicuna-13b-hf" rel="nofollow">Llava Next (1.6)</a> (Multimodal)</li> <li><a href="https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f" rel="nofollow">Llama</a></li> <li><a href="https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f" rel="nofollow">Llama4</a></li> <li><a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct" rel="nofollow">Phi 3</a></li> <li><a href="https://huggingface.co/ibm-granite/granite-3.0-8b-instruct" rel="nofollow">Granite</a></li> <li><a href="https://huggingface.co/google/gemma-7b" rel="nofollow">Gemma</a></li> <li><a href="https://huggingface.co/google/paligemma-3b-pt-224" rel="nofollow">PaliGemma</a></li> <li><a href="https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315" rel="nofollow">Gemma2</a></li> <li><a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d" rel="nofollow">Gemma3</a></li> <li><a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d" rel="nofollow">Gemma3 Text</a></li> <li><a href="https://huggingface.co/CohereForAI/c4ai-command-r-plus" rel="nofollow">Cohere</a></li> <li><a href="https://huggingface.co/databricks/dbrx-instruct" rel="nofollow">Dbrx</a></li> <li><a href="https://huggingface.co/state-spaces/mamba-2.8b-slimpj" rel="nofollow">Mamba</a></li> <li><a href="https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407" rel="nofollow">Mistral</a></li> <li><a href="https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1" rel="nofollow">Mixtral</a></li> <li><a href="https://huggingface.co/bigcode/gpt_bigcode-santacoder" rel="nofollow">Gpt Bigcode</a></li> <li><a href="https://huggingface.co/microsoft/phi-1_5" rel="nofollow">Phi</a></li> <li><a href="https://huggingface.co/microsoft/Phi-3.5-MoE-instruct" rel="nofollow">PhiMoe</a></li> <li><a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat" rel="nofollow">Baichuan</a></li> <li><a href="https://huggingface.co/tiiuae/falcon-7b-instruct" rel="nofollow">Falcon</a></li> <li><a href="https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1" rel="nofollow">StarCoder 2</a></li> <li><a href="https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f" rel="nofollow">Qwen 2</a></li> <li><a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d" rel="nofollow">Qwen 2 VL</a></li> <li><a href="https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e" rel="nofollow">Qwen 2.5 VL</a></li> <li><a href="https://huggingface.co/facebook/opt-6.7b" rel="nofollow">Opt</a></li> <li><a href="https://huggingface.co/google/flan-t5-xxl" rel="nofollow">T5</a></li> <li><a href="https://huggingface.co/facebook/galactica-120b" rel="nofollow">Galactica</a></li> <li><a href="https://huggingface.co/bigcode/santacoder" rel="nofollow">SantaCoder</a></li> <li><a href="https://huggingface.co/bigscience/bloom-560m" rel="nofollow">Bloom</a></li> <li><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" rel="nofollow">Mpt</a></li> <li><a href="https://huggingface.co/openai-community/gpt2" rel="nofollow">Gpt2</a></li> <li><a href="https://huggingface.co/EleutherAI/gpt-neox-20b" rel="nofollow">Gpt Neox</a></li> <li><a href="https://huggingface.co/EleutherAI/gpt-j-6b" rel="nofollow">Gptj</a></li> <li><a href="https://huggingface.co/HuggingFaceM4/idefics-9b" rel="nofollow">Idefics</a> (Multimodal)</li> <li><a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct" rel="nofollow">Mllama</a> (Multimodal)</li>',T,r,E="If the above list lacks the model you would like to serve, depending on the model’s pipeline type, you can try to initialize and serve the model anyways to see how well it performs, but performance isn’t guaranteed for non-optimized models:",C,g,I,h,U="If you wish to serve a supported model that already exists on a local folder, just point to the local folder.",_,m,y,p,L,b,V;return s=new K({props:{title:"Supported Models",local:"supported-models",headingTag:"h1"}}),g=new q({props:{code:"JTIzJTIwZm9yJTIwY2F1c2FsJTIwTE1zJTJGdGV4dC1nZW5lcmF0aW9uJTIwbW9kZWxzJTBBQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUzQ21vZGVsJTNFJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBJTIzJTIwb3IlMkMlMjBmb3IlMjB0ZXh0LXRvLXRleHQlMjBnZW5lcmF0aW9uJTIwbW9kZWxzJTBBQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglM0Ntb2RlbCUzRSUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyKQ==",highlighted:`<span class="hljs-comment"># for causal LMs/text-generation models</span>
AutoModelForCausalLM.from_pretrained(&lt;model&gt;, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-comment"># or, for text-to-text generation models</span>
AutoModelForSeq2SeqLM.from_pretrained(&lt;model&gt;, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),m=new q({props:{code:"dGV4dC1nZW5lcmF0aW9uLWxhdW5jaGVyJTIwLS1tb2RlbC1pZCUyMCUzQ1BBVEgtVE8tTE9DQUwtQkxPT00lM0U=",highlighted:"text-generation-launcher --model-id &lt;PATH-TO-LOCAL-BLOOM&gt;",wrap:!1}}),p=new Y({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/supported_models.md"}}),{c(){n=u("meta"),M=o(),w=u("p"),x=o(),G(s.$$.fragment),v=o(),c=u("p"),c.textContent=Z,$=o(),f=u("ul"),f.innerHTML=z,T=o(),r=u("p"),r.textContent=E,C=o(),G(g.$$.fragment),I=o(),h=u("p"),h.textContent=U,_=o(),G(m.$$.fragment),y=o(),G(p.$$.fragment),L=o(),b=u("p"),this.h()},l(e){const l=D("svelte-u9bgzb",document.head);n=d(l,"META",{name:!0,content:!0}),l.forEach(t),M=i(e),w=d(e,"P",{}),W(w).forEach(t),x=i(e),J(s.$$.fragment,e),v=i(e),c=d(e,"P",{"data-svelte-h":!0}),k(c)!=="svelte-14yd0k9"&&(c.textContent=Z),$=i(e),f=d(e,"UL",{"data-svelte-h":!0}),k(f)!=="svelte-1fhw5xf"&&(f.innerHTML=z),T=i(e),r=d(e,"P",{"data-svelte-h":!0}),k(r)!=="svelte-qvuhj"&&(r.textContent=E),C=i(e),J(g.$$.fragment,e),I=i(e),h=d(e,"P",{"data-svelte-h":!0}),k(h)!=="svelte-1ujziby"&&(h.textContent=U),_=i(e),J(m.$$.fragment,e),y=i(e),J(p.$$.fragment,e),L=i(e),b=d(e,"P",{}),W(b).forEach(t),this.h()},h(){S(n,"name","hf:doc:metadata"),S(n,"content",le)},m(e,l){O(document.head,n),a(e,M,l),a(e,w,l),a(e,x,l),B(s,e,l),a(e,v,l),a(e,c,l),a(e,$,l),a(e,f,l),a(e,T,l),a(e,r,l),a(e,C,l),B(g,e,l),a(e,I,l),a(e,h,l),a(e,_,l),B(m,e,l),a(e,y,l),B(p,e,l),a(e,L,l),a(e,b,l),V=!0},p:R,i(e){V||(j(s.$$.fragment,e),j(g.$$.fragment,e),j(m.$$.fragment,e),j(p.$$.fragment,e),V=!0)},o(e){F(s.$$.fragment,e),F(g.$$.fragment,e),F(m.$$.fragment,e),F(p.$$.fragment,e),V=!1},d(e){e&&(t(M),t(w),t(x),t(v),t(c),t($),t(f),t(T),t(r),t(C),t(I),t(h),t(_),t(y),t(L),t(b)),t(n),P(s,e),P(g,e),P(m,e),P(p,e)}}}const le='{"title":"Supported Models","local":"supported-models","sections":[],"depth":1}';function te(Q){return A(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class se extends N{constructor(n){super(),X(this,n,te,ee,H,{})}}export{se as component};
