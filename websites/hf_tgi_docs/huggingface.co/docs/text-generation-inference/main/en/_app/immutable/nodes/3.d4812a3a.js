import{s as Oa,o as Da}from"../chunks/scheduler.362310b7.js";import{S as qa,i as Ka,g as s,s as n,r as d,A as en,h as o,f as l,c as i,j as Xa,u as m,x as r,k as za,y as tn,a,v as u,d as f,t as p,w as h,m as ln,n as an}from"../chunks/index.57dfc70d.js";import{T as nn}from"../chunks/Tip.14b2ab21.js";import{C as T}from"../chunks/CodeBlock.5d40996c.js";import{H as c,E as sn}from"../chunks/index.fa158b42.js";function on(st){let M;return{c(){M=ln("We recommend always using sharding when running on a multi-card machine.")},l(g){M=an(g,"We recommend always using sharding when running on a multi-card machine.")},m(g,U){a(g,M,U)},d(g){g&&l(M)}}}function rn(st){let M,g,U,ot,b,rt,y,dt,C,Xl="Text Generation Inference (TGI) has been optimized to run on Gaudi hardware via the Gaudi backend for TGI.",mt,J,ut,$,zl='<li><strong>Gaudi1</strong>: Available on <a href="https://aws.amazon.com/ec2/instance-types/dl1/" rel="nofollow">AWS EC2 DL1 instances</a></li> <li><strong>Gaudi2</strong>: Available on <a href="https://console.cloud.intel.com/docs/reference/ai_instances.html" rel="nofollow">Intel Cloud</a></li> <li><strong>Gaudi3</strong>: Available on <a href="https://console.cloud.intel.com/docs/reference/ai_instances.html" rel="nofollow">Intel Cloud</a></li>',ft,v,pt,I,ht,j,Ol="The easiest way to run TGI on Gaudi is to use the official Docker image:",ct,E,Mt,L,Dl="Once you see the <code>connected</code> log, the server is ready to accept requests:",Tt,x,ql="<p>2024-05-22T19:31:48.302239Z  INFO text_generation_router: router/src/main.rs:378: Connected</p>",gt,_,Kl='You can find your <code>YOUR_HF_ACCESS_TOKEN</code> at <a href="https://huggingface.co/settings/tokens" rel="nofollow">https://huggingface.co/settings/tokens</a>. This is necessary to access gated models like llama3.1.',wt,R,Ut,B,ea="You can send a request from a separate terminal:",bt,k,yt,W,Ct,S,ta='You can view the full list of supported models in the <a href="https://huggingface.co/docs/text-generation-inference/backends/gaudi#supported-models" rel="nofollow">Supported Models</a> section.',Jt,A,la="For example, to run Llama3.1-8B, you can use the following command:",$t,G,vt,Z,aa='For the full list of service parameters, refer to the <a href="https://huggingface.co/docs/text-generation-inference/reference/launcher" rel="nofollow">launcher-arguments page</a>.',It,V,na='The validated docker commands can be found in the <a href="https://github.com/huggingface/text-generation-inference/tree/main/backends/gaudi/examples/docker_commands" rel="nofollow">examples/docker_commands folder</a>.',jt,F,ia='<p>Note: <code>--runtime=habana --cap-add=sys_nice --ipc=host</code> is required to enable docker to use the Gaudi hardware (more details <a href="https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/Docker_Installation.html" rel="nofollow">here</a>).</p>',Et,N,Lt,H,sa="TGI-Gaudi supports sharding for multi-card inference, allowing you to distribute the load across multiple Gaudi cards. This is recommended to run large models and to speed up inference.",xt,Q,oa="For example, on a machine with 8 Gaudi cards, you can run:",_t,P,Rt,w,Bt,Y,kt,X,Wt,z,ra="By default, all models run with BF16 precision on Gaudi hardware.",St,O,At,D,da='TGI-Gaudi supports FP8 precision inference with <a href="https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_FP8.html" rel="nofollow">Intel Neural Compressor (INC)</a>.',Gt,q,ma="To run FP8 Inference:",Zt,K,ua='<li>Measure statistics using <a href="https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation#running-with-fp8" rel="nofollow">Optimum Habana measurement script</a></li> <li>Run the model in TGI with QUANT_CONFIG setting - e.g. <code>-e QUANT_CONFIG=./quantization_config/maxabs_quant.json</code>.</li>',Vt,ee,fa="The following commmand example for FP8 inference is based on the assumption that measurement is done via the first step above.",Ft,te,pa="Example for Llama3.1-70B on 8 cards with FP8 precision:",Nt,le,Ht,ae,Qt,ne,ha="Gaudi supports VLM inference.",Pt,ie,ca="Example for Llava-v1.6-Mistral-7B on 1 card:",Yt,se,Ma="Start the TGI server via the following command:",Xt,oe,zt,re,Ta="You can then send a request to the server via the following command:",Ot,de,Dt,me,ga="<p>Note: In Llava-v1.6-Mistral-7B, an image usually accounts for 2000 input tokens. For example, an image of size 512x512 is represented by 2800 tokens. Thus, <code>max-input-tokens</code> must be larger than the number of tokens associated with the image. Otherwise the image may be truncated. We set <code>BASE_IMAGE_TOKENS=2048</code> as the default image token value. This is the minimum value of <code>max-input-tokens</code>. You can override the environment variable <code>BASE_IMAGE_TOKENS</code> to change this value. The warmup will generate graphs with input length from <code>BASE_IMAGE_TOKENS</code> to <code>max-input-tokens</code>. For Llava-v1.6-Mistral-7B, the value of <code>max-batch-prefill-tokens</code> is 16384, which is calcualted as follows: <code>prefill_batch_size</code> = <code>max-batch-prefill-tokens</code> / <code>max-input-tokens</code>.</p>",qt,ue,Kt,fe,wa='We recommend using the <a href="https://github.com/huggingface/inference-benchmarker" rel="nofollow">inference-benchmarker tool</a> to benchmark performance on Gaudi hardware.',el,pe,Ua="This benchmark tool simulates user requests and measures the performance of the model on realistic scenarios.",tl,he,ba="To run it on the same machine, you can do the following:",ll,ce,al,Me,ya='Please refer to the <a href="https://github.com/huggingface/inference-benchmarker" rel="nofollow">inference-benchmarker README</a> for more details.',nl,Te,il,ge,Ca="To collect performance profiling, you need to set the following environment variables:",sl,we,Ja='<thead><tr><th>Name</th> <th align="left">Value(s)</th> <th align="left">Default</th> <th align="left">Description</th></tr></thead> <tbody><tr><td>PROF_WAITSTEP</td> <td align="left">integer</td> <td align="left">0</td> <td align="left">Control profile wait steps</td></tr> <tr><td>PROF_WARMUPSTEP</td> <td align="left">integer</td> <td align="left">0</td> <td align="left">Control profile warmup steps</td></tr> <tr><td>PROF_STEP</td> <td align="left">integer</td> <td align="left">0</td> <td align="left">Enable/disable profile, control profile active steps</td></tr> <tr><td>PROF_PATH</td> <td align="left">string</td> <td align="left">/tmp/hpu_profile</td> <td align="left">Define profile folder</td></tr> <tr><td>PROF_RANKS</td> <td align="left">string</td> <td align="left">0</td> <td align="left">Comma-separated list of ranks to profile</td></tr> <tr><td>PROF_RECORD_SHAPES</td> <td align="left">True/False</td> <td align="left">False</td> <td align="left">Control record_shapes option in the profiler</td></tr></tbody>',ol,Ue,$a="To use these environment variables, add them to your docker run command with the -e flag. For example:",rl,be,dl,ye,ml,Ce,ul,Je,va="To ensure optimal performance, warmup is performed at the beginning of each server run. This process creates queries with various input shapes based on provided parameters and runs basic TGI operations (prefill, decode, concatenate).",fl,$e,Ia='Note: Model warmup can take several minutes, especially for FP8 inference. For faster subsequent runs, refer to <a href="https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Optimization_in_PyTorch_Models.html#disk-caching-eviction-policy" rel="nofollow">Disk Caching Eviction Policy</a>.',pl,ve,hl,Ie,cl,je,ja="<li><code>--max-input-tokens</code> is the maximum possible input prompt length. Default value is <code>4095</code>.</li> <li><code>--max-total-tokens</code> is the maximum possible total length of the sequence (input and output). Default value is <code>4096</code>.</li>",Ml,Ee,Tl,Le,Ea="<li>For prefill operation, please set <code>--max-batch-prefill-tokens</code> as <code>bs * max-input-tokens</code>, where <code>bs</code> is your expected maximum prefill batch size.</li> <li>For decode operation, please set <code>--max-batch-size</code> as <code>bs</code>, where <code>bs</code> is your expected maximum decode batch size.</li> <li>Please note that batch size will be always padded to the nearest multiplication of <code>BATCH_BUCKET_SIZE</code> and <code>PREFILL_BATCH_BUCKET_SIZE</code>.</li>",gl,xe,wl,_e,La="<li><code>PAD_SEQUENCE_TO_MULTIPLE_OF</code> determines sizes of input length buckets. Since warmup creates several graphs for each bucket, itâ€™s important to adjust that value proportionally to input sequence length. Otherwise, some out of memory issues can be observed.</li> <li><code>ENABLE_HPU_GRAPH</code> enables HPU graphs usage, which is crucial for performance results. Recommended value to keep is <code>true</code>.</li>",Ul,Re,bl,Be,xa="<li><code>--max-input-tokens</code>: Maximum possible input prompt length (default: 4095)</li> <li><code>--max-total-tokens</code>: Maximum possible total sequence length (input + output) (default: 4096)</li>",yl,ke,Cl,We,_a="<li><code>--max-batch-prefill-tokens</code>: Set as <code>bs * max-input-tokens</code> where <code>bs</code> is your expected maximum prefill batch size</li> <li><code>--max-batch-size</code>: Set as <code>bs</code> where <code>bs</code> is your expected maximum decode batch size</li> <li>Note: Batch sizes are padded to the nearest multiple of <code>BATCH_BUCKET_SIZE</code> and <code>PREFILL_BATCH_BUCKET_SIZE</code></li>",Jl,Se,$l,Ae,Ra="This section contains reference information about the Gaudi backend.",vl,Ge,Il,Ze,Ba="Text Generation Inference enables serving optimized models on Gaudi hardware. The following sections list which models (VLMs & LLMs) are supported on Gaudi.",jl,Ve,ka="<strong>Large Language Models (LLMs)</strong>",El,Fe,Wa='<li><a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" rel="nofollow">Llama2-7B</a></li> <li><a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf" rel="nofollow">Llama2-70B</a></li> <li><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct" rel="nofollow">Llama3-8B</a></li> <li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct" rel="nofollow">Llama3-70B</a></li> <li><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct" rel="nofollow">LLama3.1-8B</a></li> <li><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct" rel="nofollow">LLama3.1-70B</a></li> <li><a href="https://huggingface.co/codellama/CodeLlama-13b-hf" rel="nofollow">CodeLlama-13B</a></li> <li><a href="https://huggingface.co/facebook/opt-125m" rel="nofollow">Opt-125m</a></li> <li><a href="https://huggingface.co/openai-community/gpt2" rel="nofollow">OpenAI-gpt2</a></li> <li><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" rel="nofollow">Mixtral-8x7B</a></li> <li><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3" rel="nofollow">Mistral-7B</a></li> <li><a href="https://huggingface.co/Qwen/Qwen2-72B-Instruct" rel="nofollow">Qwen2-72B</a></li> <li><a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" rel="nofollow">Qwen2-7B</a></li> <li><a href="https://huggingface.co/microsoft/phi-1_5" rel="nofollow">Phi-1.5</a></li> <li><a href="https://huggingface.co/google/gemma-7b-it" rel="nofollow">Gemma-7b</a></li> <li><a href="https://huggingface.co/bigcode/starcoder2-3b" rel="nofollow">Starcoder2-3b</a></li> <li><a href="https://huggingface.co/bigcode/starcoder2-15b" rel="nofollow">Starcoder2-15b</a></li> <li><a href="https://huggingface.co/bigcode/starcoder" rel="nofollow">Starcoder</a></li> <li><a href="https://huggingface.co/tiiuae/falcon-7b-instruct" rel="nofollow">falcon-7b-instruct</a></li> <li><a href="https://huggingface.co/tiiuae/falcon-180B-chat" rel="nofollow">Falcon-180B</a></li> <li><a href="https://huggingface.co/openai-community/gpt2" rel="nofollow">GPT-2</a></li> <li><a href="https://huggingface.co/EleutherAI/gpt-j-6b" rel="nofollow">gpt-j-6b</a></li>',Ll,Ne,Sa="<strong>Vision-Language Models (VLMs)</strong>",xl,He,Aa='<li><a href="https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf" rel="nofollow">LLaVA-v1.6-Mistral-7B</a></li> <li><a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct" rel="nofollow">Mllama (Multimodal Llama from Meta)</a></li> <li><a href="https://huggingface.co/HuggingFaceM4/idefics-9b" rel="nofollow">Idefics</a></li> <li><a href="https://huggingface.co/HuggingFaceM4/idefics2-8b" rel="nofollow">Idefics 2</a></li> <li><a href="https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3" rel="nofollow">Idefics 2.5</a></li> <li><a href="https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct" rel="nofollow">Qwen2-VL-2B-Instruct</a></li> <li><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct" rel="nofollow">Qwen/Qwen2.5-VL-7B-Instruct</a></li>',_l,Qe,Ga='We also support on a best effort basis models with different parameters count that use the same model architecture but those models were not tested. For example, the gaudi backend supports <code>meta-llama/Llama-3.2-1B</code> as the architecture is the standard llama3 architecture. If you have an issue with a model, please open an issue on the <a href="https://github.com/huggingface/text-generation-inference/issues" rel="nofollow">Gaudi backend repository</a>.',Rl,Pe,Bl,Ye,Za="The following table contains the environment variables that can be used to configure the Gaudi backend:",kl,Xe,Va='<thead><tr><th>Name</th> <th align="left">Value(s)</th> <th align="left">Default</th> <th align="left">Description</th> <th align="left">Usage</th></tr></thead> <tbody><tr><td>ENABLE_HPU_GRAPH</td> <td align="left">True/False</td> <td align="left">True</td> <td align="left">Enable hpu graph or not</td> <td align="left">add -e in docker run command</td></tr> <tr><td>LIMIT_HPU_GRAPH</td> <td align="left">True/False</td> <td align="left">True</td> <td align="left">Skip HPU graph usage for prefill to save memory, set to <code>True</code> for large sequence/decoding lengths(e.g. 300/212)</td> <td align="left">add -e in docker run command</td></tr> <tr><td>BATCH_BUCKET_SIZE</td> <td align="left">integer</td> <td align="left">8</td> <td align="left">Batch size for decode operation will be rounded to the nearest multiple of this number. This limits the number of cached graphs</td> <td align="left">add -e in docker run command</td></tr> <tr><td>PREFILL_BATCH_BUCKET_SIZE</td> <td align="left">integer</td> <td align="left">4</td> <td align="left">Batch size for prefill operation will be rounded to the nearest multiple of this number. This limits the number of cached graphs</td> <td align="left">add -e in docker run command</td></tr> <tr><td>PAD_SEQUENCE_TO_MULTIPLE_OF</td> <td align="left">integer</td> <td align="left">128</td> <td align="left">For prefill operation, sequences will be padded to a multiple of provided value.</td> <td align="left">add -e in docker run command</td></tr> <tr><td>SKIP_TOKENIZER_IN_TGI</td> <td align="left">True/False</td> <td align="left">False</td> <td align="left">Skip tokenizer for input/output processing</td> <td align="left">add -e in docker run command</td></tr> <tr><td>WARMUP_ENABLED</td> <td align="left">True/False</td> <td align="left">True</td> <td align="left">Enable warmup during server initialization to recompile all graphs. This can increase TGI setup time.</td> <td align="left">add -e in docker run command</td></tr> <tr><td>QUEUE_THRESHOLD_MS</td> <td align="left">integer</td> <td align="left">120</td> <td align="left">Controls the threshold beyond which the request are considered overdue and handled with priority. Shorter requests are prioritized otherwise.</td> <td align="left">add -e in docker run command</td></tr> <tr><td>USE_FLASH_ATTENTION</td> <td align="left">True/False</td> <td align="left">True</td> <td align="left">Whether to enable Habana Flash Attention, provided that the model supports it. Please refer to <a href="https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Optimization_in_PyTorch_Models.html?highlight=fusedsdpa#using-fused-scaled-dot-product-attention-fusedsdpa" rel="nofollow">https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Optimization_in_PyTorch_Models.html?highlight=fusedsdpa#using-fused-scaled-dot-product-attention-fusedsdpa</a></td> <td align="left">add -e in docker run command</td></tr> <tr><td>FLASH_ATTENTION_RECOMPUTE</td> <td align="left">True/False</td> <td align="left">True</td> <td align="left">Whether to enable Habana Flash Attention in recompute mode on first token generation.</td> <td align="left">add -e in docker run command</td></tr></tbody>',Wl,ze,Sl,Oe,Fa='Contributions to the TGI-Gaudi project are welcome. Please refer to the <a href="https://github.com/huggingface/text-generation-inference/blob/main/CONTRIBUTING.md" rel="nofollow">contributing guide</a>.',Al,De,Na="<strong>Guidelines for contributing to Gaudi on TGI:</strong> All changes should be made within the <code>backends/gaudi</code> folder. In general, you should avoid modifying the router, launcher, or benchmark to accommodate Gaudi hardware, as all Gaudi-specific logic should be contained within the <code>backends/gaudi</code> folder.",Gl,qe,Zl,Ke,Ha="To build the Docker image from source:",Vl,et,Fl,tt,Qa="This builds the image and saves it as <code>tgi-gaudi</code>. You can then run TGI-Gaudi with this image:",Nl,lt,Hl,at,Pa='For more details, see the <a href="https://github.com/huggingface/text-generation-inference/blob/main/backends/gaudi/README.md" rel="nofollow">README of the Gaudi backend</a> and the <a href="https://github.com/huggingface/text-generation-inference/blob/main/backends/gaudi/Makefile" rel="nofollow">Makefile of the Gaudi backend</a>.',Ql,nt,Pl,it,Yl;return b=new c({props:{title:"Gaudi Backend for Text Generation Inference",local:"gaudi-backend-for-text-generation-inference",headingTag:"h1"}}),y=new c({props:{title:"Overview",local:"overview",headingTag:"h2"}}),J=new c({props:{title:"Supported Hardware",local:"supported-hardware",headingTag:"h2"}}),v=new c({props:{title:"Tutorial: Getting Started with TGI on Gaudi",local:"tutorial-getting-started-with-tgi-on-gaudi",headingTag:"h2"}}),I=new c({props:{title:"Basic Usage",local:"basic-usage",headingTag:"h3"}}),E=new T({props:{code:"bW9kZWwlM0RtZXRhLWxsYW1hJTJGTWV0YS1MbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMEF2b2x1bWUlM0QlMjRQV0QlMkZkYXRhJTIwJTIzJTIwc2hhcmUlMjBhJTIwdm9sdW1lJTIwd2l0aCUyMHRoZSUyMERvY2tlciUyMGNvbnRhaW5lciUyMHRvJTIwYXZvaWQlMjBkb3dubG9hZGluZyUyMHdlaWdodHMlMjBldmVyeSUyMHJ1biUwQWhmX3Rva2VuJTNEWU9VUl9IRl9BQ0NFU1NfVE9LRU4lMEElMEFkb2NrZXIlMjBydW4lMjAtLXJ1bnRpbWUlM0RoYWJhbmElMjAtLWNhcC1hZGQlM0RzeXNfbmljZSUyMC0taXBjJTNEaG9zdCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1wJTIwODA4MCUzQTgwJTIwLXYlMjAlMjR2b2x1bWUlM0ElMkZkYXRhJTIwLWUlMjBIRl9UT0tFTiUzRCUyNGhmX3Rva2VuJTIwJTVDJTBBJTIwJTIwJTIwJTIwZ2hjci5pbyUyRmh1Z2dpbmdmYWNlJTJGdGV4dC1nZW5lcmF0aW9uLWluZmVyZW5jZSUzQTMuMy4xLWdhdWRpJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1tb2RlbC1pZCUyMCUyNG1vZGVs",highlighted:`model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=<span class="hljs-variable">$PWD</span>/data <span class="hljs-comment"># share a volume with the Docker container to avoid downloading weights every run</span>
hf_token=YOUR_HF_ACCESS_TOKEN

docker run --runtime=habana --cap-add=sys_nice --ipc=host \\
    -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data -e HF_TOKEN=<span class="hljs-variable">$hf_token</span> \\
    ghcr.io/huggingface/text-generation-inference:3.3.1-gaudi \\
    --model-id <span class="hljs-variable">$model</span>`,wrap:!1}}),R=new c({props:{title:"Making Your First Request",local:"making-your-first-request",headingTag:"h3"}}),k=new T({props:{code:"Y3VybCUyMDEyNy4wLjAuMSUzQTgwODAlMkZnZW5lcmF0ZSUyMCU1QyUwQSUyMCUyMCUyMCUyMC1YJTIwUE9TVCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1kJTIwJyU3QiUyMmlucHV0cyUyMiUzQSUyMldoYXQlMjBpcyUyMERlZXAlMjBMZWFybmluZyUzRiUyMiUyQyUyMnBhcmFtZXRlcnMlMjIlM0ElN0IlMjJtYXhfbmV3X3Rva2VucyUyMiUzQTMyJTdEJTdEJyUyMCU1QyUwQSUyMCUyMCUyMCUyMC1IJTIwJ0NvbnRlbnQtVHlwZSUzQSUyMGFwcGxpY2F0aW9uJTJGanNvbic=",highlighted:`curl 127.0.0.1:8080/generate \\
    -X POST \\
    -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:32}}&#x27;</span> \\
    -H <span class="hljs-string">&#x27;Content-Type: application/json&#x27;</span>`,wrap:!1}}),W=new c({props:{title:"How-to Guides",local:"how-to-guides",headingTag:"h2"}}),G=new T({props:{code:"bW9kZWwlM0RtZXRhLWxsYW1hJTJGTWV0YS1MbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMEF2b2x1bWUlM0QlMjRQV0QlMkZkYXRhJTIwJTIzJTIwc2hhcmUlMjBhJTIwdm9sdW1lJTIwd2l0aCUyMHRoZSUyMERvY2tlciUyMGNvbnRhaW5lciUyMHRvJTIwYXZvaWQlMjBkb3dubG9hZGluZyUyMHdlaWdodHMlMjBldmVyeSUyMHJ1biUwQWhmX3Rva2VuJTNEWU9VUl9BQ0NFU1NfVE9LRU4lMEElMEFkb2NrZXIlMjBydW4lMjAtLXJ1bnRpbWUlM0RoYWJhbmElMjAtLWNhcC1hZGQlM0RzeXNfbmljZSUyMC0taXBjJTNEaG9zdCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1wJTIwODA4MCUzQTgwJTIwLXYlMjAlMjR2b2x1bWUlM0ElMkZkYXRhJTIwLWUlMjBIRl9UT0tFTiUzRCUyNGhmX3Rva2VuJTIwJTVDJTBBJTIwJTIwJTIwJTIwZ2hjci5pbyUyRmh1Z2dpbmdmYWNlJTJGdGV4dC1nZW5lcmF0aW9uLWluZmVyZW5jZSUzQTMuMy4xLWdhdWRpJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1tb2RlbC1pZCUyMCUyNG1vZGVsJTBBJTIwJTIwJTIwJTIwJTNDdGV4dC1nZW5lcmF0aW9uLWluZmVyZW5jZS1sYXVuY2hlci1hcmd1bWVudHMlM0U=",highlighted:`model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=<span class="hljs-variable">$PWD</span>/data <span class="hljs-comment"># share a volume with the Docker container to avoid downloading weights every run</span>
hf_token=YOUR_ACCESS_TOKEN

docker run --runtime=habana --cap-add=sys_nice --ipc=host \\
    -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data -e HF_TOKEN=<span class="hljs-variable">$hf_token</span> \\
    ghcr.io/huggingface/text-generation-inference:3.3.1-gaudi \\
    --model-id <span class="hljs-variable">$model</span>
    &lt;text-generation-inference-launcher-arguments&gt;`,wrap:!1}}),N=new c({props:{title:"How to Enable Multi-Card Inference (Sharding)",local:"how-to-enable-multi-card-inference-sharding",headingTag:"h3"}}),P=new T({props:{code:"ZG9ja2VyJTIwcnVuJTIwLS1ydW50aW1lJTNEaGFiYW5hJTIwLS1pcGMlM0Rob3N0JTIwLS1jYXAtYWRkJTNEc3lzX25pY2UlMjAlNUMlMEElMjAlMjAlMjAlMjAtcCUyMDgwODAlM0E4MCUyMC12JTIwJTI0dm9sdW1lJTNBJTJGZGF0YSUyMC1lJTIwSEZfVE9LRU4lM0QlMjRoZl90b2tlbiUyMCU1QyUwQSUyMCUyMCUyMCUyMHRnaS1nYXVkaSUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbW9kZWwtaWQlMjAlMjRtb2RlbCUyMC0tc2hhcmRlZCUyMHRydWUlMjAtLW51bS1zaGFyZCUyMDg=",highlighted:`docker run --runtime=habana --ipc=host --cap-add=sys_nice \\
    -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data -e HF_TOKEN=<span class="hljs-variable">$hf_token</span> \\
    tgi-gaudi \\
    --model-id <span class="hljs-variable">$model</span> --sharded <span class="hljs-literal">true</span> --num-shard 8`,wrap:!1}}),w=new nn({props:{$$slots:{default:[on]},$$scope:{ctx:st}}}),Y=new c({props:{title:"How to Use Different Precision Formats",local:"how-to-use-different-precision-formats",headingTag:"h3"}}),X=new c({props:{title:"BF16 Precision (Default)",local:"bf16-precision-default",headingTag:"h4"}}),O=new c({props:{title:"FP8 Precision",local:"fp8-precision",headingTag:"h4"}}),le=new T({props:{code:"bW9kZWwlM0RtZXRhLWxsYW1hJTJGTWV0YS1MbGFtYS0zLjEtNzBCLUluc3RydWN0JTBBaGZfdG9rZW4lM0RZT1VSX0FDQ0VTU19UT0tFTiUwQXZvbHVtZSUzRCUyNFBXRCUyRmRhdGElMjAlMjAlMjAlMjMlMjBzaGFyZSUyMGElMjB2b2x1bWUlMjB3aXRoJTIwdGhlJTIwRG9ja2VyJTIwY29udGFpbmVyJTIwdG8lMjBhdm9pZCUyMGRvd25sb2FkaW5nJTIwd2VpZ2h0cyUyMGV2ZXJ5JTIwcnVuJTBBJTBBZG9ja2VyJTIwcnVuJTIwLXAlMjA4MDgwJTNBODAlMjAlNUMlMEElMjAlMjAlMjAtLXJ1bnRpbWUlM0RoYWJhbmElMjAlNUMlMEElMjAlMjAlMjAtLWNhcC1hZGQlM0RzeXNfbmljZSUyMCU1QyUwQSUyMCUyMCUyMC0taXBjJTNEaG9zdCUyMCU1QyUwQSUyMCUyMCUyMC12JTIwJTI0dm9sdW1lJTNBJTJGZGF0YSUyMCU1QyUwQSUyMCUyMCUyMC12JTIwJTI0UFdEJTJGcXVhbnRpemF0aW9uX2NvbmZpZyUzQSUyRnVzciUyRnNyYyUyRnF1YW50aXphdGlvbl9jb25maWclMjAlNUMlMEElMjAlMjAlMjAtdiUyMCUyNFBXRCUyRmhxdF9vdXRwdXQlM0ElMkZ1c3IlMkZzcmMlMkZocXRfb3V0cHV0JTIwJTVDJTBBJTIwJTIwJTIwLWUlMjBRVUFOVF9DT05GSUclM0QuJTJGcXVhbnRpemF0aW9uX2NvbmZpZyUyRm1heGFic19xdWFudC5qc29uJTIwJTVDJTBBJTIwJTIwJTIwLWUlMjBIRl9UT0tFTiUzRCUyNGhmX3Rva2VuJTIwJTVDJTBBJTIwJTIwJTIwLWUlMjBNQVhfVE9UQUxfVE9LRU5TJTNEMjA0OCUyMCU1QyUwQSUyMCUyMCUyMC1lJTIwQkFUQ0hfQlVDS0VUX1NJWkUlM0QyNTYlMjAlNUMlMEElMjAlMjAlMjAtZSUyMFBSRUZJTExfQkFUQ0hfQlVDS0VUX1NJWkUlM0Q0JTIwJTVDJTBBJTIwJTIwJTIwLWUlMjBQQURfU0VRVUVOQ0VfVE9fTVVMVElQTEVfT0YlM0Q2NCUyMCU1QyUwQSUyMCUyMCUyMGdoY3IuaW8lMkZodWdnaW5nZmFjZSUyRnRleHQtZ2VuZXJhdGlvbi1pbmZlcmVuY2UlM0EzLjMuMS1nYXVkaSUyMCU1QyUwQSUyMCUyMCUyMC0tbW9kZWwtaWQlMjAlMjRtb2RlbCUyMCU1QyUwQSUyMCUyMCUyMC0tc2hhcmRlZCUyMHRydWUlMjAtLW51bS1zaGFyZCUyMDglMjAlNUMlMEElMjAlMjAlMjAtLW1heC1pbnB1dC10b2tlbnMlMjAxMDI0JTIwLS1tYXgtdG90YWwtdG9rZW5zJTIwMjA0OCUyMCU1QyUwQSUyMCUyMCUyMC0tbWF4LWJhdGNoLXByZWZpbGwtdG9rZW5zJTIwNDA5NiUyMC0tbWF4LWJhdGNoLXNpemUlMjAyNTYlMjAlNUMlMEElMjAlMjAlMjAtLW1heC13YWl0aW5nLXRva2VucyUyMDclMjAtLXdhaXRpbmctc2VydmVkLXJhdGlvJTIwMS4yJTIwLS1tYXgtY29uY3VycmVudC1yZXF1ZXN0cyUyMDUxMg==",highlighted:`model=meta-llama/Meta-Llama-3.1-70B-Instruct
hf_token=YOUR_ACCESS_TOKEN
volume=<span class="hljs-variable">$PWD</span>/data   <span class="hljs-comment"># share a volume with the Docker container to avoid downloading weights every run</span>

docker run -p 8080:80 \\
   --runtime=habana \\
   --cap-add=sys_nice \\
   --ipc=host \\
   -v <span class="hljs-variable">$volume</span>:/data \\
   -v <span class="hljs-variable">$PWD</span>/quantization_config:/usr/src/quantization_config \\
   -v <span class="hljs-variable">$PWD</span>/hqt_output:/usr/src/hqt_output \\
   -e QUANT_CONFIG=./quantization_config/maxabs_quant.json \\
   -e HF_TOKEN=<span class="hljs-variable">$hf_token</span> \\
   -e MAX_TOTAL_TOKENS=2048 \\
   -e BATCH_BUCKET_SIZE=256 \\
   -e PREFILL_BATCH_BUCKET_SIZE=4 \\
   -e PAD_SEQUENCE_TO_MULTIPLE_OF=64 \\
   ghcr.io/huggingface/text-generation-inference:3.3.1-gaudi \\
   --model-id <span class="hljs-variable">$model</span> \\
   --sharded <span class="hljs-literal">true</span> --num-shard 8 \\
   --max-input-tokens 1024 --max-total-tokens 2048 \\
   --max-batch-prefill-tokens 4096 --max-batch-size 256 \\
   --max-waiting-tokens 7 --waiting-served-ratio 1.2 --max-concurrent-requests 512`,wrap:!1}}),ae=new c({props:{title:"How to Run Vision-Language Models (VLMs)",local:"how-to-run-vision-language-models-vlms",headingTag:"h3"}}),oe=new T({props:{code:"bW9kZWwlM0RsbGF2YS1oZiUyRmxsYXZhLXYxLjYtbWlzdHJhbC03Yi1oZiUwQXZvbHVtZSUzRCUyNFBXRCUyRmRhdGElMjAlMjAlMjAlMjMlMjBzaGFyZSUyMGElMjB2b2x1bWUlMjB3aXRoJTIwdGhlJTIwRG9ja2VyJTIwY29udGFpbmVyJTIwdG8lMjBhdm9pZCUyMGRvd25sb2FkaW5nJTIwd2VpZ2h0cyUyMGV2ZXJ5JTIwcnVuJTBBJTBBZG9ja2VyJTIwcnVuJTIwLXAlMjA4MDgwJTNBODAlMjAlNUMlMEElMjAlMjAlMjAtLXJ1bnRpbWUlM0RoYWJhbmElMjAlNUMlMEElMjAlMjAlMjAtLWNhcC1hZGQlM0RzeXNfbmljZSUyMCU1QyUwQSUyMCUyMCUyMC0taXBjJTNEaG9zdCUyMCU1QyUwQSUyMCUyMCUyMC12JTIwJTI0dm9sdW1lJTNBJTJGZGF0YSUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwUFJFRklMTF9CQVRDSF9CVUNLRVRfU0laRSUzRDElMjAlNUMlMEElMjAlMjAlMjAlMjAtZSUyMEJBVENIX0JVQ0tFVF9TSVpFJTNEMSUyMCU1QyUwQSUyMCUyMCUyMGdoY3IuaW8lMkZodWdnaW5nZmFjZSUyRnRleHQtZ2VuZXJhdGlvbi1pbmZlcmVuY2UlM0EzLjMuMS1nYXVkaSUyMCU1QyUwQSUyMCUyMCUyMC0tbW9kZWwtaWQlMjAlMjRtb2RlbCUyMCU1QyUwQSUyMCUyMCUyMC0tbWF4LWlucHV0LXRva2VucyUyMDQwOTYlMjAtLW1heC1iYXRjaC1wcmVmaWxsLXRva2VucyUyMDE2Mzg0JTIwJTVDJTBBJTIwJTIwJTIwLS1tYXgtdG90YWwtdG9rZW5zJTIwODE5MiUyMC0tbWF4LWJhdGNoLXNpemUlMjA0",highlighted:`model=llava-hf/llava-v1.6-mistral-7b-hf
volume=<span class="hljs-variable">$PWD</span>/data   <span class="hljs-comment"># share a volume with the Docker container to avoid downloading weights every run</span>

docker run -p 8080:80 \\
   --runtime=habana \\
   --cap-add=sys_nice \\
   --ipc=host \\
   -v <span class="hljs-variable">$volume</span>:/data \\
    -e PREFILL_BATCH_BUCKET_SIZE=1 \\
    -e BATCH_BUCKET_SIZE=1 \\
   ghcr.io/huggingface/text-generation-inference:3.3.1-gaudi \\
   --model-id <span class="hljs-variable">$model</span> \\
   --max-input-tokens 4096 --max-batch-prefill-tokens 16384 \\
   --max-total-tokens 8192 --max-batch-size 4`,wrap:!1}}),de=new T({props:{code:"Y3VybCUyMC1OJTIwMTI3LjAuMC4xJTNBODA4MCUyRmdlbmVyYXRlJTIwJTVDJTBBJTIwJTIwJTIwJTIwLVglMjBQT1NUJTIwJTVDJTBBJTIwJTIwJTIwJTIwLWQlMjAnJTdCJTIyaW5wdXRzJTIyJTNBJTIyISU1QiU1RChodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZ0cmFuc2Zvcm1lcnMlMkZyYWJiaXQucG5nKVdoYXQlMjBpcyUyMHRoaXMlMjBhJTIwcGljdHVyZSUyMG9mJTNGJTVDbiU1Q24lMjIlMkMlMjJwYXJhbWV0ZXJzJTIyJTNBJTdCJTIybWF4X25ld190b2tlbnMlMjIlM0EzMiU3RCU3RCclMjAlNUMlMEElMjAlMjAlMjAlMjAtSCUyMCdDb250ZW50LVR5cGUlM0ElMjBhcHBsaWNhdGlvbiUyRmpzb24n",highlighted:`curl -N 127.0.0.1:8080/generate \\
    -X POST \\
    -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png)What is this a picture of?\\n\\n&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:32}}&#x27;</span> \\
    -H <span class="hljs-string">&#x27;Content-Type: application/json&#x27;</span>`,wrap:!1}}),ue=new c({props:{title:"How to Benchmark Performance",local:"how-to-benchmark-performance",headingTag:"h3"}}),ce=new T({props:{code:"TU9ERUwlM0RtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTBBSEZfVE9LRU4lM0QlM0N5b3VyJTIwSEYlMjBSRUFEJTIwdG9rZW4lM0UlMEElMjMlMjBydW4lMjBhJTIwYmVuY2htYXJrJTIwdG8lMjBldmFsdWF0ZSUyMHRoZSUyMHBlcmZvcm1hbmNlJTIwb2YlMjB0aGUlMjBtb2RlbCUyMGZvciUyMGNoYXQlMjB1c2UlMjBjYXNlJTBBJTIzJTIwd2UlMjBtb3VudCUyMHJlc3VsdHMlMjB0byUyMHRoZSUyMGN1cnJlbnQlMjBkaXJlY3RvcnklMEFkb2NrZXIlMjBydW4lMjAlNUMlMEElMjAlMjAlMjAlMjAtLXJtJTIwJTVDJTBBJTIwJTIwJTIwJTIwLWl0JTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1uZXQlMjBob3N0JTIwJTVDJTBBJTIwJTIwJTIwJTIwLXYlMjAlMjQocHdkKSUzQSUyRm9wdCUyRmluZmVyZW5jZS1iZW5jaG1hcmtlciUyRnJlc3VsdHMlMjAlNUMlMEElMjAlMjAlMjAlMjAtZSUyMCUyMkhGX1RPS0VOJTNEJTI0SEZfVE9LRU4lMjIlMjAlNUMlMEElMjAlMjAlMjAlMjBnaGNyLmlvJTJGaHVnZ2luZ2ZhY2UlMkZpbmZlcmVuY2UtYmVuY2htYXJrZXIlM0FsYXRlc3QlMjAlNUMlMEElMjAlMjAlMjAlMjBpbmZlcmVuY2UtYmVuY2htYXJrZXIlMjAlNUMlMEElMjAlMjAlMjAlMjAtLXRva2VuaXplci1uYW1lJTIwJTIyJTI0TU9ERUwlMjIlMjAlNUMlMEElMjAlMjAlMjAlMjAtLXVybCUyMGh0dHAlM0ElMkYlMkZsb2NhbGhvc3QlM0E4MDgwJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1wcm9maWxlJTIwY2hhdA==",highlighted:`MODEL=meta-llama/Llama-3.1-8B-Instruct
HF_TOKEN=&lt;your HF READ token&gt;
<span class="hljs-comment"># run a benchmark to evaluate the performance of the model for chat use case</span>
<span class="hljs-comment"># we mount results to the current directory</span>
docker run \\
    --<span class="hljs-built_in">rm</span> \\
    -it \\
    --net host \\
    -v $(<span class="hljs-built_in">pwd</span>):/opt/inference-benchmarker/results \\
    -e <span class="hljs-string">&quot;HF_TOKEN=<span class="hljs-variable">$HF_TOKEN</span>&quot;</span> \\
    ghcr.io/huggingface/inference-benchmarker:latest \\
    inference-benchmarker \\
    --tokenizer-name <span class="hljs-string">&quot;<span class="hljs-variable">$MODEL</span>&quot;</span> \\
    --url http://localhost:8080 \\
    --profile chat`,wrap:!1}}),Te=new c({props:{title:"How to Profile Performance",local:"how-to-profile-performance",headingTag:"h3"}}),be=new T({props:{code:"ZG9ja2VyJTIwcnVuJTIwLS1ydW50aW1lJTNEaGFiYW5hJTIwLS1pcGMlM0Rob3N0JTIwLS1jYXAtYWRkJTNEc3lzX25pY2UlMjAlNUMlMEElMjAlMjAlMjAlMjAtcCUyMDgwODAlM0E4MCUyMC12JTIwJTI0dm9sdW1lJTNBJTJGZGF0YSUyMC1lJTIwSEZfVE9LRU4lM0QlMjRoZl90b2tlbiUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwUFJPRl9XQUlUU1RFUCUzRDEwJTIwJTVDJTBBJTIwJTIwJTIwJTIwLWUlMjBQUk9GX1dBUk1VUFNURVAlM0QxMCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwUFJPRl9TVEVQJTNEMSUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwUFJPRl9QQVRIJTNEJTJGdG1wJTJGaHB1X3Byb2ZpbGUlMjAlNUMlMEElMjAlMjAlMjAlMjAtZSUyMFBST0ZfUkFOS1MlM0QwJTIwJTVDJTBBJTIwJTIwJTIwJTIwLWUlMjBQUk9GX1JFQ09SRF9TSEFQRVMlM0RUcnVlJTIwJTVDJTBBJTIwJTIwJTIwJTIwZ2hjci5pbyUyRmh1Z2dpbmdmYWNlJTJGdGV4dC1nZW5lcmF0aW9uLWluZmVyZW5jZSUzQTMuMy4xLWdhdWRpJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1tb2RlbC1pZCUyMCUyNG1vZGVs",highlighted:`docker run --runtime=habana --ipc=host --cap-add=sys_nice \\
    -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data -e HF_TOKEN=<span class="hljs-variable">$hf_token</span> \\
    -e PROF_WAITSTEP=10 \\
    -e PROF_WARMUPSTEP=10 \\
    -e PROF_STEP=1 \\
    -e PROF_PATH=/tmp/hpu_profile \\
    -e PROF_RANKS=0 \\
    -e PROF_RECORD_SHAPES=True \\
    ghcr.io/huggingface/text-generation-inference:3.3.1-gaudi \\
    --model-id <span class="hljs-variable">$model</span>`,wrap:!1}}),ye=new c({props:{title:"Explanation: Understanding TGI on Gaudi",local:"explanation-understanding-tgi-on-gaudi",headingTag:"h2"}}),Ce=new c({props:{title:"The Warmup Process",local:"the-warmup-process",headingTag:"h3"}}),ve=new c({props:{title:"Understanding Parameter Tuning",local:"understanding-parameter-tuning",headingTag:"h3"}}),Ie=new c({props:{title:"Sequence Length Parameters",local:"sequence-length-parameters",headingTag:"h4"}}),Ee=new c({props:{title:"Batch Size Parameters",local:"batch-size-parameters",headingTag:"h4"}}),xe=new c({props:{title:"Performance and Memory Parameters",local:"performance-and-memory-parameters",headingTag:"h4"}}),Re=new c({props:{title:"Sequence Length Parameters",local:"sequence-length-parameters",headingTag:"h4"}}),ke=new c({props:{title:"Batch Size Parameters",local:"batch-size-parameters",headingTag:"h4"}}),Se=new c({props:{title:"Reference",local:"reference",headingTag:"h2"}}),Ge=new c({props:{title:"Supported Models",local:"supported-models",headingTag:"h3"}}),Pe=new c({props:{title:"Environment Variables",local:"environment-variables",headingTag:"h3"}}),ze=new c({props:{title:"Contributing",local:"contributing",headingTag:"h2"}}),qe=new c({props:{title:"Building the Docker Image from Source",local:"building-the-docker-image-from-source",headingTag:"h3"}}),et=new T({props:{code:"bWFrZSUyMC1DJTIwYmFja2VuZHMlMkZnYXVkaSUyMGltYWdl",highlighted:"make -C backends/gaudi image",wrap:!1}}),lt=new T({props:{code:"bW9kZWwlM0RtZXRhLWxsYW1hJTJGTWV0YS1MbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMEF2b2x1bWUlM0QlMjRQV0QlMkZkYXRhJTBBaGZfdG9rZW4lM0RZT1VSX0FDQ0VTU19UT0tFTiUwQSUwQWRvY2tlciUyMHJ1biUyMC0tcnVudGltZSUzRGhhYmFuYSUyMC0taXBjJTNEaG9zdCUyMC0tY2FwLWFkZCUzRHN5c19uaWNlJTIwJTVDJTBBJTIwJTIwJTIwJTIwLXAlMjA4MDgwJTNBODAlMjAtdiUyMCUyNHZvbHVtZSUzQSUyRmRhdGElMjAtZSUyMEhGX1RPS0VOJTNEJTI0aGZfdG9rZW4lMjAlNUMlMEElMjAlMjAlMjAlMjB0Z2ktZ2F1ZGklMjAlNUMlMEElMjAlMjAlMjAlMjAtLW1vZGVsLWlkJTIwJTI0bW9kZWw=",highlighted:`model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=<span class="hljs-variable">$PWD</span>/data
hf_token=YOUR_ACCESS_TOKEN

docker run --runtime=habana --ipc=host --cap-add=sys_nice \\
    -p 8080:80 -v <span class="hljs-variable">$volume</span>:/data -e HF_TOKEN=<span class="hljs-variable">$hf_token</span> \\
    tgi-gaudi \\
    --model-id <span class="hljs-variable">$model</span>`,wrap:!1}}),nt=new sn({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/backends/gaudi.mdx"}}),{c(){M=s("meta"),g=n(),U=s("p"),ot=n(),d(b.$$.fragment),rt=n(),d(y.$$.fragment),dt=n(),C=s("p"),C.textContent=Xl,mt=n(),d(J.$$.fragment),ut=n(),$=s("ul"),$.innerHTML=zl,ft=n(),d(v.$$.fragment),pt=n(),d(I.$$.fragment),ht=n(),j=s("p"),j.textContent=Ol,ct=n(),d(E.$$.fragment),Mt=n(),L=s("p"),L.innerHTML=Dl,Tt=n(),x=s("blockquote"),x.innerHTML=ql,gt=n(),_=s("p"),_.innerHTML=Kl,wt=n(),d(R.$$.fragment),Ut=n(),B=s("p"),B.textContent=ea,bt=n(),d(k.$$.fragment),yt=n(),d(W.$$.fragment),Ct=n(),S=s("p"),S.innerHTML=ta,Jt=n(),A=s("p"),A.textContent=la,$t=n(),d(G.$$.fragment),vt=n(),Z=s("p"),Z.innerHTML=aa,It=n(),V=s("p"),V.innerHTML=na,jt=n(),F=s("blockquote"),F.innerHTML=ia,Et=n(),d(N.$$.fragment),Lt=n(),H=s("p"),H.textContent=sa,xt=n(),Q=s("p"),Q.textContent=oa,_t=n(),d(P.$$.fragment),Rt=n(),d(w.$$.fragment),Bt=n(),d(Y.$$.fragment),kt=n(),d(X.$$.fragment),Wt=n(),z=s("p"),z.textContent=ra,St=n(),d(O.$$.fragment),At=n(),D=s("p"),D.innerHTML=da,Gt=n(),q=s("p"),q.textContent=ma,Zt=n(),K=s("ol"),K.innerHTML=ua,Vt=n(),ee=s("p"),ee.textContent=fa,Ft=n(),te=s("p"),te.textContent=pa,Nt=n(),d(le.$$.fragment),Ht=n(),d(ae.$$.fragment),Qt=n(),ne=s("p"),ne.textContent=ha,Pt=n(),ie=s("p"),ie.textContent=ca,Yt=n(),se=s("p"),se.textContent=Ma,Xt=n(),d(oe.$$.fragment),zt=n(),re=s("p"),re.textContent=Ta,Ot=n(),d(de.$$.fragment),Dt=n(),me=s("blockquote"),me.innerHTML=ga,qt=n(),d(ue.$$.fragment),Kt=n(),fe=s("p"),fe.innerHTML=wa,el=n(),pe=s("p"),pe.textContent=Ua,tl=n(),he=s("p"),he.textContent=ba,ll=n(),d(ce.$$.fragment),al=n(),Me=s("p"),Me.innerHTML=ya,nl=n(),d(Te.$$.fragment),il=n(),ge=s("p"),ge.textContent=Ca,sl=n(),we=s("table"),we.innerHTML=Ja,ol=n(),Ue=s("p"),Ue.textContent=$a,rl=n(),d(be.$$.fragment),dl=n(),d(ye.$$.fragment),ml=n(),d(Ce.$$.fragment),ul=n(),Je=s("p"),Je.textContent=va,fl=n(),$e=s("p"),$e.innerHTML=Ia,pl=n(),d(ve.$$.fragment),hl=n(),d(Ie.$$.fragment),cl=n(),je=s("ul"),je.innerHTML=ja,Ml=n(),d(Ee.$$.fragment),Tl=n(),Le=s("ul"),Le.innerHTML=Ea,gl=n(),d(xe.$$.fragment),wl=n(),_e=s("ul"),_e.innerHTML=La,Ul=n(),d(Re.$$.fragment),bl=n(),Be=s("ul"),Be.innerHTML=xa,yl=n(),d(ke.$$.fragment),Cl=n(),We=s("ul"),We.innerHTML=_a,Jl=n(),d(Se.$$.fragment),$l=n(),Ae=s("p"),Ae.textContent=Ra,vl=n(),d(Ge.$$.fragment),Il=n(),Ze=s("p"),Ze.textContent=Ba,jl=n(),Ve=s("p"),Ve.innerHTML=ka,El=n(),Fe=s("ul"),Fe.innerHTML=Wa,Ll=n(),Ne=s("p"),Ne.innerHTML=Sa,xl=n(),He=s("ul"),He.innerHTML=Aa,_l=n(),Qe=s("p"),Qe.innerHTML=Ga,Rl=n(),d(Pe.$$.fragment),Bl=n(),Ye=s("p"),Ye.textContent=Za,kl=n(),Xe=s("table"),Xe.innerHTML=Va,Wl=n(),d(ze.$$.fragment),Sl=n(),Oe=s("p"),Oe.innerHTML=Fa,Al=n(),De=s("p"),De.innerHTML=Na,Gl=n(),d(qe.$$.fragment),Zl=n(),Ke=s("p"),Ke.textContent=Ha,Vl=n(),d(et.$$.fragment),Fl=n(),tt=s("p"),tt.innerHTML=Qa,Nl=n(),d(lt.$$.fragment),Hl=n(),at=s("p"),at.innerHTML=Pa,Ql=n(),d(nt.$$.fragment),Pl=n(),it=s("p"),this.h()},l(e){const t=en("svelte-u9bgzb",document.head);M=o(t,"META",{name:!0,content:!0}),t.forEach(l),g=i(e),U=o(e,"P",{}),Xa(U).forEach(l),ot=i(e),m(b.$$.fragment,e),rt=i(e),m(y.$$.fragment,e),dt=i(e),C=o(e,"P",{"data-svelte-h":!0}),r(C)!=="svelte-1lkgyoj"&&(C.textContent=Xl),mt=i(e),m(J.$$.fragment,e),ut=i(e),$=o(e,"UL",{"data-svelte-h":!0}),r($)!=="svelte-zvm268"&&($.innerHTML=zl),ft=i(e),m(v.$$.fragment,e),pt=i(e),m(I.$$.fragment,e),ht=i(e),j=o(e,"P",{"data-svelte-h":!0}),r(j)!=="svelte-1jfnu9a"&&(j.textContent=Ol),ct=i(e),m(E.$$.fragment,e),Mt=i(e),L=o(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-12fc12k"&&(L.innerHTML=Dl),Tt=i(e),x=o(e,"BLOCKQUOTE",{"data-svelte-h":!0}),r(x)!=="svelte-h52ldr"&&(x.innerHTML=ql),gt=i(e),_=o(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-12hdlue"&&(_.innerHTML=Kl),wt=i(e),m(R.$$.fragment,e),Ut=i(e),B=o(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-1qc1nc3"&&(B.textContent=ea),bt=i(e),m(k.$$.fragment,e),yt=i(e),m(W.$$.fragment,e),Ct=i(e),S=o(e,"P",{"data-svelte-h":!0}),r(S)!=="svelte-16fdc52"&&(S.innerHTML=ta),Jt=i(e),A=o(e,"P",{"data-svelte-h":!0}),r(A)!=="svelte-1z0y2mq"&&(A.textContent=la),$t=i(e),m(G.$$.fragment,e),vt=i(e),Z=o(e,"P",{"data-svelte-h":!0}),r(Z)!=="svelte-jq0xg3"&&(Z.innerHTML=aa),It=i(e),V=o(e,"P",{"data-svelte-h":!0}),r(V)!=="svelte-15oyfnr"&&(V.innerHTML=na),jt=i(e),F=o(e,"BLOCKQUOTE",{"data-svelte-h":!0}),r(F)!=="svelte-j1bbbz"&&(F.innerHTML=ia),Et=i(e),m(N.$$.fragment,e),Lt=i(e),H=o(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-l6qmeb"&&(H.textContent=sa),xt=i(e),Q=o(e,"P",{"data-svelte-h":!0}),r(Q)!=="svelte-1nqhfr1"&&(Q.textContent=oa),_t=i(e),m(P.$$.fragment,e),Rt=i(e),m(w.$$.fragment,e),Bt=i(e),m(Y.$$.fragment,e),kt=i(e),m(X.$$.fragment,e),Wt=i(e),z=o(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-1sr71rs"&&(z.textContent=ra),St=i(e),m(O.$$.fragment,e),At=i(e),D=o(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-1vq1t1k"&&(D.innerHTML=da),Gt=i(e),q=o(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-1mkcigt"&&(q.textContent=ma),Zt=i(e),K=o(e,"OL",{"data-svelte-h":!0}),r(K)!=="svelte-motr5y"&&(K.innerHTML=ua),Vt=i(e),ee=o(e,"P",{"data-svelte-h":!0}),r(ee)!=="svelte-vb84vz"&&(ee.textContent=fa),Ft=i(e),te=o(e,"P",{"data-svelte-h":!0}),r(te)!=="svelte-f2aih0"&&(te.textContent=pa),Nt=i(e),m(le.$$.fragment,e),Ht=i(e),m(ae.$$.fragment,e),Qt=i(e),ne=o(e,"P",{"data-svelte-h":!0}),r(ne)!=="svelte-65ex9k"&&(ne.textContent=ha),Pt=i(e),ie=o(e,"P",{"data-svelte-h":!0}),r(ie)!=="svelte-1uu3hni"&&(ie.textContent=ca),Yt=i(e),se=o(e,"P",{"data-svelte-h":!0}),r(se)!=="svelte-cvom77"&&(se.textContent=Ma),Xt=i(e),m(oe.$$.fragment,e),zt=i(e),re=o(e,"P",{"data-svelte-h":!0}),r(re)!=="svelte-a8iedu"&&(re.textContent=Ta),Ot=i(e),m(de.$$.fragment,e),Dt=i(e),me=o(e,"BLOCKQUOTE",{"data-svelte-h":!0}),r(me)!=="svelte-101oubq"&&(me.innerHTML=ga),qt=i(e),m(ue.$$.fragment,e),Kt=i(e),fe=o(e,"P",{"data-svelte-h":!0}),r(fe)!=="svelte-mh074q"&&(fe.innerHTML=wa),el=i(e),pe=o(e,"P",{"data-svelte-h":!0}),r(pe)!=="svelte-ohja5t"&&(pe.textContent=Ua),tl=i(e),he=o(e,"P",{"data-svelte-h":!0}),r(he)!=="svelte-p1rvl0"&&(he.textContent=ba),ll=i(e),m(ce.$$.fragment,e),al=i(e),Me=o(e,"P",{"data-svelte-h":!0}),r(Me)!=="svelte-6jbbji"&&(Me.innerHTML=ya),nl=i(e),m(Te.$$.fragment,e),il=i(e),ge=o(e,"P",{"data-svelte-h":!0}),r(ge)!=="svelte-117jjx"&&(ge.textContent=Ca),sl=i(e),we=o(e,"TABLE",{"data-svelte-h":!0}),r(we)!=="svelte-t13n7i"&&(we.innerHTML=Ja),ol=i(e),Ue=o(e,"P",{"data-svelte-h":!0}),r(Ue)!=="svelte-z5a6a"&&(Ue.textContent=$a),rl=i(e),m(be.$$.fragment,e),dl=i(e),m(ye.$$.fragment,e),ml=i(e),m(Ce.$$.fragment,e),ul=i(e),Je=o(e,"P",{"data-svelte-h":!0}),r(Je)!=="svelte-18motaf"&&(Je.textContent=va),fl=i(e),$e=o(e,"P",{"data-svelte-h":!0}),r($e)!=="svelte-i2vp5a"&&($e.innerHTML=Ia),pl=i(e),m(ve.$$.fragment,e),hl=i(e),m(Ie.$$.fragment,e),cl=i(e),je=o(e,"UL",{"data-svelte-h":!0}),r(je)!=="svelte-rfsxop"&&(je.innerHTML=ja),Ml=i(e),m(Ee.$$.fragment,e),Tl=i(e),Le=o(e,"UL",{"data-svelte-h":!0}),r(Le)!=="svelte-jvxq1w"&&(Le.innerHTML=Ea),gl=i(e),m(xe.$$.fragment,e),wl=i(e),_e=o(e,"UL",{"data-svelte-h":!0}),r(_e)!=="svelte-1ll1x0x"&&(_e.innerHTML=La),Ul=i(e),m(Re.$$.fragment,e),bl=i(e),Be=o(e,"UL",{"data-svelte-h":!0}),r(Be)!=="svelte-q300gx"&&(Be.innerHTML=xa),yl=i(e),m(ke.$$.fragment,e),Cl=i(e),We=o(e,"UL",{"data-svelte-h":!0}),r(We)!=="svelte-svwfqa"&&(We.innerHTML=_a),Jl=i(e),m(Se.$$.fragment,e),$l=i(e),Ae=o(e,"P",{"data-svelte-h":!0}),r(Ae)!=="svelte-1g31upz"&&(Ae.textContent=Ra),vl=i(e),m(Ge.$$.fragment,e),Il=i(e),Ze=o(e,"P",{"data-svelte-h":!0}),r(Ze)!=="svelte-viw9gl"&&(Ze.textContent=Ba),jl=i(e),Ve=o(e,"P",{"data-svelte-h":!0}),r(Ve)!=="svelte-1gk5zr"&&(Ve.innerHTML=ka),El=i(e),Fe=o(e,"UL",{"data-svelte-h":!0}),r(Fe)!=="svelte-ciqdp8"&&(Fe.innerHTML=Wa),Ll=i(e),Ne=o(e,"P",{"data-svelte-h":!0}),r(Ne)!=="svelte-1qjqjz"&&(Ne.innerHTML=Sa),xl=i(e),He=o(e,"UL",{"data-svelte-h":!0}),r(He)!=="svelte-1dphtel"&&(He.innerHTML=Aa),_l=i(e),Qe=o(e,"P",{"data-svelte-h":!0}),r(Qe)!=="svelte-1djv2zs"&&(Qe.innerHTML=Ga),Rl=i(e),m(Pe.$$.fragment,e),Bl=i(e),Ye=o(e,"P",{"data-svelte-h":!0}),r(Ye)!=="svelte-103d867"&&(Ye.textContent=Za),kl=i(e),Xe=o(e,"TABLE",{"data-svelte-h":!0}),r(Xe)!=="svelte-17kyy38"&&(Xe.innerHTML=Va),Wl=i(e),m(ze.$$.fragment,e),Sl=i(e),Oe=o(e,"P",{"data-svelte-h":!0}),r(Oe)!=="svelte-ff3n37"&&(Oe.innerHTML=Fa),Al=i(e),De=o(e,"P",{"data-svelte-h":!0}),r(De)!=="svelte-j30flr"&&(De.innerHTML=Na),Gl=i(e),m(qe.$$.fragment,e),Zl=i(e),Ke=o(e,"P",{"data-svelte-h":!0}),r(Ke)!=="svelte-eehrcc"&&(Ke.textContent=Ha),Vl=i(e),m(et.$$.fragment,e),Fl=i(e),tt=o(e,"P",{"data-svelte-h":!0}),r(tt)!=="svelte-19ntzpc"&&(tt.innerHTML=Qa),Nl=i(e),m(lt.$$.fragment,e),Hl=i(e),at=o(e,"P",{"data-svelte-h":!0}),r(at)!=="svelte-q8wixj"&&(at.innerHTML=Pa),Ql=i(e),m(nt.$$.fragment,e),Pl=i(e),it=o(e,"P",{}),Xa(it).forEach(l),this.h()},h(){za(M,"name","hf:doc:metadata"),za(M,"content",dn)},m(e,t){tn(document.head,M),a(e,g,t),a(e,U,t),a(e,ot,t),u(b,e,t),a(e,rt,t),u(y,e,t),a(e,dt,t),a(e,C,t),a(e,mt,t),u(J,e,t),a(e,ut,t),a(e,$,t),a(e,ft,t),u(v,e,t),a(e,pt,t),u(I,e,t),a(e,ht,t),a(e,j,t),a(e,ct,t),u(E,e,t),a(e,Mt,t),a(e,L,t),a(e,Tt,t),a(e,x,t),a(e,gt,t),a(e,_,t),a(e,wt,t),u(R,e,t),a(e,Ut,t),a(e,B,t),a(e,bt,t),u(k,e,t),a(e,yt,t),u(W,e,t),a(e,Ct,t),a(e,S,t),a(e,Jt,t),a(e,A,t),a(e,$t,t),u(G,e,t),a(e,vt,t),a(e,Z,t),a(e,It,t),a(e,V,t),a(e,jt,t),a(e,F,t),a(e,Et,t),u(N,e,t),a(e,Lt,t),a(e,H,t),a(e,xt,t),a(e,Q,t),a(e,_t,t),u(P,e,t),a(e,Rt,t),u(w,e,t),a(e,Bt,t),u(Y,e,t),a(e,kt,t),u(X,e,t),a(e,Wt,t),a(e,z,t),a(e,St,t),u(O,e,t),a(e,At,t),a(e,D,t),a(e,Gt,t),a(e,q,t),a(e,Zt,t),a(e,K,t),a(e,Vt,t),a(e,ee,t),a(e,Ft,t),a(e,te,t),a(e,Nt,t),u(le,e,t),a(e,Ht,t),u(ae,e,t),a(e,Qt,t),a(e,ne,t),a(e,Pt,t),a(e,ie,t),a(e,Yt,t),a(e,se,t),a(e,Xt,t),u(oe,e,t),a(e,zt,t),a(e,re,t),a(e,Ot,t),u(de,e,t),a(e,Dt,t),a(e,me,t),a(e,qt,t),u(ue,e,t),a(e,Kt,t),a(e,fe,t),a(e,el,t),a(e,pe,t),a(e,tl,t),a(e,he,t),a(e,ll,t),u(ce,e,t),a(e,al,t),a(e,Me,t),a(e,nl,t),u(Te,e,t),a(e,il,t),a(e,ge,t),a(e,sl,t),a(e,we,t),a(e,ol,t),a(e,Ue,t),a(e,rl,t),u(be,e,t),a(e,dl,t),u(ye,e,t),a(e,ml,t),u(Ce,e,t),a(e,ul,t),a(e,Je,t),a(e,fl,t),a(e,$e,t),a(e,pl,t),u(ve,e,t),a(e,hl,t),u(Ie,e,t),a(e,cl,t),a(e,je,t),a(e,Ml,t),u(Ee,e,t),a(e,Tl,t),a(e,Le,t),a(e,gl,t),u(xe,e,t),a(e,wl,t),a(e,_e,t),a(e,Ul,t),u(Re,e,t),a(e,bl,t),a(e,Be,t),a(e,yl,t),u(ke,e,t),a(e,Cl,t),a(e,We,t),a(e,Jl,t),u(Se,e,t),a(e,$l,t),a(e,Ae,t),a(e,vl,t),u(Ge,e,t),a(e,Il,t),a(e,Ze,t),a(e,jl,t),a(e,Ve,t),a(e,El,t),a(e,Fe,t),a(e,Ll,t),a(e,Ne,t),a(e,xl,t),a(e,He,t),a(e,_l,t),a(e,Qe,t),a(e,Rl,t),u(Pe,e,t),a(e,Bl,t),a(e,Ye,t),a(e,kl,t),a(e,Xe,t),a(e,Wl,t),u(ze,e,t),a(e,Sl,t),a(e,Oe,t),a(e,Al,t),a(e,De,t),a(e,Gl,t),u(qe,e,t),a(e,Zl,t),a(e,Ke,t),a(e,Vl,t),u(et,e,t),a(e,Fl,t),a(e,tt,t),a(e,Nl,t),u(lt,e,t),a(e,Hl,t),a(e,at,t),a(e,Ql,t),u(nt,e,t),a(e,Pl,t),a(e,it,t),Yl=!0},p(e,[t]){const Ya={};t&2&&(Ya.$$scope={dirty:t,ctx:e}),w.$set(Ya)},i(e){Yl||(f(b.$$.fragment,e),f(y.$$.fragment,e),f(J.$$.fragment,e),f(v.$$.fragment,e),f(I.$$.fragment,e),f(E.$$.fragment,e),f(R.$$.fragment,e),f(k.$$.fragment,e),f(W.$$.fragment,e),f(G.$$.fragment,e),f(N.$$.fragment,e),f(P.$$.fragment,e),f(w.$$.fragment,e),f(Y.$$.fragment,e),f(X.$$.fragment,e),f(O.$$.fragment,e),f(le.$$.fragment,e),f(ae.$$.fragment,e),f(oe.$$.fragment,e),f(de.$$.fragment,e),f(ue.$$.fragment,e),f(ce.$$.fragment,e),f(Te.$$.fragment,e),f(be.$$.fragment,e),f(ye.$$.fragment,e),f(Ce.$$.fragment,e),f(ve.$$.fragment,e),f(Ie.$$.fragment,e),f(Ee.$$.fragment,e),f(xe.$$.fragment,e),f(Re.$$.fragment,e),f(ke.$$.fragment,e),f(Se.$$.fragment,e),f(Ge.$$.fragment,e),f(Pe.$$.fragment,e),f(ze.$$.fragment,e),f(qe.$$.fragment,e),f(et.$$.fragment,e),f(lt.$$.fragment,e),f(nt.$$.fragment,e),Yl=!0)},o(e){p(b.$$.fragment,e),p(y.$$.fragment,e),p(J.$$.fragment,e),p(v.$$.fragment,e),p(I.$$.fragment,e),p(E.$$.fragment,e),p(R.$$.fragment,e),p(k.$$.fragment,e),p(W.$$.fragment,e),p(G.$$.fragment,e),p(N.$$.fragment,e),p(P.$$.fragment,e),p(w.$$.fragment,e),p(Y.$$.fragment,e),p(X.$$.fragment,e),p(O.$$.fragment,e),p(le.$$.fragment,e),p(ae.$$.fragment,e),p(oe.$$.fragment,e),p(de.$$.fragment,e),p(ue.$$.fragment,e),p(ce.$$.fragment,e),p(Te.$$.fragment,e),p(be.$$.fragment,e),p(ye.$$.fragment,e),p(Ce.$$.fragment,e),p(ve.$$.fragment,e),p(Ie.$$.fragment,e),p(Ee.$$.fragment,e),p(xe.$$.fragment,e),p(Re.$$.fragment,e),p(ke.$$.fragment,e),p(Se.$$.fragment,e),p(Ge.$$.fragment,e),p(Pe.$$.fragment,e),p(ze.$$.fragment,e),p(qe.$$.fragment,e),p(et.$$.fragment,e),p(lt.$$.fragment,e),p(nt.$$.fragment,e),Yl=!1},d(e){e&&(l(g),l(U),l(ot),l(rt),l(dt),l(C),l(mt),l(ut),l($),l(ft),l(pt),l(ht),l(j),l(ct),l(Mt),l(L),l(Tt),l(x),l(gt),l(_),l(wt),l(Ut),l(B),l(bt),l(yt),l(Ct),l(S),l(Jt),l(A),l($t),l(vt),l(Z),l(It),l(V),l(jt),l(F),l(Et),l(Lt),l(H),l(xt),l(Q),l(_t),l(Rt),l(Bt),l(kt),l(Wt),l(z),l(St),l(At),l(D),l(Gt),l(q),l(Zt),l(K),l(Vt),l(ee),l(Ft),l(te),l(Nt),l(Ht),l(Qt),l(ne),l(Pt),l(ie),l(Yt),l(se),l(Xt),l(zt),l(re),l(Ot),l(Dt),l(me),l(qt),l(Kt),l(fe),l(el),l(pe),l(tl),l(he),l(ll),l(al),l(Me),l(nl),l(il),l(ge),l(sl),l(we),l(ol),l(Ue),l(rl),l(dl),l(ml),l(ul),l(Je),l(fl),l($e),l(pl),l(hl),l(cl),l(je),l(Ml),l(Tl),l(Le),l(gl),l(wl),l(_e),l(Ul),l(bl),l(Be),l(yl),l(Cl),l(We),l(Jl),l($l),l(Ae),l(vl),l(Il),l(Ze),l(jl),l(Ve),l(El),l(Fe),l(Ll),l(Ne),l(xl),l(He),l(_l),l(Qe),l(Rl),l(Bl),l(Ye),l(kl),l(Xe),l(Wl),l(Sl),l(Oe),l(Al),l(De),l(Gl),l(Zl),l(Ke),l(Vl),l(Fl),l(tt),l(Nl),l(Hl),l(at),l(Ql),l(Pl),l(it)),l(M),h(b,e),h(y,e),h(J,e),h(v,e),h(I,e),h(E,e),h(R,e),h(k,e),h(W,e),h(G,e),h(N,e),h(P,e),h(w,e),h(Y,e),h(X,e),h(O,e),h(le,e),h(ae,e),h(oe,e),h(de,e),h(ue,e),h(ce,e),h(Te,e),h(be,e),h(ye,e),h(Ce,e),h(ve,e),h(Ie,e),h(Ee,e),h(xe,e),h(Re,e),h(ke,e),h(Se,e),h(Ge,e),h(Pe,e),h(ze,e),h(qe,e),h(et,e),h(lt,e),h(nt,e)}}}const dn='{"title":"Gaudi Backend for Text Generation Inference","local":"gaudi-backend-for-text-generation-inference","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Supported Hardware","local":"supported-hardware","sections":[],"depth":2},{"title":"Tutorial: Getting Started with TGI on Gaudi","local":"tutorial-getting-started-with-tgi-on-gaudi","sections":[{"title":"Basic Usage","local":"basic-usage","sections":[],"depth":3},{"title":"Making Your First Request","local":"making-your-first-request","sections":[],"depth":3}],"depth":2},{"title":"How-to Guides","local":"how-to-guides","sections":[{"title":"How to Enable Multi-Card Inference (Sharding)","local":"how-to-enable-multi-card-inference-sharding","sections":[],"depth":3},{"title":"How to Use Different Precision Formats","local":"how-to-use-different-precision-formats","sections":[{"title":"BF16 Precision (Default)","local":"bf16-precision-default","sections":[],"depth":4},{"title":"FP8 Precision","local":"fp8-precision","sections":[],"depth":4}],"depth":3},{"title":"How to Run Vision-Language Models (VLMs)","local":"how-to-run-vision-language-models-vlms","sections":[],"depth":3},{"title":"How to Benchmark Performance","local":"how-to-benchmark-performance","sections":[],"depth":3},{"title":"How to Profile Performance","local":"how-to-profile-performance","sections":[],"depth":3}],"depth":2},{"title":"Explanation: Understanding TGI on Gaudi","local":"explanation-understanding-tgi-on-gaudi","sections":[{"title":"The Warmup Process","local":"the-warmup-process","sections":[],"depth":3},{"title":"Understanding Parameter Tuning","local":"understanding-parameter-tuning","sections":[{"title":"Sequence Length Parameters","local":"sequence-length-parameters","sections":[],"depth":4},{"title":"Batch Size Parameters","local":"batch-size-parameters","sections":[],"depth":4},{"title":"Performance and Memory Parameters","local":"performance-and-memory-parameters","sections":[],"depth":4},{"title":"Sequence Length Parameters","local":"sequence-length-parameters","sections":[],"depth":4},{"title":"Batch Size Parameters","local":"batch-size-parameters","sections":[],"depth":4}],"depth":3}],"depth":2},{"title":"Reference","local":"reference","sections":[{"title":"Supported Models","local":"supported-models","sections":[],"depth":3},{"title":"Environment Variables","local":"environment-variables","sections":[],"depth":3}],"depth":2},{"title":"Contributing","local":"contributing","sections":[{"title":"Building the Docker Image from Source","local":"building-the-docker-image-from-source","sections":[],"depth":3}],"depth":2}],"depth":1}';function mn(st){return Da(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mn extends qa{constructor(M){super(),Ka(this,M,mn,rn,Oa,{})}}export{Mn as component};
