import{s as I,n as A,o as N}from"../chunks/scheduler.362310b7.js";import{S as R,i as U,g as l,s as u,r as y,A as j,h as _,f as d,c as m,j as M,u as P,x as $,k as z,y as D,a as r,v as E,d as G,t as L,w as B}from"../chunks/index.57dfc70d.js";import{H as O,E as F}from"../chunks/index.fa158b42.js";function J(k){let o,f,p,g,n,b,s,v=`TGI exposes multiple metrics that can be collected via the <code>/metrics</code> Prometheus endpoint.
These metrics can be used to monitor the performance of TGI, autoscale deployment and to help identify bottlenecks.`,q,i,S="The following metrics are exposed:",C,c,w="<thead><tr><th>Metric Name</th> <th>Description</th> <th>Type</th> <th>Unit</th></tr></thead> <tbody><tr><td><code>tgi_batch_current_max_tokens</code></td> <td>Maximum tokens for the current batch</td> <td>Gauge</td> <td>Count</td></tr> <tr><td><code>tgi_batch_current_size</code></td> <td>Current batch size</td> <td>Gauge</td> <td>Count</td></tr> <tr><td><code>tgi_batch_decode_duration</code></td> <td>Time spent decoding a batch per method (prefill or decode)</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_batch_filter_duration</code></td> <td>Time spent filtering batches and sending generated tokens per method (prefill or decode)</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_batch_forward_duration</code></td> <td>Batch forward duration per method (prefill or decode)</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_batch_inference_count</code></td> <td>Inference calls per method (prefill or decode)</td> <td>Counter</td> <td>Count</td></tr> <tr><td><code>tgi_batch_inference_duration</code></td> <td>Batch inference duration</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_batch_inference_success</code></td> <td>Number of successful inference calls per method (prefill or decode)</td> <td>Counter</td> <td>Count</td></tr> <tr><td><code>tgi_batch_next_size</code></td> <td>Batch size of the next batch</td> <td>Histogram</td> <td>Count</td></tr> <tr><td><code>tgi_queue_size</code></td> <td>Current queue size</td> <td>Gauge</td> <td>Count</td></tr> <tr><td><code>tgi_request_count</code></td> <td>Total number of requests</td> <td>Counter</td> <td>Count</td></tr> <tr><td><code>tgi_request_duration</code></td> <td>Total time spent processing the request (e2e latency)</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_request_generated_tokens</code></td> <td>Generated tokens per request</td> <td>Histogram</td> <td>Count</td></tr> <tr><td><code>tgi_request_inference_duration</code></td> <td>Request inference duration</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_request_input_length</code></td> <td>Input token length per request</td> <td>Histogram</td> <td>Count</td></tr> <tr><td><code>tgi_request_max_new_tokens</code></td> <td>Maximum new tokens per request</td> <td>Histogram</td> <td>Count</td></tr> <tr><td><code>tgi_request_mean_time_per_token_duration</code></td> <td>Mean time per token per request (inter-token latency)</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_request_queue_duration</code></td> <td>Time spent in the queue per request</td> <td>Histogram</td> <td>Seconds</td></tr> <tr><td><code>tgi_request_skipped_tokens</code></td> <td>Speculated tokens per request</td> <td>Histogram</td> <td>Count</td></tr> <tr><td><code>tgi_request_success</code></td> <td>Number of successful requests</td> <td>Counter</td> <td></td></tr> <tr><td><code>tgi_request_validation_duration</code></td> <td>Time spent validating the request</td> <td>Histogram</td> <td>Seconds</td></tr></tbody>",H,a,x,h,T;return n=new O({props:{title:"Metrics",local:"metrics",headingTag:"h1"}}),a=new F({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/reference/metrics.md"}}),{c(){o=l("meta"),f=u(),p=l("p"),g=u(),y(n.$$.fragment),b=u(),s=l("p"),s.innerHTML=v,q=u(),i=l("p"),i.textContent=S,C=u(),c=l("table"),c.innerHTML=w,H=u(),y(a.$$.fragment),x=u(),h=l("p"),this.h()},l(t){const e=j("svelte-u9bgzb",document.head);o=_(e,"META",{name:!0,content:!0}),e.forEach(d),f=m(t),p=_(t,"P",{}),M(p).forEach(d),g=m(t),P(n.$$.fragment,t),b=m(t),s=_(t,"P",{"data-svelte-h":!0}),$(s)!=="svelte-ny8d9s"&&(s.innerHTML=v),q=m(t),i=_(t,"P",{"data-svelte-h":!0}),$(i)!=="svelte-gv6qxl"&&(i.textContent=S),C=m(t),c=_(t,"TABLE",{"data-svelte-h":!0}),$(c)!=="svelte-4239tp"&&(c.innerHTML=w),H=m(t),P(a.$$.fragment,t),x=m(t),h=_(t,"P",{}),M(h).forEach(d),this.h()},h(){z(o,"name","hf:doc:metadata"),z(o,"content",K)},m(t,e){D(document.head,o),r(t,f,e),r(t,p,e),r(t,g,e),E(n,t,e),r(t,b,e),r(t,s,e),r(t,q,e),r(t,i,e),r(t,C,e),r(t,c,e),r(t,H,e),E(a,t,e),r(t,x,e),r(t,h,e),T=!0},p:A,i(t){T||(G(n.$$.fragment,t),G(a.$$.fragment,t),T=!0)},o(t){L(n.$$.fragment,t),L(a.$$.fragment,t),T=!1},d(t){t&&(d(f),d(p),d(g),d(b),d(s),d(q),d(i),d(C),d(c),d(H),d(x),d(h)),d(o),B(n,t),B(a,t)}}}const K='{"title":"Metrics","local":"metrics","sections":[],"depth":1}';function Q(k){return N(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Y extends R{constructor(o){super(),U(this,o,Q,J,I,{})}}export{Y as component};
