import{s as K,o as V,n as Y}from"../chunks/scheduler.362310b7.js";import{S as Z,i as ee,g as c,s as i,r as _,A as te,h as f,f as n,c as s,j as U,u as q,x as M,k as J,y as ne,a as o,v as x,d as y,t as P,w as E}from"../chunks/index.57dfc70d.js";import{T as oe}from"../chunks/Tip.14b2ab21.js";import{H as D,E as ae}from"../chunks/index.fa158b42.js";function ie(L){let a,d="We recommend using <code>dynamic</code> RoPE scaling.";return{c(){a=c("p"),a.innerHTML=d},l(l){a=f(l,"P",{"data-svelte-h":!0}),M(a)!=="svelte-jsf1qj"&&(a.innerHTML=d)},m(l,v){o(l,a,v)},p:Y,d(l){l&&n(a)}}}function se(L){let a,d,l,v,p,S,m,X="Text Generation Inference improves the model in several aspects.",H,u,C,h,B='TGI supports <a href="https://github.com/TimDettmers/bitsandbytes#bitsandbytes" rel="nofollow">bits-and-bytes</a>, <a href="https://arxiv.org/abs/2210.17323" rel="nofollow">GPT-Q</a>, <a href="https://arxiv.org/abs/2306.00978" rel="nofollow">AWQ</a>, <a href="https://github.com/IST-DASLab/marlin" rel="nofollow">Marlin</a>, <a href="https://github.com/NetEase-FuXi/EETQ" rel="nofollow">EETQ</a>, <a href="https://github.com/turboderp/exllamav2" rel="nofollow">EXL2</a>, and <a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/" rel="nofollow">fp8</a> quantization. To speed up inference with quantization, simply set <code>quantize</code> flag to <code>bitsandbytes</code>, <code>gptq</code>, <code>awq</code>, <code>marlin</code>, <code>exl2</code>, <code>eetq</code> or <code>fp8</code> depending on the quantization technique you wish to use. When using GPT-Q quantization, you need to point to one of the models <a href="https://huggingface.co/models?search=gptq" rel="nofollow">here</a>. Similarly, when using AWQ quantization, you need to point to one of <a href="https://huggingface.co/models?search=awq" rel="nofollow">these models</a>. To get more information about quantization, please refer to <a href="./../conceptual/quantization">quantization guide</a>',I,g,G,$,F="RoPE scaling can be used to increase the sequence length of the model during the inference time without necessarily fine-tuning it. To enable RoPE scaling, simply pass <code>--rope-scaling</code>, <code>--max-input-length</code> and <code>--rope-factors</code> flags when running through CLI. <code>--rope-scaling</code> can take the values <code>linear</code> or <code>dynamic</code>. If your model is not fine-tuned to a longer sequence length, use <code>dynamic</code>. <code>--rope-factor</code> is the ratio between the intended max sequence length and the model’s original max sequence length. Make sure to pass <code>--max-input-length</code> to provide maximum input length for extension.",Q,r,R,w,A,b,N='<a href="https://github.com/huggingface/safetensors" rel="nofollow">Safetensors</a> is a fast and safe persistence format for deep learning models, and is required for tensor parallelism. TGI supports <code>safetensors</code> model loading under the hood. By default, given a repository with <code>safetensors</code> and <code>pytorch</code> weights, TGI will always load <code>safetensors</code>. If there’s no <code>pytorch</code> weights, TGI will convert the weights to <code>safetensors</code> format.',W,T,j,z,k;return p=new D({props:{title:"Preparing the Model",local:"preparing-the-model",headingTag:"h1"}}),u=new D({props:{title:"Quantization",local:"quantization",headingTag:"h2"}}),g=new D({props:{title:"RoPE Scaling",local:"rope-scaling",headingTag:"h2"}}),r=new oe({props:{$$slots:{default:[ie]},$$scope:{ctx:L}}}),w=new D({props:{title:"Safetensors",local:"safetensors",headingTag:"h2"}}),T=new ae({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/basic_tutorials/preparing_model.md"}}),{c(){a=c("meta"),d=i(),l=c("p"),v=i(),_(p.$$.fragment),S=i(),m=c("p"),m.textContent=X,H=i(),_(u.$$.fragment),C=i(),h=c("p"),h.innerHTML=B,I=i(),_(g.$$.fragment),G=i(),$=c("p"),$.innerHTML=F,Q=i(),_(r.$$.fragment),R=i(),_(w.$$.fragment),A=i(),b=c("p"),b.innerHTML=N,W=i(),_(T.$$.fragment),j=i(),z=c("p"),this.h()},l(e){const t=te("svelte-u9bgzb",document.head);a=f(t,"META",{name:!0,content:!0}),t.forEach(n),d=s(e),l=f(e,"P",{}),U(l).forEach(n),v=s(e),q(p.$$.fragment,e),S=s(e),m=f(e,"P",{"data-svelte-h":!0}),M(m)!=="svelte-14h6xgp"&&(m.textContent=X),H=s(e),q(u.$$.fragment,e),C=s(e),h=f(e,"P",{"data-svelte-h":!0}),M(h)!=="svelte-5dyh9c"&&(h.innerHTML=B),I=s(e),q(g.$$.fragment,e),G=s(e),$=f(e,"P",{"data-svelte-h":!0}),M($)!=="svelte-1o4vern"&&($.innerHTML=F),Q=s(e),q(r.$$.fragment,e),R=s(e),q(w.$$.fragment,e),A=s(e),b=f(e,"P",{"data-svelte-h":!0}),M(b)!=="svelte-mcf08f"&&(b.innerHTML=N),W=s(e),q(T.$$.fragment,e),j=s(e),z=f(e,"P",{}),U(z).forEach(n),this.h()},h(){J(a,"name","hf:doc:metadata"),J(a,"content",le)},m(e,t){ne(document.head,a),o(e,d,t),o(e,l,t),o(e,v,t),x(p,e,t),o(e,S,t),o(e,m,t),o(e,H,t),x(u,e,t),o(e,C,t),o(e,h,t),o(e,I,t),x(g,e,t),o(e,G,t),o(e,$,t),o(e,Q,t),x(r,e,t),o(e,R,t),x(w,e,t),o(e,A,t),o(e,b,t),o(e,W,t),x(T,e,t),o(e,j,t),o(e,z,t),k=!0},p(e,[t]){const O={};t&2&&(O.$$scope={dirty:t,ctx:e}),r.$set(O)},i(e){k||(y(p.$$.fragment,e),y(u.$$.fragment,e),y(g.$$.fragment,e),y(r.$$.fragment,e),y(w.$$.fragment,e),y(T.$$.fragment,e),k=!0)},o(e){P(p.$$.fragment,e),P(u.$$.fragment,e),P(g.$$.fragment,e),P(r.$$.fragment,e),P(w.$$.fragment,e),P(T.$$.fragment,e),k=!1},d(e){e&&(n(d),n(l),n(v),n(S),n(m),n(H),n(C),n(h),n(I),n(G),n($),n(Q),n(R),n(A),n(b),n(W),n(j),n(z)),n(a),E(p,e),E(u,e),E(g,e),E(r,e),E(w,e),E(T,e)}}}const le='{"title":"Preparing the Model","local":"preparing-the-model","sections":[{"title":"Quantization","local":"quantization","sections":[],"depth":2},{"title":"RoPE Scaling","local":"rope-scaling","sections":[],"depth":2},{"title":"Safetensors","local":"safetensors","sections":[],"depth":2}],"depth":1}';function re(L){return V(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class me extends Z{constructor(a){super(),ee(this,a,re,se,K,{})}}export{me as component};
