import{s as ve,n as ye,o as Ce}from"../chunks/scheduler.362310b7.js";import{S as $e,i as Ue,g as o,s as a,r as x,A as _e,h as s,f as n,c as i,j as be,u as A,x as r,k as ge,y as Ie,a as l,v as G,d as J,t as L,w as P}from"../chunks/index.57dfc70d.js";import{C as ke}from"../chunks/CodeBlock.5d40996c.js";import{H as j,E as xe}from"../chunks/index.fa158b42.js";function Ae(oe){let m,H,R,S,p,B,u,se='TGI is supported and tested on <a href="https://www.amd.com/en/products/accelerators/instinct/mi200/mi210.html" rel="nofollow">AMD Instinct MI210</a>, <a href="https://www.amd.com/en/products/accelerators/instinct/mi200/mi250.html" rel="nofollow">MI250</a> and <a href="https://www.amd.com/en/products/accelerators/instinct/mi300.html" rel="nofollow">MI300</a> GPUs. The support may be extended in the future. The recommended usage is through Docker. Make sure to check the <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/docker.html" rel="nofollow">AMD documentation</a> on how to use Docker with AMD GPUs.',O,d,re="On a server powered by AMD GPUs, TGI can be launched with the following command:",E,h,V,f,me='The launched TGI server can then be queried from clients, make sure to check out the <a href="./basic_tutorials/consuming_tgi">Consuming TGI</a> guide.',W,c,F,w,pe='TGI’s docker image for AMD GPUs integrates <a href="https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/cuda/tunable" rel="nofollow">PyTorch’s TunableOp</a>, which allows to do an additional warmup to select the best performing matrix multiplication (GEMM) kernel from rocBLAS or hipBLASLt.',N,T,ue="Experimentally, on MI300X, we noticed a 6-8% latency improvement when using TunableOp on top of ROCm 6.1 and PyTorch 2.3.",z,M,de="TunableOp is enabled by default, the warmup may take 1-2 minutes. In case you would like to disable TunableOp, please pass <code>--env PYTORCH_TUNABLEOP_ENABLED=&quot;0&quot;</code> when launcher TGI’s docker container.",D,b,X,g,he='Two implementations of Flash Attention are available for ROCm, the first is <a href="https://github.com/ROCm/flash-attention" rel="nofollow">ROCm/flash-attention</a> based on a <a href="https://github.com/ROCm/composable_kernel" rel="nofollow">Composable Kernel</a> (CK) implementation, and the second is a <a href="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/layers/attention/flash_attn_triton.py" rel="nofollow">Triton implementation</a>.',Y,v,fe="By default, the Composable Kernel implementation is used. However, the Triton implementation has slightly lower latency on MI250 and MI300, but requires a warmup which can be prohibitive as it needs to be done again for each new prompt length. If needed, FA Triton impelmentation can be enabled with <code>--env ROCM_USE_FLASH_ATTN_V2_TRITON=&quot;0&quot;</code> when launching TGI’s docker container.",Q,y,q,C,ce="For better performance on ROCm, a custom Paged Attention kernel is available and is enabled by default. To disable it and fall back to the PagedAttention v2 kernel, set the environment variable <code>ROCM_USE_CUSTOM_PAGED_ATTN=0</code>.",K,$,we="The custom kernel supports bf16 and fp16 data types, block size of 16, head size of 128, a maximum context length of 16k, and GQA ratios between 1 and 16. For other configurations, we use the PagedAttention v2 kernel.",ee,U,te,_,Te="The following features are currently not supported in the ROCm version of TGI, and the support may be extended in the future:",ne,I,Me='<li>Loading <a href="https://huggingface.co/docs/transformers/quantization#awq" rel="nofollow">AWQ</a> checkpoints.</li> <li>Kernel for sliding window attention (Mistral)</li>',le,k,ae,Z,ie;return p=new j({props:{title:"Using TGI with AMD GPUs",local:"using-tgi-with-amd-gpus",headingTag:"h1"}}),h=new ke({props:{code:"bW9kZWwlM0R0ZWtuaXVtJTJGT3Blbkhlcm1lcy0yLjUtTWlzdHJhbC03QiUwQXZvbHVtZSUzRCUyNFBXRCUyRmRhdGElMjAlMjMlMjBzaGFyZSUyMGElMjB2b2x1bWUlMjB3aXRoJTIwdGhlJTIwRG9ja2VyJTIwY29udGFpbmVyJTIwdG8lMjBhdm9pZCUyMGRvd25sb2FkaW5nJTIwd2VpZ2h0cyUyMGV2ZXJ5JTIwcnVuJTBBJTBBZG9ja2VyJTIwcnVuJTIwLS1ybSUyMC1pdCUyMC0tY2FwLWFkZCUzRFNZU19QVFJBQ0UlMjAtLXNlY3VyaXR5LW9wdCUyMHNlY2NvbXAlM0R1bmNvbmZpbmVkJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1kZXZpY2UlM0QlMkZkZXYlMkZrZmQlMjAtLWRldmljZSUzRCUyRmRldiUyRmRyaSUyMC0tZ3JvdXAtYWRkJTIwdmlkZW8lMjAlNUMlMEElMjAlMjAlMjAlMjAtLWlwYyUzRGhvc3QlMjAtLXNobS1zaXplJTIwMjU2ZyUyMC0tbmV0JTIwaG9zdCUyMC12JTIwJTI0dm9sdW1lJTNBJTJGZGF0YSUyMCU1QyUwQSUyMCUyMCUyMCUyMGdoY3IuaW8lMkZodWdnaW5nZmFjZSUyRnRleHQtZ2VuZXJhdGlvbi1pbmZlcmVuY2UlM0EzLjMuMS1yb2NtJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1tb2RlbC1pZCUyMCUyNG1vZGVs",highlighted:`model=teknium/OpenHermes-2.5-Mistral-7B
volume=<span class="hljs-variable">$PWD</span>/data <span class="hljs-comment"># share a volume with the Docker container to avoid downloading weights every run</span>

docker run --<span class="hljs-built_in">rm</span> -it --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\
    --device=/dev/kfd --device=/dev/dri --group-add video \\
    --ipc=host --shm-size 256g --net host -v <span class="hljs-variable">$volume</span>:/data \\
    ghcr.io/huggingface/text-generation-inference:3.3.1-rocm \\
    --model-id <span class="hljs-variable">$model</span>`,wrap:!1}}),c=new j({props:{title:"TunableOp",local:"tunableop",headingTag:"h2"}}),b=new j({props:{title:"Flash attention implementation",local:"flash-attention-implementation",headingTag:"h2"}}),y=new j({props:{title:"Custom PagedAttention",local:"custom-pagedattention",headingTag:"h2"}}),U=new j({props:{title:"Unsupported features",local:"unsupported-features",headingTag:"h2"}}),k=new xe({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/installation_amd.md"}}),{c(){m=o("meta"),H=a(),R=o("p"),S=a(),x(p.$$.fragment),B=a(),u=o("p"),u.innerHTML=se,O=a(),d=o("p"),d.textContent=re,E=a(),x(h.$$.fragment),V=a(),f=o("p"),f.innerHTML=me,W=a(),x(c.$$.fragment),F=a(),w=o("p"),w.innerHTML=pe,N=a(),T=o("p"),T.textContent=ue,z=a(),M=o("p"),M.innerHTML=de,D=a(),x(b.$$.fragment),X=a(),g=o("p"),g.innerHTML=he,Y=a(),v=o("p"),v.innerHTML=fe,Q=a(),x(y.$$.fragment),q=a(),C=o("p"),C.innerHTML=ce,K=a(),$=o("p"),$.textContent=we,ee=a(),x(U.$$.fragment),te=a(),_=o("p"),_.textContent=Te,ne=a(),I=o("ul"),I.innerHTML=Me,le=a(),x(k.$$.fragment),ae=a(),Z=o("p"),this.h()},l(e){const t=_e("svelte-u9bgzb",document.head);m=s(t,"META",{name:!0,content:!0}),t.forEach(n),H=i(e),R=s(e,"P",{}),be(R).forEach(n),S=i(e),A(p.$$.fragment,e),B=i(e),u=s(e,"P",{"data-svelte-h":!0}),r(u)!=="svelte-1oiskdb"&&(u.innerHTML=se),O=i(e),d=s(e,"P",{"data-svelte-h":!0}),r(d)!=="svelte-1culvup"&&(d.textContent=re),E=i(e),A(h.$$.fragment,e),V=i(e),f=s(e,"P",{"data-svelte-h":!0}),r(f)!=="svelte-uo4xf6"&&(f.innerHTML=me),W=i(e),A(c.$$.fragment,e),F=i(e),w=s(e,"P",{"data-svelte-h":!0}),r(w)!=="svelte-9xgviz"&&(w.innerHTML=pe),N=i(e),T=s(e,"P",{"data-svelte-h":!0}),r(T)!=="svelte-fxh8t6"&&(T.textContent=ue),z=i(e),M=s(e,"P",{"data-svelte-h":!0}),r(M)!=="svelte-1pxq52n"&&(M.innerHTML=de),D=i(e),A(b.$$.fragment,e),X=i(e),g=s(e,"P",{"data-svelte-h":!0}),r(g)!=="svelte-vrgeg6"&&(g.innerHTML=he),Y=i(e),v=s(e,"P",{"data-svelte-h":!0}),r(v)!=="svelte-16up6ld"&&(v.innerHTML=fe),Q=i(e),A(y.$$.fragment,e),q=i(e),C=s(e,"P",{"data-svelte-h":!0}),r(C)!=="svelte-pt23lw"&&(C.innerHTML=ce),K=i(e),$=s(e,"P",{"data-svelte-h":!0}),r($)!=="svelte-smd0oi"&&($.textContent=we),ee=i(e),A(U.$$.fragment,e),te=i(e),_=s(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-1ytqs59"&&(_.textContent=Te),ne=i(e),I=s(e,"UL",{"data-svelte-h":!0}),r(I)!=="svelte-xh8ses"&&(I.innerHTML=Me),le=i(e),A(k.$$.fragment,e),ae=i(e),Z=s(e,"P",{}),be(Z).forEach(n),this.h()},h(){ge(m,"name","hf:doc:metadata"),ge(m,"content",Ge)},m(e,t){Ie(document.head,m),l(e,H,t),l(e,R,t),l(e,S,t),G(p,e,t),l(e,B,t),l(e,u,t),l(e,O,t),l(e,d,t),l(e,E,t),G(h,e,t),l(e,V,t),l(e,f,t),l(e,W,t),G(c,e,t),l(e,F,t),l(e,w,t),l(e,N,t),l(e,T,t),l(e,z,t),l(e,M,t),l(e,D,t),G(b,e,t),l(e,X,t),l(e,g,t),l(e,Y,t),l(e,v,t),l(e,Q,t),G(y,e,t),l(e,q,t),l(e,C,t),l(e,K,t),l(e,$,t),l(e,ee,t),G(U,e,t),l(e,te,t),l(e,_,t),l(e,ne,t),l(e,I,t),l(e,le,t),G(k,e,t),l(e,ae,t),l(e,Z,t),ie=!0},p:ye,i(e){ie||(J(p.$$.fragment,e),J(h.$$.fragment,e),J(c.$$.fragment,e),J(b.$$.fragment,e),J(y.$$.fragment,e),J(U.$$.fragment,e),J(k.$$.fragment,e),ie=!0)},o(e){L(p.$$.fragment,e),L(h.$$.fragment,e),L(c.$$.fragment,e),L(b.$$.fragment,e),L(y.$$.fragment,e),L(U.$$.fragment,e),L(k.$$.fragment,e),ie=!1},d(e){e&&(n(H),n(R),n(S),n(B),n(u),n(O),n(d),n(E),n(V),n(f),n(W),n(F),n(w),n(N),n(T),n(z),n(M),n(D),n(X),n(g),n(Y),n(v),n(Q),n(q),n(C),n(K),n($),n(ee),n(te),n(_),n(ne),n(I),n(le),n(ae),n(Z)),n(m),P(p,e),P(h,e),P(c,e),P(b,e),P(y,e),P(U,e),P(k,e)}}}const Ge='{"title":"Using TGI with AMD GPUs","local":"using-tgi-with-amd-gpus","sections":[{"title":"TunableOp","local":"tunableop","sections":[],"depth":2},{"title":"Flash attention implementation","local":"flash-attention-implementation","sections":[],"depth":2},{"title":"Custom PagedAttention","local":"custom-pagedattention","sections":[],"depth":2},{"title":"Unsupported features","local":"unsupported-features","sections":[],"depth":2}],"depth":1}';function Je(oe){return Ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class je extends $e{constructor(m){super(),Ue(this,m,Je,Ae,ve,{})}}export{je as component};
