import{s as Wt,n as Bt,o as qt}from"../chunks/scheduler.362310b7.js";import{S as Pt,i as zt,g as c,s as i,r as s,A as Dt,h as u,f as a,c as n,j as Lt,u as d,x as y,k as Nt,y as Yt,a as l,v as r,d as o,t as m,w as p}from"../chunks/index.57dfc70d.js";import{C as q}from"../chunks/CodeBlock.5d40996c.js";import{H as f,E as Xt}from"../chunks/index.fa158b42.js";function Ot(It){let M,D,P,Y,U,X,g,At=`The llamacpp backend facilitates the deployment of large language models
(LLMs) by integrating <a href="https://github.com/ggerganov/llama.cpp" rel="nofollow">llama.cpp</a>, an advanced inference engine
optimized for both CPU and GPU computation. This backend is a component
of Hugging Face’s <strong>Text Generation Inference (TGI)</strong> suite,
specifically designed to streamline the deployment of LLMs in production
environments.`,O,$,K,b,kt=`<li>Full compatibility with GGUF format and all quantization formats
(GGUF-related constraints may be mitigated dynamically by on-the-fly
generation in future updates)</li> <li>Optimized inference on CPU and GPU architectures</li> <li>Containerized deployment, eliminating dependency complexity</li> <li>Seamless interoperability with the Hugging Face ecosystem</li>`,tt,h,et,C,Qt=`This backend leverages models formatted in <strong>GGUF</strong>, providing an
optimized balance between computational efficiency and model accuracy.
You will find the best models on <a href="https://huggingface.co/models?library=gguf&amp;sort=trending" rel="nofollow">Hugging Face</a>.`,at,w,lt,T,xt=`For optimal performance, the Docker image is compiled with native CPU
instructions by default. As a result, it is strongly recommended to run
the container on the same host architecture used during the build
process. Efforts are ongoing to improve portability across different
systems while preserving high computational efficiency.`,it,J,St="To build the Docker image, use the following command:",nt,j,st,v,dt,G,Ht="<thead><tr><th>Parameter (with —build-arg)</th> <th>Description</th></tr></thead> <tbody><tr><td><code>llamacpp_version=bXXXX</code></td> <td>Specific version of llama.cpp</td></tr> <tr><td><code>llamacpp_cuda=ON</code></td> <td>Enables CUDA acceleration</td></tr> <tr><td><code>llamacpp_native=OFF</code></td> <td>Disable automatic CPU detection</td></tr> <tr><td><code>llamacpp_cpu_arm_arch=ARCH[+FEATURE]...</code></td> <td>Specific ARM CPU and features</td></tr> <tr><td><code>cuda_arch=ARCH</code></td> <td>Defines target CUDA architecture</td></tr></tbody>",rt,I,_t=`For example, to target Graviton4 when building on another ARM
architecture:`,ot,A,mt,k,pt,Q,ct,x,ut,S,yt,H,ft,_,Mt,F,Ft=`GGUF files are optional as they will be automatically generated at
startup if not already present in the <code>models</code> directory. However, if
the default GGUF generation is not suitable for your use case, you can
provide your own GGUF file with <code>--model-gguf</code>, for example:`,Ut,V,gt,R,Vt="Note that <code>--model-id</code> is still required.",$t,Z,bt,E,Rt="A full listing of configurable parameters is available in the <code>--help</code>:",ht,L,Ct,N,Zt="The table below summarizes key options:",wt,W,Et="<thead><tr><th>Parameter</th> <th>Description</th></tr></thead> <tbody><tr><td><code>--n-threads</code></td> <td>Number of threads to use for generation</td></tr> <tr><td><code>--n-threads-batch</code></td> <td>Number of threads to use for batch processing</td></tr> <tr><td><code>--n-gpu-layers</code></td> <td>Number of layers to store in VRAM</td></tr> <tr><td><code>--split-mode</code></td> <td>Split the model across multiple GPUs</td></tr> <tr><td><code>--defrag-threshold</code></td> <td>Defragment the KV cache if holes/size &gt; threshold</td></tr> <tr><td><code>--numa</code></td> <td>Enable NUMA optimizations</td></tr> <tr><td><code>--disable-mmap</code></td> <td>Disable memory mapping for the model</td></tr> <tr><td><code>--use-mlock</code></td> <td>Use memory locking to prevent swapping</td></tr> <tr><td><code>--disable-offload-kqv</code></td> <td>Disable offloading of KQV operations to the GPU</td></tr> <tr><td><code>--disable-flash-attention</code></td> <td>Disable flash attention</td></tr> <tr><td><code>--type-k</code></td> <td>Data type used for K cache</td></tr> <tr><td><code>--type-v</code></td> <td>Data type used for V cache</td></tr> <tr><td><code>--validation-workers</code></td> <td>Number of tokenizer workers used for payload validation and truncation</td></tr> <tr><td><code>--max-concurrent-requests</code></td> <td>Maximum number of concurrent requests</td></tr> <tr><td><code>--max-input-tokens</code></td> <td>Maximum number of input tokens per request</td></tr> <tr><td><code>--max-total-tokens</code></td> <td>Maximum number of total tokens (input + output) per request</td></tr> <tr><td><code>--max-batch-total-tokens</code></td> <td>Maximum number of tokens in a batch</td></tr> <tr><td><code>--max-physical-batch-total-tokens</code></td> <td>Maximum number of tokens in a physical batch</td></tr> <tr><td><code>--max-batch-size</code></td> <td>Maximum number of requests per batch</td></tr></tbody>",Tt,Jt,jt,B,vt,z,Gt;return U=new f({props:{title:"Llamacpp Backend",local:"llamacpp-backend",headingTag:"h1"}}),$=new f({props:{title:"Key Capabilities",local:"key-capabilities",headingTag:"h2"}}),h=new f({props:{title:"Model Compatibility",local:"model-compatibility",headingTag:"h2"}}),w=new f({props:{title:"Build Docker image",local:"build-docker-image",headingTag:"h2"}}),j=new q({props:{code:"ZG9ja2VyJTIwYnVpbGQlMjAlNUMlMEElMjAlMjAlMjAlMjAtdCUyMHRnaS1sbGFtYWNwcCUyMCU1QyUwQSUyMCUyMCUyMCUyMGh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmh1Z2dpbmdmYWNlJTJGdGV4dC1nZW5lcmF0aW9uLWluZmVyZW5jZS5naXQlMjAlNUMlMEElMjAlMjAlMjAlMjAtZiUyMERvY2tlcmZpbGVfbGxhbWFjcHA=",highlighted:`docker build \\
    -t tgi-llamacpp \\
    https://github.com/huggingface/text-generation-inference.git \\
    -f Dockerfile_llamacpp`,wrap:!1}}),v=new f({props:{title:"Build parameters",local:"build-parameters",headingTag:"h3"}}),A=new q({props:{code:"ZG9ja2VyJTIwYnVpbGQlMjAlNUMlMEElMjAlMjAlMjAlMjAtdCUyMHRnaS1sbGFtYWNwcCUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tYnVpbGQtYXJnJTIwbGxhbWFjcHBfbmF0aXZlJTNET0ZGJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1idWlsZC1hcmclMjBsbGFtYWNwcF9jcHVfYXJtX2FyY2glM0Rhcm12OS1hJTJCaThtbSUyMCU1QyUwQSUyMCUyMCUyMCUyMGh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmh1Z2dpbmdmYWNlJTJGdGV4dC1nZW5lcmF0aW9uLWluZmVyZW5jZS5naXQlMjAlNUMlMEElMjAlMjAlMjAlMjAtZiUyMERvY2tlcmZpbGVfbGxhbWFjcHA=",highlighted:`docker build \\
    -t tgi-llamacpp \\
    --build-arg llamacpp_native=OFF \\
    --build-arg llamacpp_cpu_arm_arch=armv9-a+i8mm \\
    https://github.com/huggingface/text-generation-inference.git \\
    -f Dockerfile_llamacpp`,wrap:!1}}),k=new f({props:{title:"Run Docker image",local:"run-docker-image",headingTag:"h2"}}),Q=new f({props:{title:"CPU-based inference",local:"cpu-based-inference",headingTag:"h3"}}),x=new q({props:{code:"ZG9ja2VyJTIwcnVuJTIwJTVDJTBBJTIwJTIwJTIwJTIwLXAlMjAzMDAwJTNBMzAwMCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwJTIySEZfVE9LRU4lM0QlMjRIRl9UT0tFTiUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMC12JTIwJTIyJTI0SE9NRSUyRm1vZGVscyUzQSUyRmFwcCUyRm1vZGVscyUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMHRnaS1sbGFtYWNwcCUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbW9kZWwtaWQlMjAlMjJRd2VuJTJGUXdlbjIuNS0zQi1JbnN0cnVjdCUyMg==",highlighted:`docker run \\
    -p 3000:3000 \\
    -e <span class="hljs-string">&quot;HF_TOKEN=<span class="hljs-variable">$HF_TOKEN</span>&quot;</span> \\
    -v <span class="hljs-string">&quot;<span class="hljs-variable">$HOME</span>/models:/app/models&quot;</span> \\
    tgi-llamacpp \\
    --model-id <span class="hljs-string">&quot;Qwen/Qwen2.5-3B-Instruct&quot;</span>`,wrap:!1}}),S=new f({props:{title:"GPU-Accelerated inference",local:"gpu-accelerated-inference",headingTag:"h3"}}),H=new q({props:{code:"ZG9ja2VyJTIwcnVuJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1ncHVzJTIwYWxsJTIwJTVDJTBBJTIwJTIwJTIwJTIwLXAlMjAzMDAwJTNBMzAwMCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwJTIySEZfVE9LRU4lM0QlMjRIRl9UT0tFTiUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMC12JTIwJTIyJTI0SE9NRSUyRm1vZGVscyUzQSUyRmFwcCUyRm1vZGVscyUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMHRnaS1sbGFtYWNwcCUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbi1ncHUtbGF5ZXJzJTIwOTklMEElMjAlMjAlMjAlMjAtLW1vZGVsLWlkJTIwJTIyUXdlbiUyRlF3ZW4yLjUtM0ItSW5zdHJ1Y3QlMjI=",highlighted:`docker run \\
    --gpus all \\
    -p 3000:3000 \\
    -e <span class="hljs-string">&quot;HF_TOKEN=<span class="hljs-variable">$HF_TOKEN</span>&quot;</span> \\
    -v <span class="hljs-string">&quot;<span class="hljs-variable">$HOME</span>/models:/app/models&quot;</span> \\
    tgi-llamacpp \\
    --n-gpu-layers 99
    --model-id <span class="hljs-string">&quot;Qwen/Qwen2.5-3B-Instruct&quot;</span>`,wrap:!1}}),_=new f({props:{title:"Using a custom GGUF",local:"using-a-custom-gguf",headingTag:"h2"}}),V=new q({props:{code:"ZG9ja2VyJTIwcnVuJTIwJTVDJTBBJTIwJTIwJTIwJTIwLXAlMjAzMDAwJTNBMzAwMCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1lJTIwJTIySEZfVE9LRU4lM0QlMjRIRl9UT0tFTiUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMC12JTIwJTIyJTI0SE9NRSUyRm1vZGVscyUzQSUyRmFwcCUyRm1vZGVscyUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMHRnaS1sbGFtYWNwcCUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbW9kZWwtaWQlMjAlMjJRd2VuJTJGUXdlbjIuNS0zQi1JbnN0cnVjdCUyMiUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbW9kZWwtZ2d1ZiUyMCUyMm1vZGVscyUyRnF3ZW4yLjUtM2ItaW5zdHJ1Y3QtcTRfMC5nZ3VmJTIy",highlighted:`docker run \\
    -p 3000:3000 \\
    -e <span class="hljs-string">&quot;HF_TOKEN=<span class="hljs-variable">$HF_TOKEN</span>&quot;</span> \\
    -v <span class="hljs-string">&quot;<span class="hljs-variable">$HOME</span>/models:/app/models&quot;</span> \\
    tgi-llamacpp \\
    --model-id <span class="hljs-string">&quot;Qwen/Qwen2.5-3B-Instruct&quot;</span> \\
    --model-gguf <span class="hljs-string">&quot;models/qwen2.5-3b-instruct-q4_0.gguf&quot;</span>`,wrap:!1}}),Z=new f({props:{title:"Advanced parameters",local:"advanced-parameters",headingTag:"h2"}}),L=new q({props:{code:"ZG9ja2VyJTIwcnVuJTIwdGdpLWxsYW1hY3BwJTIwLS1oZWxwJTBB",highlighted:`docker run tgi-llamacpp --<span class="hljs-built_in">help</span>
`,wrap:!1}}),B=new Xt({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/backends/llamacpp.md"}}),{c(){M=c("meta"),D=i(),P=c("p"),Y=i(),s(U.$$.fragment),X=i(),g=c("p"),g.innerHTML=At,O=i(),s($.$$.fragment),K=i(),b=c("ul"),b.innerHTML=kt,tt=i(),s(h.$$.fragment),et=i(),C=c("p"),C.innerHTML=Qt,at=i(),s(w.$$.fragment),lt=i(),T=c("p"),T.textContent=xt,it=i(),J=c("p"),J.textContent=St,nt=i(),s(j.$$.fragment),st=i(),s(v.$$.fragment),dt=i(),G=c("table"),G.innerHTML=Ht,rt=i(),I=c("p"),I.textContent=_t,ot=i(),s(A.$$.fragment),mt=i(),s(k.$$.fragment),pt=i(),s(Q.$$.fragment),ct=i(),s(x.$$.fragment),ut=i(),s(S.$$.fragment),yt=i(),s(H.$$.fragment),ft=i(),s(_.$$.fragment),Mt=i(),F=c("p"),F.innerHTML=Ft,Ut=i(),s(V.$$.fragment),gt=i(),R=c("p"),R.innerHTML=Vt,$t=i(),s(Z.$$.fragment),bt=i(),E=c("p"),E.innerHTML=Rt,ht=i(),s(L.$$.fragment),Ct=i(),N=c("p"),N.textContent=Zt,wt=i(),W=c("table"),W.innerHTML=Et,Tt=i(),Jt=c("hr"),jt=i(),s(B.$$.fragment),vt=i(),z=c("p"),this.h()},l(t){const e=Dt("svelte-u9bgzb",document.head);M=u(e,"META",{name:!0,content:!0}),e.forEach(a),D=n(t),P=u(t,"P",{}),Lt(P).forEach(a),Y=n(t),d(U.$$.fragment,t),X=n(t),g=u(t,"P",{"data-svelte-h":!0}),y(g)!=="svelte-1dj8yor"&&(g.innerHTML=At),O=n(t),d($.$$.fragment,t),K=n(t),b=u(t,"UL",{"data-svelte-h":!0}),y(b)!=="svelte-x2nd8x"&&(b.innerHTML=kt),tt=n(t),d(h.$$.fragment,t),et=n(t),C=u(t,"P",{"data-svelte-h":!0}),y(C)!=="svelte-ojliya"&&(C.innerHTML=Qt),at=n(t),d(w.$$.fragment,t),lt=n(t),T=u(t,"P",{"data-svelte-h":!0}),y(T)!=="svelte-191arso"&&(T.textContent=xt),it=n(t),J=u(t,"P",{"data-svelte-h":!0}),y(J)!=="svelte-137845n"&&(J.textContent=St),nt=n(t),d(j.$$.fragment,t),st=n(t),d(v.$$.fragment,t),dt=n(t),G=u(t,"TABLE",{"data-svelte-h":!0}),y(G)!=="svelte-cxjc79"&&(G.innerHTML=Ht),rt=n(t),I=u(t,"P",{"data-svelte-h":!0}),y(I)!=="svelte-13bl7a0"&&(I.textContent=_t),ot=n(t),d(A.$$.fragment,t),mt=n(t),d(k.$$.fragment,t),pt=n(t),d(Q.$$.fragment,t),ct=n(t),d(x.$$.fragment,t),ut=n(t),d(S.$$.fragment,t),yt=n(t),d(H.$$.fragment,t),ft=n(t),d(_.$$.fragment,t),Mt=n(t),F=u(t,"P",{"data-svelte-h":!0}),y(F)!=="svelte-p31jg4"&&(F.innerHTML=Ft),Ut=n(t),d(V.$$.fragment,t),gt=n(t),R=u(t,"P",{"data-svelte-h":!0}),y(R)!=="svelte-f556aw"&&(R.innerHTML=Vt),$t=n(t),d(Z.$$.fragment,t),bt=n(t),E=u(t,"P",{"data-svelte-h":!0}),y(E)!=="svelte-4vg0j3"&&(E.innerHTML=Rt),ht=n(t),d(L.$$.fragment,t),Ct=n(t),N=u(t,"P",{"data-svelte-h":!0}),y(N)!=="svelte-16y2brx"&&(N.textContent=Zt),wt=n(t),W=u(t,"TABLE",{"data-svelte-h":!0}),y(W)!=="svelte-cqsuz1"&&(W.innerHTML=Et),Tt=n(t),Jt=u(t,"HR",{}),jt=n(t),d(B.$$.fragment,t),vt=n(t),z=u(t,"P",{}),Lt(z).forEach(a),this.h()},h(){Nt(M,"name","hf:doc:metadata"),Nt(M,"content",Kt)},m(t,e){Yt(document.head,M),l(t,D,e),l(t,P,e),l(t,Y,e),r(U,t,e),l(t,X,e),l(t,g,e),l(t,O,e),r($,t,e),l(t,K,e),l(t,b,e),l(t,tt,e),r(h,t,e),l(t,et,e),l(t,C,e),l(t,at,e),r(w,t,e),l(t,lt,e),l(t,T,e),l(t,it,e),l(t,J,e),l(t,nt,e),r(j,t,e),l(t,st,e),r(v,t,e),l(t,dt,e),l(t,G,e),l(t,rt,e),l(t,I,e),l(t,ot,e),r(A,t,e),l(t,mt,e),r(k,t,e),l(t,pt,e),r(Q,t,e),l(t,ct,e),r(x,t,e),l(t,ut,e),r(S,t,e),l(t,yt,e),r(H,t,e),l(t,ft,e),r(_,t,e),l(t,Mt,e),l(t,F,e),l(t,Ut,e),r(V,t,e),l(t,gt,e),l(t,R,e),l(t,$t,e),r(Z,t,e),l(t,bt,e),l(t,E,e),l(t,ht,e),r(L,t,e),l(t,Ct,e),l(t,N,e),l(t,wt,e),l(t,W,e),l(t,Tt,e),l(t,Jt,e),l(t,jt,e),r(B,t,e),l(t,vt,e),l(t,z,e),Gt=!0},p:Bt,i(t){Gt||(o(U.$$.fragment,t),o($.$$.fragment,t),o(h.$$.fragment,t),o(w.$$.fragment,t),o(j.$$.fragment,t),o(v.$$.fragment,t),o(A.$$.fragment,t),o(k.$$.fragment,t),o(Q.$$.fragment,t),o(x.$$.fragment,t),o(S.$$.fragment,t),o(H.$$.fragment,t),o(_.$$.fragment,t),o(V.$$.fragment,t),o(Z.$$.fragment,t),o(L.$$.fragment,t),o(B.$$.fragment,t),Gt=!0)},o(t){m(U.$$.fragment,t),m($.$$.fragment,t),m(h.$$.fragment,t),m(w.$$.fragment,t),m(j.$$.fragment,t),m(v.$$.fragment,t),m(A.$$.fragment,t),m(k.$$.fragment,t),m(Q.$$.fragment,t),m(x.$$.fragment,t),m(S.$$.fragment,t),m(H.$$.fragment,t),m(_.$$.fragment,t),m(V.$$.fragment,t),m(Z.$$.fragment,t),m(L.$$.fragment,t),m(B.$$.fragment,t),Gt=!1},d(t){t&&(a(D),a(P),a(Y),a(X),a(g),a(O),a(K),a(b),a(tt),a(et),a(C),a(at),a(lt),a(T),a(it),a(J),a(nt),a(st),a(dt),a(G),a(rt),a(I),a(ot),a(mt),a(pt),a(ct),a(ut),a(yt),a(ft),a(Mt),a(F),a(Ut),a(gt),a(R),a($t),a(bt),a(E),a(ht),a(Ct),a(N),a(wt),a(W),a(Tt),a(Jt),a(jt),a(vt),a(z)),a(M),p(U,t),p($,t),p(h,t),p(w,t),p(j,t),p(v,t),p(A,t),p(k,t),p(Q,t),p(x,t),p(S,t),p(H,t),p(_,t),p(V,t),p(Z,t),p(L,t),p(B,t)}}}const Kt='{"title":"Llamacpp Backend","local":"llamacpp-backend","sections":[{"title":"Key Capabilities","local":"key-capabilities","sections":[],"depth":2},{"title":"Model Compatibility","local":"model-compatibility","sections":[],"depth":2},{"title":"Build Docker image","local":"build-docker-image","sections":[{"title":"Build parameters","local":"build-parameters","sections":[],"depth":3}],"depth":2},{"title":"Run Docker image","local":"run-docker-image","sections":[{"title":"CPU-based inference","local":"cpu-based-inference","sections":[],"depth":3},{"title":"GPU-Accelerated inference","local":"gpu-accelerated-inference","sections":[],"depth":3}],"depth":2},{"title":"Using a custom GGUF","local":"using-a-custom-gguf","sections":[],"depth":2},{"title":"Advanced parameters","local":"advanced-parameters","sections":[],"depth":2}],"depth":1}';function te(It){return qt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ne extends Pt{constructor(M){super(),zt(this,M,te,Ot,Wt,{})}}export{ne as component};
