import{s as D,n as N,o as X}from"../chunks/scheduler.362310b7.js";import{S as J,i as K,g as s,s as i,r as U,A as V,h as r,f as n,c as o,j,u as q,x,k as F,y as Y,a,v as Q,d as R,t as W,w as B}from"../chunks/index.57dfc70d.js";import{H as Z,E as ee}from"../chunks/index.fa158b42.js";function te(H){let l,b,v,T,u,w,c,E="Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and T5.",_,p,I='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="Text Generation Inference"/>',y,f,k="Text Generation Inference implements many optimizations and features, such as:",$,m,z='<li>Simple launcher to serve most popular LLMs</li> <li>Production ready (distributed tracing with Open Telemetry, Prometheus metrics)</li> <li>Tensor Parallelism for faster inference on multiple GPUs</li> <li>Token streaming using Server-Sent Events (SSE)</li> <li>Continuous batching of incoming requests for increased total throughput</li> <li>Optimized transformers code for inference using <a href="https://github.com/HazyResearch/flash-attention" rel="nofollow">Flash Attention</a> and <a href="https://github.com/vllm-project/vllm" rel="nofollow">Paged Attention</a> on the most popular architectures</li> <li>Quantization with <a href="https://github.com/TimDettmers/bitsandbytes" rel="nofollow">bitsandbytes</a> and <a href="https://arxiv.org/abs/2210.17323" rel="nofollow">GPT-Q</a></li> <li><a href="https://github.com/huggingface/safetensors" rel="nofollow">Safetensors</a> weight loading</li> <li>Watermarking with <a href="https://arxiv.org/abs/2301.10226" rel="nofollow">A Watermark for Large Language Models</a></li> <li>Logits warper (temperature scaling, top-p, top-k, repetition penalty)</li> <li>Stop sequences</li> <li>Log probabilities</li> <li>Fine-tuning Support: Utilize fine-tuned models for specific tasks to achieve higher accuracy and performance.</li> <li><a href="conceptual/guidance">Guidance</a>: Enable function calling and tool-use by forcing the model to generate structured outputs based on your own predefined output schemas.</li>',P,h,A="Text Generation Inference is used in production by multiple projects, such as:",C,g,O='<li><a href="https://github.com/huggingface/chat-ui" rel="nofollow">Hugging Chat</a>, an open-source interface for open-access models, such as Open Assistant and Llama</li> <li><a href="https://open-assistant.io/" rel="nofollow">OpenAssistant</a>, an open-source community effort to train LLMs in the open</li> <li><a href="http://nat.dev/" rel="nofollow">nat.dev</a>, a playground to explore and compare LLMs.</li>',M,d,G,L,S;return u=new Z({props:{title:"Text Generation Inference",local:"text-generation-inference",headingTag:"h1"}}),d=new ee({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/index.md"}}),{c(){l=s("meta"),b=i(),v=s("p"),T=i(),U(u.$$.fragment),w=i(),c=s("p"),c.textContent=E,_=i(),p=s("p"),p.innerHTML=I,y=i(),f=s("p"),f.textContent=k,$=i(),m=s("ul"),m.innerHTML=z,P=i(),h=s("p"),h.textContent=A,C=i(),g=s("ul"),g.innerHTML=O,M=i(),U(d.$$.fragment),G=i(),L=s("p"),this.h()},l(e){const t=V("svelte-u9bgzb",document.head);l=r(t,"META",{name:!0,content:!0}),t.forEach(n),b=o(e),v=r(e,"P",{}),j(v).forEach(n),T=o(e),q(u.$$.fragment,e),w=o(e),c=r(e,"P",{"data-svelte-h":!0}),x(c)!=="svelte-m7gv0"&&(c.textContent=E),_=o(e),p=r(e,"P",{"data-svelte-h":!0}),x(p)!=="svelte-1rntzol"&&(p.innerHTML=I),y=o(e),f=r(e,"P",{"data-svelte-h":!0}),x(f)!=="svelte-u1ae6o"&&(f.textContent=k),$=o(e),m=r(e,"UL",{"data-svelte-h":!0}),x(m)!=="svelte-hit0a2"&&(m.innerHTML=z),P=o(e),h=r(e,"P",{"data-svelte-h":!0}),x(h)!=="svelte-uziyu5"&&(h.textContent=A),C=o(e),g=r(e,"UL",{"data-svelte-h":!0}),x(g)!=="svelte-stiskr"&&(g.innerHTML=O),M=o(e),q(d.$$.fragment,e),G=o(e),L=r(e,"P",{}),j(L).forEach(n),this.h()},h(){F(l,"name","hf:doc:metadata"),F(l,"content",ne)},m(e,t){Y(document.head,l),a(e,b,t),a(e,v,t),a(e,T,t),Q(u,e,t),a(e,w,t),a(e,c,t),a(e,_,t),a(e,p,t),a(e,y,t),a(e,f,t),a(e,$,t),a(e,m,t),a(e,P,t),a(e,h,t),a(e,C,t),a(e,g,t),a(e,M,t),Q(d,e,t),a(e,G,t),a(e,L,t),S=!0},p:N,i(e){S||(R(u.$$.fragment,e),R(d.$$.fragment,e),S=!0)},o(e){W(u.$$.fragment,e),W(d.$$.fragment,e),S=!1},d(e){e&&(n(b),n(v),n(T),n(w),n(c),n(_),n(p),n(y),n(f),n($),n(m),n(P),n(h),n(C),n(g),n(M),n(G),n(L)),n(l),B(u,e),B(d,e)}}}const ne='{"title":"Text Generation Inference","local":"text-generation-inference","sections":[],"depth":1}';function ae(H){return X(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class se extends J{constructor(l){super(),K(this,l,ae,te,D,{})}}export{se as component};
