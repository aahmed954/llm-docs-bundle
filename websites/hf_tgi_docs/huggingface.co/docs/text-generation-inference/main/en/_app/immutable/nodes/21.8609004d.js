import{s as at,n as lt,o as nt}from"../chunks/scheduler.362310b7.js";import{S as st,i as it,g as i,s as n,r as u,A as pt,h as p,f as a,c as s,j as et,u as r,x as o,k as tt,y as ot,a as l,v as m,d,t as c,w as h}from"../chunks/index.57dfc70d.js";import{C as N}from"../chunks/CodeBlock.5d40996c.js";import{H as X,E as ut}from"../chunks/index.fa158b42.js";function rt(He){let f,K,D,ee,y,te,M,ae,w,Ze="LoRA is a technique that allows for efficent fine-tuning a model while only updating a small portion of the model’s weights. This is useful when you have a large model that has been pre-trained on a large dataset, but you want to fine-tune it on a smaller dataset or for a specific task.",le,$,Ee="LoRA works by adding a small number of additional weights to the model, which are used to adapt the model to the new dataset or task. These additional weights are learned during the fine-tuning process, while the rest of the model’s weights are kept fixed.",ne,g,se,U,Ge="LoRA can be used in many ways and the community is always finding new ways to use it. Here are some examples of how you can use LoRA:",ie,b,Ie="Technically, LoRA can be used to fine-tune a large language model on a small dataset. However, these use cases can span a wide range of applications, such as:",pe,j,Qe="<li>fine-tuning a language model on a small dataset</li> <li>fine-tuning a language model on a domain-specific dataset</li> <li>fine-tuning a language model on a dataset with limited labels</li>",oe,T,ue,R,qe='LoRA’s can be used during inference by mutliplying the adapter weights with the model weights at each specified layer. This process can be computationally expensive, but due to awesome work by <a href="https://github.com/punica-ai/punica" rel="nofollow">punica-ai</a> and the <a href="https://github.com/predibase/lorax" rel="nofollow">lorax</a> team, optimized kernels/and frameworks have been developed to make this process more efficient. TGI leverages these optimizations in order to provide fast and efficient inference with mulitple LoRA models.',re,C,me,v,Ve="Once a LoRA model has been trained, it can be used to generate text or perform other tasks just like a regular language model. However, because the model has been fine-tuned on a specific dataset, it may perform better on that dataset than a model that has not been fine-tuned.",de,A,ze="In practice its often useful to have multiple LoRA models, each fine-tuned on a different dataset or for a different task. This allows you to use the model that is best suited for a particular task or dataset.",ce,x,We="Text Generation Inference (TGI) now supports loading multiple LoRA models at startup that can be used in generation requests. This feature is available starting from version <code>~2.0.6</code> and is compatible with LoRA models trained using the <code>peft</code> library.",he,L,fe,J,Ye="To use LoRA in TGI, when starting the server, you can specify the list of LoRA models to load using the <code>LORA_ADAPTERS</code> environment variable. For example:",ye,_,Me,k,Be="To specify model revision, use <code>adapter_id@revision</code>, as follows:",we,S,$e,H,Pe="To use a locally stored lora adapter, use <code>adapter-name=/path/to/adapter</code>, as seen below. When you want to use this adapter, set <code>&quot;parameters&quot;: {&quot;adapter_id&quot;: &quot;adapter-name&quot;}&quot;</code>",ge,Z,Ue,E,Fe="note it’s possible to mix adapter_ids with adapter_id=adapter_path e.g.",be,G,je,I,Ne="In the server logs, you will see the following message:",Te,Q,Re,q,Ce,V,Xe="You can then use these models in generation requests by specifying the <code>lora_model</code> parameter in the request payload. For example:",ve,z,Ae,W,De="If you are using a lora adapter stored locally that was set in the following manner: <code>LORA_ADAPTERS=myadapter=/some/path/to/adapter</code>, here is an example payload:",xe,Y,Le,B,Oe='<p><strong>Note:</strong> The Lora feature is new and still being improved. If you encounter any issues or have any feedback, please let us know by opening an issue on the <a href="https://github.com/huggingface/text-generation-inference/issues/new/choose" rel="nofollow">GitHub repository</a>. Additionally documentation and an improved client library will be published soon.</p>',Je,P,Ke="An updated tutorial with detailed examples will be published soon. Stay tuned!",_e,F,ke,O,Se;return y=new X({props:{title:"LoRA (Low-Rank Adaptation)",local:"lora-low-rank-adaptation",headingTag:"h1"}}),M=new X({props:{title:"What is LoRA?",local:"what-is-lora",headingTag:"h2"}}),g=new X({props:{title:"How is it used?",local:"how-is-it-used",headingTag:"h2"}}),T=new X({props:{title:"Optimizing Inference with LoRA",local:"optimizing-inference-with-lora",headingTag:"h2"}}),C=new X({props:{title:"Serving multiple LoRA adapters with TGI",local:"serving-multiple-lora-adapters-with-tgi",headingTag:"h2"}}),L=new X({props:{title:"Specifying LoRA models",local:"specifying-lora-models",headingTag:"h3"}}),_=new N({props:{code:"TE9SQV9BREFQVEVSUyUzRHByZWRpYmFzZSUyRmN1c3RvbWVyX3N1cHBvcnQlMkNwcmVkaWJhc2UlMkZkYnBlZGlh",highlighted:"LORA_ADAPTERS=predibase/customer_support,predibase/dbpedia",wrap:!1}}),S=new N({props:{code:"TE9SQV9BREFQVEVSUyUzRHByZWRpYmFzZSUyRmN1c3RvbWVyX3N1cHBvcnQlNDBtYWluJTJDcHJlZGliYXNlJTJGZGJwZWRpYSU0MHJldjI=",highlighted:"LORA_ADAPTERS=predibase/customer_support@main,predibase/dbpedia@rev2",wrap:!1}}),Z=new N({props:{code:"TE9SQV9BREFQVEVSUyUzRG15YWRhcHRlciUzRCUyRnNvbWUlMkZwYXRoJTJGdG8lMkZhZGFwdGVyJTJDbXlhZGFwdGVyMiUzRCUyRmFub3RoZXIlMkZwYXRoJTJGdG8lMkZhZGFwdGVy",highlighted:"LORA_ADAPTERS=myadapter=/some/path/to/adapter,myadapter2=/another/path/to/adapter",wrap:!1}}),G=new N({props:{code:"TE9SQV9BREFQVEVSUyUzRHByZWRpYmFzZSUyRmRicGVkaWElMkNteWFkYXB0ZXIlM0QlMkZwYXRoJTJGdG8lMkZkaXIlMkY=",highlighted:"LORA_ADAPTERS=predibase/dbpedia,myadapter=/path/to/dir/",wrap:!1}}),Q=new N({props:{code:"TG9hZGluZyUyMGFkYXB0ZXIlMjB3ZWlnaHRzJTIwaW50byUyMG1vZGVsJTNBJTIwcHJlZGliYXNlJTJGY3VzdG9tZXJfc3VwcG9ydCUwQUxvYWRpbmclMjBhZGFwdGVyJTIwd2VpZ2h0cyUyMGludG8lMjBtb2RlbCUzQSUyMHByZWRpYmFzZSUyRmRicGVkaWE=",highlighted:`Loading adapter weights into model: predibase/customer_support
Loading adapter weights into model: predibase/dbpedia`,wrap:!1}}),q=new X({props:{title:"Generate text",local:"generate-text",headingTag:"h2"}}),z=new N({props:{code:"Y3VybCUyMDEyNy4wLjAuMSUzQTMwMDAlMkZnZW5lcmF0ZSUyMCU1QyUwQSUyMCUyMCUyMCUyMC1YJTIwUE9TVCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1IJTIwJ0NvbnRlbnQtVHlwZSUzQSUyMGFwcGxpY2F0aW9uJTJGanNvbiclMjAlNUMlMEElMjAlMjAlMjAlMjAtZCUyMCclN0IlMEElMjAlMjAlMjJpbnB1dHMlMjIlM0ElMjAlMjJIZWxsbyUyMHdobyUyMGFyZSUyMHlvdSUzRiUyMiUyQyUwQSUyMCUyMCUyMnBhcmFtZXRlcnMlMjIlM0ElMjAlN0IlMEElMjAlMjAlMjAlMjAlMjJtYXhfbmV3X3Rva2VucyUyMiUzQSUyMDQwJTJDJTBBJTIwJTIwJTIwJTIwJTIyYWRhcHRlcl9pZCUyMiUzQSUyMCUyMnByZWRpYmFzZSUyRmN1c3RvbWVyX3N1cHBvcnQlMjIlMEElMjAlMjAlN0QlMEElN0Qn",highlighted:`curl <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-punctuation">:</span><span class="hljs-number">3000</span>/generate \\
    -X POST \\
    -H &#x27;Content-Type<span class="hljs-punctuation">:</span> application/json&#x27; \\
    -d &#x27;<span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;inputs&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Hello who are you?&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;parameters&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;max_new_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">40</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;adapter_id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;predibase/customer_support&quot;</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>&#x27;`,wrap:!1}}),Y=new N({props:{code:"Y3VybCUyMDEyNy4wLjAuMSUzQTMwMDAlMkZnZW5lcmF0ZSUyMCU1QyUwQSUyMCUyMCUyMCUyMC1YJTIwUE9TVCUyMCU1QyUwQSUyMCUyMCUyMCUyMC1IJTIwJ0NvbnRlbnQtVHlwZSUzQSUyMGFwcGxpY2F0aW9uJTJGanNvbiclMjAlNUMlMEElMjAlMjAlMjAlMjAtZCUyMCclN0IlMEElMjAlMjAlMjJpbnB1dHMlMjIlM0ElMjAlMjJIZWxsbyUyMHdobyUyMGFyZSUyMHlvdSUzRiUyMiUyQyUwQSUyMCUyMCUyMnBhcmFtZXRlcnMlMjIlM0ElMjAlN0IlMEElMjAlMjAlMjAlMjAlMjJtYXhfbmV3X3Rva2VucyUyMiUzQSUyMDQwJTJDJTBBJTIwJTIwJTIwJTIwJTIyYWRhcHRlcl9pZCUyMiUzQSUyMCUyMm15YWRhcHRlciUyMiUwQSUyMCUyMCU3RCUwQSU3RCc=",highlighted:`curl <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-punctuation">:</span><span class="hljs-number">3000</span>/generate \\
    -X POST \\
    -H &#x27;Content-Type<span class="hljs-punctuation">:</span> application/json&#x27; \\
    -d &#x27;<span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;inputs&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Hello who are you?&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;parameters&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;max_new_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">40</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;adapter_id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;myadapter&quot;</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>&#x27;`,wrap:!1}}),F=new ut({props:{source:"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/conceptual/lora.md"}}),{c(){f=i("meta"),K=n(),D=i("p"),ee=n(),u(y.$$.fragment),te=n(),u(M.$$.fragment),ae=n(),w=i("p"),w.textContent=Ze,le=n(),$=i("p"),$.textContent=Ee,ne=n(),u(g.$$.fragment),se=n(),U=i("p"),U.textContent=Ge,ie=n(),b=i("p"),b.textContent=Ie,pe=n(),j=i("ul"),j.innerHTML=Qe,oe=n(),u(T.$$.fragment),ue=n(),R=i("p"),R.innerHTML=qe,re=n(),u(C.$$.fragment),me=n(),v=i("p"),v.textContent=Ve,de=n(),A=i("p"),A.textContent=ze,ce=n(),x=i("p"),x.innerHTML=We,he=n(),u(L.$$.fragment),fe=n(),J=i("p"),J.innerHTML=Ye,ye=n(),u(_.$$.fragment),Me=n(),k=i("p"),k.innerHTML=Be,we=n(),u(S.$$.fragment),$e=n(),H=i("p"),H.innerHTML=Pe,ge=n(),u(Z.$$.fragment),Ue=n(),E=i("p"),E.textContent=Fe,be=n(),u(G.$$.fragment),je=n(),I=i("p"),I.textContent=Ne,Te=n(),u(Q.$$.fragment),Re=n(),u(q.$$.fragment),Ce=n(),V=i("p"),V.innerHTML=Xe,ve=n(),u(z.$$.fragment),Ae=n(),W=i("p"),W.innerHTML=De,xe=n(),u(Y.$$.fragment),Le=n(),B=i("blockquote"),B.innerHTML=Oe,Je=n(),P=i("p"),P.textContent=Ke,_e=n(),u(F.$$.fragment),ke=n(),O=i("p"),this.h()},l(e){const t=pt("svelte-u9bgzb",document.head);f=p(t,"META",{name:!0,content:!0}),t.forEach(a),K=s(e),D=p(e,"P",{}),et(D).forEach(a),ee=s(e),r(y.$$.fragment,e),te=s(e),r(M.$$.fragment,e),ae=s(e),w=p(e,"P",{"data-svelte-h":!0}),o(w)!=="svelte-t78gld"&&(w.textContent=Ze),le=s(e),$=p(e,"P",{"data-svelte-h":!0}),o($)!=="svelte-1ojg20d"&&($.textContent=Ee),ne=s(e),r(g.$$.fragment,e),se=s(e),U=p(e,"P",{"data-svelte-h":!0}),o(U)!=="svelte-1kjva3q"&&(U.textContent=Ge),ie=s(e),b=p(e,"P",{"data-svelte-h":!0}),o(b)!=="svelte-4ny44z"&&(b.textContent=Ie),pe=s(e),j=p(e,"UL",{"data-svelte-h":!0}),o(j)!=="svelte-1qlylqm"&&(j.innerHTML=Qe),oe=s(e),r(T.$$.fragment,e),ue=s(e),R=p(e,"P",{"data-svelte-h":!0}),o(R)!=="svelte-248ncg"&&(R.innerHTML=qe),re=s(e),r(C.$$.fragment,e),me=s(e),v=p(e,"P",{"data-svelte-h":!0}),o(v)!=="svelte-1fi42d1"&&(v.textContent=Ve),de=s(e),A=p(e,"P",{"data-svelte-h":!0}),o(A)!=="svelte-u78dgu"&&(A.textContent=ze),ce=s(e),x=p(e,"P",{"data-svelte-h":!0}),o(x)!=="svelte-rk8g1x"&&(x.innerHTML=We),he=s(e),r(L.$$.fragment,e),fe=s(e),J=p(e,"P",{"data-svelte-h":!0}),o(J)!=="svelte-dj7isz"&&(J.innerHTML=Ye),ye=s(e),r(_.$$.fragment,e),Me=s(e),k=p(e,"P",{"data-svelte-h":!0}),o(k)!=="svelte-1nm3h5k"&&(k.innerHTML=Be),we=s(e),r(S.$$.fragment,e),$e=s(e),H=p(e,"P",{"data-svelte-h":!0}),o(H)!=="svelte-kcocpu"&&(H.innerHTML=Pe),ge=s(e),r(Z.$$.fragment,e),Ue=s(e),E=p(e,"P",{"data-svelte-h":!0}),o(E)!=="svelte-1aqrfma"&&(E.textContent=Fe),be=s(e),r(G.$$.fragment,e),je=s(e),I=p(e,"P",{"data-svelte-h":!0}),o(I)!=="svelte-1bmvbjz"&&(I.textContent=Ne),Te=s(e),r(Q.$$.fragment,e),Re=s(e),r(q.$$.fragment,e),Ce=s(e),V=p(e,"P",{"data-svelte-h":!0}),o(V)!=="svelte-12kjl2c"&&(V.innerHTML=Xe),ve=s(e),r(z.$$.fragment,e),Ae=s(e),W=p(e,"P",{"data-svelte-h":!0}),o(W)!=="svelte-7o07ce"&&(W.innerHTML=De),xe=s(e),r(Y.$$.fragment,e),Le=s(e),B=p(e,"BLOCKQUOTE",{"data-svelte-h":!0}),o(B)!=="svelte-zh5fx6"&&(B.innerHTML=Oe),Je=s(e),P=p(e,"P",{"data-svelte-h":!0}),o(P)!=="svelte-19a3mbh"&&(P.textContent=Ke),_e=s(e),r(F.$$.fragment,e),ke=s(e),O=p(e,"P",{}),et(O).forEach(a),this.h()},h(){tt(f,"name","hf:doc:metadata"),tt(f,"content",mt)},m(e,t){ot(document.head,f),l(e,K,t),l(e,D,t),l(e,ee,t),m(y,e,t),l(e,te,t),m(M,e,t),l(e,ae,t),l(e,w,t),l(e,le,t),l(e,$,t),l(e,ne,t),m(g,e,t),l(e,se,t),l(e,U,t),l(e,ie,t),l(e,b,t),l(e,pe,t),l(e,j,t),l(e,oe,t),m(T,e,t),l(e,ue,t),l(e,R,t),l(e,re,t),m(C,e,t),l(e,me,t),l(e,v,t),l(e,de,t),l(e,A,t),l(e,ce,t),l(e,x,t),l(e,he,t),m(L,e,t),l(e,fe,t),l(e,J,t),l(e,ye,t),m(_,e,t),l(e,Me,t),l(e,k,t),l(e,we,t),m(S,e,t),l(e,$e,t),l(e,H,t),l(e,ge,t),m(Z,e,t),l(e,Ue,t),l(e,E,t),l(e,be,t),m(G,e,t),l(e,je,t),l(e,I,t),l(e,Te,t),m(Q,e,t),l(e,Re,t),m(q,e,t),l(e,Ce,t),l(e,V,t),l(e,ve,t),m(z,e,t),l(e,Ae,t),l(e,W,t),l(e,xe,t),m(Y,e,t),l(e,Le,t),l(e,B,t),l(e,Je,t),l(e,P,t),l(e,_e,t),m(F,e,t),l(e,ke,t),l(e,O,t),Se=!0},p:lt,i(e){Se||(d(y.$$.fragment,e),d(M.$$.fragment,e),d(g.$$.fragment,e),d(T.$$.fragment,e),d(C.$$.fragment,e),d(L.$$.fragment,e),d(_.$$.fragment,e),d(S.$$.fragment,e),d(Z.$$.fragment,e),d(G.$$.fragment,e),d(Q.$$.fragment,e),d(q.$$.fragment,e),d(z.$$.fragment,e),d(Y.$$.fragment,e),d(F.$$.fragment,e),Se=!0)},o(e){c(y.$$.fragment,e),c(M.$$.fragment,e),c(g.$$.fragment,e),c(T.$$.fragment,e),c(C.$$.fragment,e),c(L.$$.fragment,e),c(_.$$.fragment,e),c(S.$$.fragment,e),c(Z.$$.fragment,e),c(G.$$.fragment,e),c(Q.$$.fragment,e),c(q.$$.fragment,e),c(z.$$.fragment,e),c(Y.$$.fragment,e),c(F.$$.fragment,e),Se=!1},d(e){e&&(a(K),a(D),a(ee),a(te),a(ae),a(w),a(le),a($),a(ne),a(se),a(U),a(ie),a(b),a(pe),a(j),a(oe),a(ue),a(R),a(re),a(me),a(v),a(de),a(A),a(ce),a(x),a(he),a(fe),a(J),a(ye),a(Me),a(k),a(we),a($e),a(H),a(ge),a(Ue),a(E),a(be),a(je),a(I),a(Te),a(Re),a(Ce),a(V),a(ve),a(Ae),a(W),a(xe),a(Le),a(B),a(Je),a(P),a(_e),a(ke),a(O)),a(f),h(y,e),h(M,e),h(g,e),h(T,e),h(C,e),h(L,e),h(_,e),h(S,e),h(Z,e),h(G,e),h(Q,e),h(q,e),h(z,e),h(Y,e),h(F,e)}}}const mt='{"title":"LoRA (Low-Rank Adaptation)","local":"lora-low-rank-adaptation","sections":[{"title":"What is LoRA?","local":"what-is-lora","sections":[],"depth":2},{"title":"How is it used?","local":"how-is-it-used","sections":[],"depth":2},{"title":"Optimizing Inference with LoRA","local":"optimizing-inference-with-lora","sections":[],"depth":2},{"title":"Serving multiple LoRA adapters with TGI","local":"serving-multiple-lora-adapters-with-tgi","sections":[{"title":"Specifying LoRA models","local":"specifying-lora-models","sections":[],"depth":3}],"depth":2},{"title":"Generate text","local":"generate-text","sections":[],"depth":2}],"depth":1}';function dt(He){return nt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mt extends st{constructor(f){super(),it(this,f,dt,rt,at,{})}}export{Mt as component};
