import{s as F,n as L,o as A}from"../chunks/scheduler.8c3d61f6.js";import{S as X,i as q,g as c,s as i,r as y,A as K,h as b,f as a,c as n,j as N,u as J,x as C,k as P,y as O,a as s,v as T,d as _,t as Z,w as $}from"../chunks/index.da70eac4.js";import{C as Y}from"../chunks/CodeBlock.a9c4becf.js";import{D as ee}from"../chunks/DocNotebookDropdown.48852948.js";import{H as te,E as ae}from"../chunks/index.5d4ab994.js";function se(R){let l,U,w,j,o,k,p,v,m,S='The <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/depth2img#diffusers.StableDiffusionDepth2ImgPipeline">StableDiffusionDepth2ImgPipeline</a> lets you pass a text prompt and an initial image to condition the generation of new images. In addition, you can also pass a <code>depth_map</code> to preserve the image structure. If no <code>depth_map</code> is provided, the pipeline automatically predicts the depth via an integrated <a href="https://github.com/isl-org/MiDaS" rel="nofollow">depth-estimation model</a>.',W,r,B='Start by creating an instance of the <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/depth2img#diffusers.StableDiffusionDepth2ImgPipeline">StableDiffusionDepth2ImgPipeline</a>:',I,d,G,g,z="Now pass your prompt to the pipeline. You can also pass a <code>negative_prompt</code> to prevent certain words from guiding how an image is generated:",Q,u,D,h,E='<thead><tr><th>Input</th> <th>Output</th></tr></thead> <tbody><tr><td><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/coco-cats.png" width="500"/></td> <td><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/depth2img-tigers.png" width="500"/></td></tr></tbody>',H,f,V,M,x;return o=new te({props:{title:"Text-guided depth-to-image generation",local:"text-guided-depth-to-image-generation",headingTag:"h1"}}),p=new ee({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/depth2img.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/depth2img.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/tensorflow/depth2img.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/diffusers_doc/en/depth2img.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/depth2img.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/diffusers_doc/en/tensorflow/depth2img.ipynb"}]}}),d=new Y({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGlmZnVzZXJzJTIwaW1wb3J0JTIwU3RhYmxlRGlmZnVzaW9uRGVwdGgySW1nUGlwZWxpbmUlMEFmcm9tJTIwZGlmZnVzZXJzLnV0aWxzJTIwaW1wb3J0JTIwbG9hZF9pbWFnZSUyQyUyMG1ha2VfaW1hZ2VfZ3JpZCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwU3RhYmxlRGlmZnVzaW9uRGVwdGgySW1nUGlwZWxpbmUuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi0yLWRlcHRoJTIyJTJDJTBBJTIwJTIwJTIwJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTBBJTIwJTIwJTIwJTIwdXNlX3NhZmV0ZW5zb3JzJTNEVHJ1ZSUyQyUwQSkudG8oJTIyY3VkYSUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionDepth2ImgPipeline
<span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image, make_image_grid

pipeline = StableDiffusionDepth2ImgPipeline.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-2-depth&quot;</span>,
    torch_dtype=torch.float16,
    use_safetensors=<span class="hljs-literal">True</span>,
).to(<span class="hljs-string">&quot;cuda&quot;</span>)`,wrap:!1}}),u=new Y({props:{code:"dXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWluaXRfaW1hZ2UlMjAlM0QlMjBsb2FkX2ltYWdlKHVybCklMEFwcm9tcHQlMjAlM0QlMjAlMjJ0d28lMjB0aWdlcnMlMjIlMEFuZWdhdGl2ZV9wcm9tcHQlMjAlM0QlMjAlMjJiYWQlMkMlMjBkZWZvcm1lZCUyQyUyMHVnbHklMkMlMjBiYWQlMjBhbmF0b215JTIyJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZShwcm9tcHQlM0Rwcm9tcHQlMkMlMjBpbWFnZSUzRGluaXRfaW1hZ2UlMkMlMjBuZWdhdGl2ZV9wcm9tcHQlM0RuZWdhdGl2ZV9wcm9tcHQlMkMlMjBzdHJlbmd0aCUzRDAuNykuaW1hZ2VzJTVCMCU1RCUwQW1ha2VfaW1hZ2VfZ3JpZCglNUJpbml0X2ltYWdlJTJDJTIwaW1hZ2UlNUQlMkMlMjByb3dzJTNEMSUyQyUyMGNvbHMlM0QyKQ==",highlighted:`url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
init_image = load_image(url)
prompt = <span class="hljs-string">&quot;two tigers&quot;</span>
negative_prompt = <span class="hljs-string">&quot;bad, deformed, ugly, bad anatomy&quot;</span>
image = pipeline(prompt=prompt, image=init_image, negative_prompt=negative_prompt, strength=<span class="hljs-number">0.7</span>).images[<span class="hljs-number">0</span>]
make_image_grid([init_image, image], rows=<span class="hljs-number">1</span>, cols=<span class="hljs-number">2</span>)`,wrap:!1}}),f=new ae({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/depth2img.md"}}),{c(){l=c("meta"),U=i(),w=c("p"),j=i(),y(o.$$.fragment),k=i(),y(p.$$.fragment),v=i(),m=c("p"),m.innerHTML=S,W=i(),r=c("p"),r.innerHTML=B,I=i(),y(d.$$.fragment),G=i(),g=c("p"),g.innerHTML=z,Q=i(),y(u.$$.fragment),D=i(),h=c("table"),h.innerHTML=E,H=i(),y(f.$$.fragment),V=i(),M=c("p"),this.h()},l(e){const t=K("svelte-u9bgzb",document.head);l=b(t,"META",{name:!0,content:!0}),t.forEach(a),U=n(e),w=b(e,"P",{}),N(w).forEach(a),j=n(e),J(o.$$.fragment,e),k=n(e),J(p.$$.fragment,e),v=n(e),m=b(e,"P",{"data-svelte-h":!0}),C(m)!=="svelte-g4ruca"&&(m.innerHTML=S),W=n(e),r=b(e,"P",{"data-svelte-h":!0}),C(r)!=="svelte-1fv4f3f"&&(r.innerHTML=B),I=n(e),J(d.$$.fragment,e),G=n(e),g=b(e,"P",{"data-svelte-h":!0}),C(g)!=="svelte-deexa5"&&(g.innerHTML=z),Q=n(e),J(u.$$.fragment,e),D=n(e),h=b(e,"TABLE",{"data-svelte-h":!0}),C(h)!=="svelte-175x2as"&&(h.innerHTML=E),H=n(e),J(f.$$.fragment,e),V=n(e),M=b(e,"P",{}),N(M).forEach(a),this.h()},h(){P(l,"name","hf:doc:metadata"),P(l,"content",ie)},m(e,t){O(document.head,l),s(e,U,t),s(e,w,t),s(e,j,t),T(o,e,t),s(e,k,t),T(p,e,t),s(e,v,t),s(e,m,t),s(e,W,t),s(e,r,t),s(e,I,t),T(d,e,t),s(e,G,t),s(e,g,t),s(e,Q,t),T(u,e,t),s(e,D,t),s(e,h,t),s(e,H,t),T(f,e,t),s(e,V,t),s(e,M,t),x=!0},p:L,i(e){x||(_(o.$$.fragment,e),_(p.$$.fragment,e),_(d.$$.fragment,e),_(u.$$.fragment,e),_(f.$$.fragment,e),x=!0)},o(e){Z(o.$$.fragment,e),Z(p.$$.fragment,e),Z(d.$$.fragment,e),Z(u.$$.fragment,e),Z(f.$$.fragment,e),x=!1},d(e){e&&(a(U),a(w),a(j),a(k),a(v),a(m),a(W),a(r),a(I),a(G),a(g),a(Q),a(D),a(h),a(H),a(V),a(M)),a(l),$(o,e),$(p,e),$(d,e),$(u,e),$(f,e)}}}const ie='{"title":"Text-guided depth-to-image generation","local":"text-guided-depth-to-image-generation","sections":[],"depth":1}';function ne(R){return A(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class de extends X{constructor(l){super(),q(this,l,ne,se,F,{})}}export{de as component};
