import{s as ne,o as pe,n as K}from"../chunks/scheduler.8c3d61f6.js";import{S as oe,i as re,g as y,s as f,r as J,A as me,h as M,f as s,c,j as ae,u as G,x as U,k as C,y as ue,a as n,v as w,d as T,t as W,w as v}from"../chunks/index.da70eac4.js";import{C as S}from"../chunks/CodeBlock.a9c4becf.js";import{H as se,E as fe}from"../chunks/index.5d4ab994.js";import{H as ce,a as D}from"../chunks/HfOption.6ab18950.js";function de(k){let i,m,a,d='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png"/>',r;return i=new S({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVfdHh0MmltZyUyMCUzRCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmRyZWFtbGlrZS1hcnQlMkZkcmVhbWxpa2UtcGhvdG9yZWFsLTIuMCUyMiUyQyUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUyMHVzZV9zYWZldGVuc29ycyUzRFRydWUlMEEpLnRvKCUyMmN1ZGElMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyY2luZW1hdGljJTIwcGhvdG8lMjBvZiUyMEdvZHppbGxhJTIwZWF0aW5nJTIwc3VzaGklMjB3aXRoJTIwYSUyMGNhdCUyMGluJTIwYSUyMGl6YWtheWElMkMlMjAzNW1tJTIwcGhvdG9ncmFwaCUyQyUyMGZpbG0lMkMlMjBwcm9mZXNzaW9uYWwlMkMlMjA0ayUyQyUyMGhpZ2hseSUyMGRldGFpbGVkJTIyJTBBZ2VuZXJhdG9yJTIwJTNEJTIwdG9yY2guR2VuZXJhdG9yKGRldmljZSUzRCUyMmNwdSUyMikubWFudWFsX3NlZWQoMzcpJTBBaW1hZ2UlMjAlM0QlMjBwaXBlX3R4dDJpbWcocHJvbXB0JTJDJTIwZ2VuZXJhdG9yJTNEZ2VuZXJhdG9yKS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2U=",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipe_txt2img = AutoPipelineForText2Image.from_pretrained(
    <span class="hljs-string">&quot;dreamlike-art/dreamlike-photoreal-2.0&quot;</span>, torch_dtype=torch.float16, use_safetensors=<span class="hljs-literal">True</span>
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

prompt = <span class="hljs-string">&quot;cinematic photo of Godzilla eating sushi with a cat in a izakaya, 35mm photograph, film, professional, 4k, highly detailed&quot;</span>
generator = torch.Generator(device=<span class="hljs-string">&quot;cpu&quot;</span>).manual_seed(<span class="hljs-number">37</span>)
image = pipe_txt2img(prompt, generator=generator).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),{c(){J(i.$$.fragment),m=f(),a=y("div"),a.innerHTML=d,this.h()},l(l){G(i.$$.fragment,l),m=c(l),a=M(l,"DIV",{class:!0,"data-svelte-h":!0}),U(a)!=="svelte-1f1ik8h"&&(a.innerHTML=d),this.h()},h(){C(a,"class","flex justify-center")},m(l,t){w(i,l,t),n(l,m,t),n(l,a,t),r=!0},p:K,i(l){r||(T(i.$$.fragment,l),r=!0)},o(l){W(i.$$.fragment,l),r=!1},d(l){l&&(s(m),s(a)),v(i,l)}}}function he(k){let i,m,a,d='Notice how the <a href="https://hf.co/dreamlike-art/dreamlike-photoreal-2.0" rel="nofollow">dreamlike-art/dreamlike-photoreal-2.0</a> checkpoint is used for both text-to-image and image-to-image tasks? To save memory and avoid loading the checkpoint twice, use the <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pipe">from_pipe()</a> method.',r,l,t,u,j='You can learn more about the <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pipe">from_pipe()</a> method in the <a href="../using-diffusers/loading#reuse-a-pipeline">Reuse a pipeline</a> guide.',h,g,B='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png"/>',b;return i=new S({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvckltYWdlMkltYWdlJTBBZnJvbSUyMGRpZmZ1c2Vycy51dGlscyUyMGltcG9ydCUyMGxvYWRfaW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVfaW1nMmltZyUyMCUzRCUyMEF1dG9QaXBlbGluZUZvckltYWdlMkltYWdlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJkcmVhbWxpa2UtYXJ0JTJGZHJlYW1saWtlLXBob3RvcmVhbC0yLjAlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMjB1c2Vfc2FmZXRlbnNvcnMlM0RUcnVlJTBBKS50byglMjJjdWRhJTIyKSUwQSUwQWluaXRfaW1hZ2UlMjAlM0QlMjBsb2FkX2ltYWdlKCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRmRpZmZ1c2VycyUyRmF1dG9waXBlbGluZS10ZXh0MmltZy5wbmclMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyY2luZW1hdGljJTIwcGhvdG8lMjBvZiUyMEdvZHppbGxhJTIwZWF0aW5nJTIwYnVyZ2VycyUyMHdpdGglMjBhJTIwY2F0JTIwaW4lMjBhJTIwZmFzdCUyMGZvb2QlMjByZXN0YXVyYW50JTJDJTIwMzVtbSUyMHBob3RvZ3JhcGglMkMlMjBmaWxtJTJDJTIwcHJvZmVzc2lvbmFsJTJDJTIwNGslMkMlMjBoaWdobHklMjBkZXRhaWxlZCUyMiUwQWdlbmVyYXRvciUyMCUzRCUyMHRvcmNoLkdlbmVyYXRvcihkZXZpY2UlM0QlMjJjcHUlMjIpLm1hbnVhbF9zZWVkKDUzKSUwQWltYWdlJTIwJTNEJTIwcGlwZV9pbWcyaW1nKHByb21wdCUyQyUyMGltYWdlJTNEaW5pdF9pbWFnZSUyQyUyMGdlbmVyYXRvciUzRGdlbmVyYXRvcikuaW1hZ2VzJTVCMCU1RCUwQWltYWdl",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForImage2Image
<span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image
<span class="hljs-keyword">import</span> torch

pipe_img2img = AutoPipelineForImage2Image.from_pretrained(
    <span class="hljs-string">&quot;dreamlike-art/dreamlike-photoreal-2.0&quot;</span>, torch_dtype=torch.float16, use_safetensors=<span class="hljs-literal">True</span>
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

init_image = load_image(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png&quot;</span>)

prompt = <span class="hljs-string">&quot;cinematic photo of Godzilla eating burgers with a cat in a fast food restaurant, 35mm photograph, film, professional, 4k, highly detailed&quot;</span>
generator = torch.Generator(device=<span class="hljs-string">&quot;cpu&quot;</span>).manual_seed(<span class="hljs-number">53</span>)
image = pipe_img2img(prompt, image=init_image, generator=generator).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),l=new S({props:{code:"cGlwZV9pbWcyaW1nJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9ySW1hZ2UySW1hZ2UuZnJvbV9waXBlKHBpcGVfdHh0MmltZykudG8oJTIyY3VkYSUyMiklMEFpbWFnZSUyMCUzRCUyMHBpcGVsaW5lKHByb21wdCUyQyUyMGltYWdlJTNEaW5pdF9pbWFnZSUyQyUyMGdlbmVyYXRvciUzRGdlbmVyYXRvcikuaW1hZ2VzJTVCMCU1RCUwQWltYWdl",highlighted:`pipe_img2img = AutoPipelineForImage2Image.from_pipe(pipe_txt2img).to(<span class="hljs-string">&quot;cuda&quot;</span>)
image = pipeline(prompt, image=init_image, generator=generator).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),{c(){J(i.$$.fragment),m=f(),a=y("p"),a.innerHTML=d,r=f(),J(l.$$.fragment),t=f(),u=y("p"),u.innerHTML=j,h=f(),g=y("div"),g.innerHTML=B,this.h()},l(o){G(i.$$.fragment,o),m=c(o),a=M(o,"P",{"data-svelte-h":!0}),U(a)!=="svelte-1cd01nu"&&(a.innerHTML=d),r=c(o),G(l.$$.fragment,o),t=c(o),u=M(o,"P",{"data-svelte-h":!0}),U(u)!=="svelte-ll6pwd"&&(u.innerHTML=j),h=c(o),g=M(o,"DIV",{class:!0,"data-svelte-h":!0}),U(g)!=="svelte-h9ph7h"&&(g.innerHTML=B),this.h()},h(){C(g,"class","flex justify-center")},m(o,Z){w(i,o,Z),n(o,m,Z),n(o,a,Z),n(o,r,Z),w(l,o,Z),n(o,t,Z),n(o,u,Z),n(o,h,Z),n(o,g,Z),b=!0},p:K,i(o){b||(T(i.$$.fragment,o),T(l.$$.fragment,o),b=!0)},o(o){W(i.$$.fragment,o),W(l.$$.fragment,o),b=!1},d(o){o&&(s(m),s(a),s(r),s(t),s(u),s(h),s(g)),v(i,o),v(l,o)}}}function ye(k){let i,m,a,d='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png"/>',r;return i=new S({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvcklucGFpbnRpbmclMEFmcm9tJTIwZGlmZnVzZXJzLnV0aWxzJTIwaW1wb3J0JTIwbG9hZF9pbWFnZSUwQWltcG9ydCUyMHRvcmNoJTBBJTBBcGlwZWxpbmUlMjAlM0QlMjBBdXRvUGlwZWxpbmVGb3JJbnBhaW50aW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtYmFzZS0xLjAlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMjB1c2Vfc2FmZXRlbnNvcnMlM0RUcnVlJTBBKS50byglMjJjdWRhJTIyKSUwQSUwQWluaXRfaW1hZ2UlMjAlM0QlMjBsb2FkX2ltYWdlKCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRmRpZmZ1c2VycyUyRmF1dG9waXBlbGluZS1pbWcyaW1nLnBuZyUyMiklMEFtYXNrX2ltYWdlJTIwJTNEJTIwbG9hZF9pbWFnZSglMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZkaWZmdXNlcnMlMkZhdXRvcGlwZWxpbmUtbWFzay5wbmclMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyY2luZW1hdGljJTIwcGhvdG8lMjBvZiUyMGElMjBvd2wlMkMlMjAzNW1tJTIwcGhvdG9ncmFwaCUyQyUyMGZpbG0lMkMlMjBwcm9mZXNzaW9uYWwlMkMlMjA0ayUyQyUyMGhpZ2hseSUyMGRldGFpbGVkJTIyJTBBZ2VuZXJhdG9yJTIwJTNEJTIwdG9yY2guR2VuZXJhdG9yKGRldmljZSUzRCUyMmNwdSUyMikubWFudWFsX3NlZWQoMzgpJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZShwcm9tcHQlMkMlMjBpbWFnZSUzRGluaXRfaW1hZ2UlMkMlMjBtYXNrX2ltYWdlJTNEbWFza19pbWFnZSUyQyUyMGdlbmVyYXRvciUzRGdlbmVyYXRvciUyQyUyMHN0cmVuZ3RoJTNEMC40KS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2U=",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForInpainting
<span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForInpainting.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.float16, use_safetensors=<span class="hljs-literal">True</span>
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

init_image = load_image(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png&quot;</span>)
mask_image = load_image(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-mask.png&quot;</span>)

prompt = <span class="hljs-string">&quot;cinematic photo of a owl, 35mm photograph, film, professional, 4k, highly detailed&quot;</span>
generator = torch.Generator(device=<span class="hljs-string">&quot;cpu&quot;</span>).manual_seed(<span class="hljs-number">38</span>)
image = pipeline(prompt, image=init_image, mask_image=mask_image, generator=generator, strength=<span class="hljs-number">0.4</span>).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),{c(){J(i.$$.fragment),m=f(),a=y("div"),a.innerHTML=d,this.h()},l(l){G(i.$$.fragment,l),m=c(l),a=M(l,"DIV",{class:!0,"data-svelte-h":!0}),U(a)!=="svelte-18lc082"&&(a.innerHTML=d),this.h()},h(){C(a,"class","flex justify-center")},m(l,t){w(i,l,t),n(l,m,t),n(l,a,t),r=!0},p:K,i(l){r||(T(i.$$.fragment,l),r=!0)},o(l){W(i.$$.fragment,l),r=!1},d(l){l&&(s(m),s(a)),v(i,l)}}}function Me(k){let i,m,a,d,r,l;return i=new D({props:{id:"autopipeline",option:"text-to-image",$$slots:{default:[de]},$$scope:{ctx:k}}}),a=new D({props:{id:"autopipeline",option:"image-to-image",$$slots:{default:[he]},$$scope:{ctx:k}}}),r=new D({props:{id:"autopipeline",option:"inpainting",$$slots:{default:[ye]},$$scope:{ctx:k}}}),{c(){J(i.$$.fragment),m=f(),J(a.$$.fragment),d=f(),J(r.$$.fragment)},l(t){G(i.$$.fragment,t),m=c(t),G(a.$$.fragment,t),d=c(t),G(r.$$.fragment,t)},m(t,u){w(i,t,u),n(t,m,u),w(a,t,u),n(t,d,u),w(r,t,u),l=!0},p(t,u){const j={};u&2&&(j.$$scope={dirty:u,ctx:t}),i.$set(j);const h={};u&2&&(h.$$scope={dirty:u,ctx:t}),a.$set(h);const g={};u&2&&(g.$$scope={dirty:u,ctx:t}),r.$set(g)},i(t){l||(T(i.$$.fragment,t),T(a.$$.fragment,t),T(r.$$.fragment,t),l=!0)},o(t){W(i.$$.fragment,t),W(a.$$.fragment,t),W(r.$$.fragment,t),l=!1},d(t){t&&(s(m),s(d)),v(i,t),v(a,t),v(r,t)}}}function ge(k){let i,m,a,d,r,l,t,u="Diffusers provides many pipelines for basic tasks like generating images, videos, audio, and inpainting. On top of these, there are specialized pipelines for adapters and features like upscaling, super-resolution, and more. Different pipeline classes can even use the same checkpoint because they share the same pretrained model! With so many different pipelines, it can be overwhelming to know which pipeline class to use.",j,h,g='The <a href="../api/pipelines/auto_pipeline">AutoPipeline</a> class is designed to simplify the variety of pipelines in Diffusers. It is a generic <em>task-first</em> pipeline that lets you focus on a task (<a href="/docs/diffusers/v0.33.1/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image">AutoPipelineForText2Image</a>, <a href="/docs/diffusers/v0.33.1/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image">AutoPipelineForImage2Image</a>, and <a href="/docs/diffusers/v0.33.1/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting">AutoPipelineForInpainting</a>) without needing to know the specific pipeline class. The <a href="../api/pipelines/auto_pipeline">AutoPipeline</a> automatically detects the correct pipeline class to use.',B,b,o='For example, let’s use the <a href="https://hf.co/dreamlike-art/dreamlike-photoreal-2.0" rel="nofollow">dreamlike-art/dreamlike-photoreal-2.0</a> checkpoint.',Z,_,O='Under the hood, <a href="../api/pipelines/auto_pipeline">AutoPipeline</a>:',x,I,ee='<li>Detects a <code>&quot;stable-diffusion&quot;</code> class from the <a href="https://hf.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/model_index.json" rel="nofollow">model_index.json</a> file.</li> <li>Depending on the task you’re interested in, it loads the <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline">StableDiffusionPipeline</a>, <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline">StableDiffusionImg2ImgPipeline</a>, or <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline">StableDiffusionInpaintPipeline</a>. Any parameter (<code>strength</code>, <code>num_inference_steps</code>, etc.) you would pass to these specific pipelines can also be passed to the <a href="../api/pipelines/auto_pipeline">AutoPipeline</a>.</li>',E,$,N,R,Q,F,te='The <a href="../api/pipelines/auto_pipeline">AutoPipeline</a> supports <a href="../api/pipelines/stable_diffusion/overview">Stable Diffusion</a>, <a href="../api/pipelines/stable_diffusion/stable_diffusion_xl">Stable Diffusion XL</a>, <a href="../api/pipelines/controlnet">ControlNet</a>, <a href="../api/pipelines/kandinsky.md">Kandinsky 2.1</a>, <a href="../api/pipelines/kandinsky_v22">Kandinsky 2.2</a>, and <a href="../api/pipelines/deepfloyd_if">DeepFloyd IF</a> checkpoints.',z,X,le="If you try to load an unsupported checkpoint, you’ll get an error.",A,V,P,H,q,Y,L;return r=new se({props:{title:"AutoPipeline",local:"autopipeline",headingTag:"h1"}}),$=new ce({props:{id:"autopipeline",options:["text-to-image","image-to-image","inpainting"],$$slots:{default:[Me]},$$scope:{ctx:k}}}),R=new se({props:{title:"Unsupported checkpoints",local:"unsupported-checkpoints",headingTag:"h2"}}),V=new S({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvckltYWdlMkltYWdlJTBBaW1wb3J0JTIwdG9yY2glMEElMEFwaXBlbGluZSUyMCUzRCUyMEF1dG9QaXBlbGluZUZvckltYWdlMkltYWdlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZzaGFwLWUtaW1nMmltZyUyMiUyQyUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUyMHVzZV9zYWZldGVuc29ycyUzRFRydWUlMEEpJTBBJTIyVmFsdWVFcnJvciUzQSUyMEF1dG9QaXBlbGluZSUyMGNhbid0JTIwZmluZCUyMGElMjBwaXBlbGluZSUyMGxpbmtlZCUyMHRvJTIwU2hhcEVJbWcySW1nUGlwZWxpbmUlMjBmb3IlMjBOb25lJTIy",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForImage2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForImage2Image.from_pretrained(
    <span class="hljs-string">&quot;openai/shap-e-img2img&quot;</span>, torch_dtype=torch.float16, use_safetensors=<span class="hljs-literal">True</span>
)
<span class="hljs-string">&quot;ValueError: AutoPipeline can&#x27;t find a pipeline linked to ShapEImg2ImgPipeline for None&quot;</span>`,wrap:!1}}),H=new fe({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md"}}),{c(){i=y("meta"),m=f(),a=y("p"),d=f(),J(r.$$.fragment),l=f(),t=y("p"),t.textContent=u,j=f(),h=y("p"),h.innerHTML=g,B=f(),b=y("p"),b.innerHTML=o,Z=f(),_=y("p"),_.innerHTML=O,x=f(),I=y("ol"),I.innerHTML=ee,E=f(),J($.$$.fragment),N=f(),J(R.$$.fragment),Q=f(),F=y("p"),F.innerHTML=te,z=f(),X=y("p"),X.textContent=le,A=f(),J(V.$$.fragment),P=f(),J(H.$$.fragment),q=f(),Y=y("p"),this.h()},l(e){const p=me("svelte-u9bgzb",document.head);i=M(p,"META",{name:!0,content:!0}),p.forEach(s),m=c(e),a=M(e,"P",{}),ae(a).forEach(s),d=c(e),G(r.$$.fragment,e),l=c(e),t=M(e,"P",{"data-svelte-h":!0}),U(t)!=="svelte-1v85y3v"&&(t.textContent=u),j=c(e),h=M(e,"P",{"data-svelte-h":!0}),U(h)!=="svelte-1ky0abl"&&(h.innerHTML=g),B=c(e),b=M(e,"P",{"data-svelte-h":!0}),U(b)!=="svelte-1toes9"&&(b.innerHTML=o),Z=c(e),_=M(e,"P",{"data-svelte-h":!0}),U(_)!=="svelte-8rbew7"&&(_.innerHTML=O),x=c(e),I=M(e,"OL",{"data-svelte-h":!0}),U(I)!=="svelte-tevh0a"&&(I.innerHTML=ee),E=c(e),G($.$$.fragment,e),N=c(e),G(R.$$.fragment,e),Q=c(e),F=M(e,"P",{"data-svelte-h":!0}),U(F)!=="svelte-opo356"&&(F.innerHTML=te),z=c(e),X=M(e,"P",{"data-svelte-h":!0}),U(X)!=="svelte-1czlhdx"&&(X.textContent=le),A=c(e),G(V.$$.fragment,e),P=c(e),G(H.$$.fragment,e),q=c(e),Y=M(e,"P",{}),ae(Y).forEach(s),this.h()},h(){C(i,"name","hf:doc:metadata"),C(i,"content",Ze)},m(e,p){ue(document.head,i),n(e,m,p),n(e,a,p),n(e,d,p),w(r,e,p),n(e,l,p),n(e,t,p),n(e,j,p),n(e,h,p),n(e,B,p),n(e,b,p),n(e,Z,p),n(e,_,p),n(e,x,p),n(e,I,p),n(e,E,p),w($,e,p),n(e,N,p),w(R,e,p),n(e,Q,p),n(e,F,p),n(e,z,p),n(e,X,p),n(e,A,p),w(V,e,p),n(e,P,p),w(H,e,p),n(e,q,p),n(e,Y,p),L=!0},p(e,[p]){const ie={};p&2&&(ie.$$scope={dirty:p,ctx:e}),$.$set(ie)},i(e){L||(T(r.$$.fragment,e),T($.$$.fragment,e),T(R.$$.fragment,e),T(V.$$.fragment,e),T(H.$$.fragment,e),L=!0)},o(e){W(r.$$.fragment,e),W($.$$.fragment,e),W(R.$$.fragment,e),W(V.$$.fragment,e),W(H.$$.fragment,e),L=!1},d(e){e&&(s(m),s(a),s(d),s(l),s(t),s(j),s(h),s(B),s(b),s(Z),s(_),s(x),s(I),s(E),s(N),s(Q),s(F),s(z),s(X),s(A),s(P),s(q),s(Y)),s(i),v(r,e),v($,e),v(R,e),v(V,e),v(H,e)}}}const Ze='{"title":"AutoPipeline","local":"autopipeline","sections":[{"title":"Unsupported checkpoints","local":"unsupported-checkpoints","sections":[],"depth":2}],"depth":1}';function be(k){return pe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class We extends oe{constructor(i){super(),re(this,i,be,ge,ne,{})}}export{We as component};
