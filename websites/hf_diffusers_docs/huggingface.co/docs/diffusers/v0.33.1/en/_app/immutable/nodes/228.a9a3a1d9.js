import{s as X,o as Y,n as V}from"../chunks/scheduler.8c3d61f6.js";import{S as Z,i as tt,g as h,s as r,r as T,A as et,h as c,f as a,c as u,j,u as C,x as q,k as J,y as nt,a as i,v as y,d as L,t as H,w as P}from"../chunks/index.da70eac4.js";import{T as K}from"../chunks/Tip.1d9b8c37.js";import{H as N,E as at}from"../chunks/index.5d4ab994.js";function it(b){let n,m='Interested in adding a new quantization method to Diffusers? Refer to the <a href="https://huggingface.co/docs/transformers/main/en/quantization/contribute" rel="nofollow">Contribute new quantization method guide</a> to learn more about adding a new quantization method.';return{c(){n=h("p"),n.innerHTML=m},l(o){n=c(o,"P",{"data-svelte-h":!0}),q(n)!=="svelte-1rkietk"&&(n.innerHTML=m)},m(o,s){i(o,n,s)},p:V,d(o){o&&a(n)}}}function ot(b){let n,m="If you are new to the quantization field, we recommend you to check out these beginner-friendly courses about quantization in collaboration with DeepLearning.AI:",o,s,p='<li><a href="https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/" rel="nofollow">Quantization Fundamentals with Hugging Face</a></li> <li><a href="https://www.deeplearning.ai/short-courses/quantization-in-depth/" rel="nofollow">Quantization in Depth</a></li>';return{c(){n=h("p"),n.textContent=m,o=r(),s=h("ul"),s.innerHTML=p},l(l){n=c(l,"P",{"data-svelte-h":!0}),q(n)!=="svelte-lvs4zq"&&(n.textContent=m),o=u(l),s=c(l,"UL",{"data-svelte-h":!0}),q(s)!=="svelte-tsydsg"&&(s.innerHTML=p)},m(l,f){i(l,n,f),i(l,o,f),i(l,s,f)},p:V,d(l){l&&(a(n),a(o),a(s))}}}function st(b){let n,m,o,s,p,l,f,I="Quantization techniques focus on representing data with less information while also trying to not lose too much accuracy. This often means converting a data type to represent the same information with fewer bits. For example, if your model weights are stored as 32-bit floating points and theyâ€™re quantized to 16-bit floating points, this halves the model size which makes it easier to store and reduces memory-usage. Lower precision can also speedup inference because it takes less time to perform calculations with fewer bits.",M,$,k,d,E,w,Q,g,S="Diffusers currently supports the following quantization methods.",A,v,B='<li><a href="./bitsandbytes">BitsandBytes</a></li> <li><a href="./torchao">TorchAO</a></li> <li><a href="./gguf">GGUF</a></li> <li><a href="./quanto.md">Quanto</a></li>',D,_,O='<a href="https://huggingface.co/docs/transformers/main/en/quantization/overview#when-to-use-what" rel="nofollow">This resource</a> provides a good overview of the pros and cons of different quantization techniques.',F,z,U,x,G;return p=new N({props:{title:"Quantization",local:"quantization",headingTag:"h1"}}),$=new K({props:{$$slots:{default:[it]},$$scope:{ctx:b}}}),d=new K({props:{$$slots:{default:[ot]},$$scope:{ctx:b}}}),w=new N({props:{title:"When to use what?",local:"when-to-use-what",headingTag:"h2"}}),z=new at({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/overview.md"}}),{c(){n=h("meta"),m=r(),o=h("p"),s=r(),T(p.$$.fragment),l=r(),f=h("p"),f.textContent=I,M=r(),T($.$$.fragment),k=r(),T(d.$$.fragment),E=r(),T(w.$$.fragment),Q=r(),g=h("p"),g.textContent=S,A=r(),v=h("ul"),v.innerHTML=B,D=r(),_=h("p"),_.innerHTML=O,F=r(),T(z.$$.fragment),U=r(),x=h("p"),this.h()},l(t){const e=et("svelte-u9bgzb",document.head);n=c(e,"META",{name:!0,content:!0}),e.forEach(a),m=u(t),o=c(t,"P",{}),j(o).forEach(a),s=u(t),C(p.$$.fragment,t),l=u(t),f=c(t,"P",{"data-svelte-h":!0}),q(f)!=="svelte-1euzkei"&&(f.textContent=I),M=u(t),C($.$$.fragment,t),k=u(t),C(d.$$.fragment,t),E=u(t),C(w.$$.fragment,t),Q=u(t),g=c(t,"P",{"data-svelte-h":!0}),q(g)!=="svelte-2vpsnq"&&(g.textContent=S),A=u(t),v=c(t,"UL",{"data-svelte-h":!0}),q(v)!=="svelte-ct2mst"&&(v.innerHTML=B),D=u(t),_=c(t,"P",{"data-svelte-h":!0}),q(_)!=="svelte-199sve8"&&(_.innerHTML=O),F=u(t),C(z.$$.fragment,t),U=u(t),x=c(t,"P",{}),j(x).forEach(a),this.h()},h(){J(n,"name","hf:doc:metadata"),J(n,"content",lt)},m(t,e){nt(document.head,n),i(t,m,e),i(t,o,e),i(t,s,e),y(p,t,e),i(t,l,e),i(t,f,e),i(t,M,e),y($,t,e),i(t,k,e),y(d,t,e),i(t,E,e),y(w,t,e),i(t,Q,e),i(t,g,e),i(t,A,e),i(t,v,e),i(t,D,e),i(t,_,e),i(t,F,e),y(z,t,e),i(t,U,e),i(t,x,e),G=!0},p(t,[e]){const R={};e&2&&(R.$$scope={dirty:e,ctx:t}),$.$set(R);const W={};e&2&&(W.$$scope={dirty:e,ctx:t}),d.$set(W)},i(t){G||(L(p.$$.fragment,t),L($.$$.fragment,t),L(d.$$.fragment,t),L(w.$$.fragment,t),L(z.$$.fragment,t),G=!0)},o(t){H(p.$$.fragment,t),H($.$$.fragment,t),H(d.$$.fragment,t),H(w.$$.fragment,t),H(z.$$.fragment,t),G=!1},d(t){t&&(a(m),a(o),a(s),a(l),a(f),a(M),a(k),a(E),a(Q),a(g),a(A),a(v),a(D),a(_),a(F),a(U),a(x)),a(n),P(p,t),P($,t),P(d,t),P(w,t),P(z,t)}}}const lt='{"title":"Quantization","local":"quantization","sections":[{"title":"When to use what?","local":"when-to-use-what","sections":[],"depth":2}],"depth":1}';function rt(b){return Y(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ht extends Z{constructor(n){super(),tt(this,n,rt,st,X,{})}}export{ht as component};
