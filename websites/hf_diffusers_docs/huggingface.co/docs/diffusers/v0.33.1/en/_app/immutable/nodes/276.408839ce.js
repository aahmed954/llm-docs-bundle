import{s as Ba,o as Ca,n as P}from"../chunks/scheduler.8c3d61f6.js";import{S as xa,i as La,g as r,s as i,r as u,A as Ra,h as p,f as l,c as n,j as Wa,u as c,x as d,k as z,y as ka,a,v as M,d as y,t as g,w as b,m as Ha,n as Aa}from"../chunks/index.da70eac4.js";import{T as E}from"../chunks/Tip.1d9b8c37.js";import{C as U}from"../chunks/CodeBlock.a9c4becf.js";import{D as Xa}from"../chunks/DocNotebookDropdown.48852948.js";import{H as x,E as Va}from"../chunks/index.5d4ab994.js";import{H as Fa,a as Ia}from"../chunks/HfOption.6ab18950.js";function Qa(T){let s,w='Feel free to browse the <a href="https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer" rel="nofollow">Stable Diffusion Conceptualizer</a>, <a href="https://huggingface.co/spaces/multimodalart/LoraTheExplorer" rel="nofollow">LoRA the Explorer</a>, and the <a href="https://huggingface.co/spaces/huggingface-projects/diffusers-gallery" rel="nofollow">Diffusers Models Gallery</a> for checkpoints and embeddings to use.';return{c(){s=r("p"),s.innerHTML=w},l(o){s=p(o,"P",{"data-svelte-h":!0}),d(s)!=="svelte-unc393"&&(s.innerHTML=w)},m(o,h){a(o,s,h)},p:P,d(o){o&&l(s)}}}function qa(T){let s,w='LoRA is a very general training technique that can be used with other training methods. For example, it is common to train a model with DreamBooth and LoRA. It is also increasingly common to load and merge multiple LoRAs to create new and unique images. You can learn more about it in the in-depth <a href="merge_loras">Merge LoRAs</a> guide since merging is outside the scope of this loading guide.';return{c(){s=r("p"),s.innerHTML=w},l(o){s=p(o,"P",{"data-svelte-h":!0}),d(s)!=="svelte-bp0omm"&&(s.innerHTML=w)},m(o,h){a(o,s,h)},p:P,d(o){o&&l(s)}}}function Sa(T){let s,w="Currently, <code>set_adapters()</code> only supports scaling attention weights. If a LoRA has other parts (e.g., resnets or down-/upsamplers), they will keep a scale of 1.0.";return{c(){s=r("p"),s.innerHTML=w},l(o){s=p(o,"P",{"data-svelte-h":!0}),d(s)!=="svelte-1aq2a67"&&(s.innerHTML=w)},m(o,h){a(o,s,h)},p:P,d(o){o&&l(s)}}}function Ya(T){let s,w="Hotswapping is not currently supported for LoRA adapters that target the text encoder.";return{c(){s=r("p"),s.textContent=w},l(o){s=p(o,"P",{"data-svelte-h":!0}),d(s)!=="svelte-fs1y0g"&&(s.textContent=w)},m(o,h){a(o,s,h)},p:P,d(o){o&&l(s)}}}function Na(T){let s,w='Move your code inside the <code>with torch._dynamo.config.patch(error_on_recompile=True)</code> context manager to detect if a model was recompiled. If you detect recompilation despite following all the steps above, please open an issue with <a href="https://github.com/huggingface/diffusers/issues" rel="nofollow">Diffusers</a> with a reproducible example.';return{c(){s=r("p"),s.innerHTML=w},l(o){s=p(o,"P",{"data-svelte-h":!0}),d(s)!=="svelte-6omohz"&&(s.innerHTML=w)},m(o,h){a(o,s,h)},p:P,d(o){o&&l(s)}}}function za(T){let s,w="Some limitations of using Kohya LoRAs with 🤗 Diffusers include:",o,h,j='<li>Images may not look like those generated by UIs - like ComfyUI - for multiple reasons, which are explained <a href="https://github.com/huggingface/diffusers/pull/4287/#issuecomment-1655110736" rel="nofollow">here</a>.</li> <li><a href="https://github.com/KohakuBlueleaf/LyCORIS" rel="nofollow">LyCORIS checkpoints</a> aren’t fully supported. The <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a> method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported.</li>';return{c(){s=r("p"),s.textContent=w,o=i(),h=r("ul"),h.innerHTML=j},l(f){s=p(f,"P",{"data-svelte-h":!0}),d(s)!=="svelte-1aa8at7"&&(s.textContent=w),o=n(f),h=p(f,"UL",{"data-svelte-h":!0}),d(h)!=="svelte-duaqhy"&&(h.innerHTML=j)},m(f,$){a(f,s,$),a(f,o,$),a(f,h,$)},p:P,d(f){f&&(l(s),l(o),l(h))}}}function Ea(T){let s,w='To load a Kohya LoRA, let’s download the <a href="https://civitai.com/models/150986/blueprintify-sd-xl-10" rel="nofollow">Blueprintify SD XL 1.0</a> checkpoint from <a href="https://civitai.com/" rel="nofollow">Civitai</a> as an example:',o,h,j,f,$='Load the LoRA checkpoint with the <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a> method, and specify the filename in the <code>weight_name</code> parameter:',G,Z,D,W,C="Generate an image:",K,I,_,v,B;return h=new U({props:{code:"IXdnZXQlMjBodHRwcyUzQSUyRiUyRmNpdml0YWkuY29tJTJGYXBpJTJGZG93bmxvYWQlMkZtb2RlbHMlMkYxNjg3NzYlMjAtTyUyMGJsdWVwcmludGlmeS1zZC14bC0xMC5zYWZldGVuc29ycw==",highlighted:"!wget https://civitai.com/api/download/models/168776 -O blueprintify-sd-xl-10.safetensors",wrap:!1}}),Z=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9yVGV4dDJJbWFnZS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmlsaXR5YWklMkZzdGFibGUtZGlmZnVzaW9uLXhsLWJhc2UtMS4wJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byglMjJjdWRhJTIyKSUwQXBpcGVsaW5lLmxvYWRfbG9yYV93ZWlnaHRzKCUyMnBhdGglMkZ0byUyRndlaWdodHMlMjIlMkMlMjB3ZWlnaHRfbmFtZSUzRCUyMmJsdWVwcmludGlmeS1zZC14bC0xMC5zYWZldGVuc29ycyUyMik=",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)
pipeline.load_lora_weights(<span class="hljs-string">&quot;path/to/weights&quot;</span>, weight_name=<span class="hljs-string">&quot;blueprintify-sd-xl-10.safetensors&quot;</span>)`,wrap:!1}}),I=new U({props:{code:"JTIzJTIwdXNlJTIwYmwzdXByaW50JTIwaW4lMjB0aGUlMjBwcm9tcHQlMjB0byUyMHRyaWdnZXIlMjB0aGUlMjBMb1JBJTBBcHJvbXB0JTIwJTNEJTIwJTIyYmwzdXByaW50JTJDJTIwYSUyMGhpZ2hseSUyMGRldGFpbGVkJTIwYmx1ZXByaW50JTIwb2YlMjB0aGUlMjBlaWZmZWwlMjB0b3dlciUyQyUyMGV4cGxhaW5pbmclMjBob3clMjB0byUyMGJ1aWxkJTIwYWxsJTIwcGFydHMlMkMlMjBtYW55JTIwdHh0JTJDJTIwYmx1ZXByaW50JTIwZ3JpZCUyMGJhY2tkcm9wJTIyJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZShwcm9tcHQpLmltYWdlcyU1QjAlNUQlMEFpbWFnZQ==",highlighted:`<span class="hljs-comment"># use bl3uprint in the prompt to trigger the LoRA</span>
prompt = <span class="hljs-string">&quot;bl3uprint, a highly detailed blueprint of the eiffel tower, explaining how to build all parts, many txt, blueprint grid backdrop&quot;</span>
image = pipeline(prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),v=new E({props:{warning:!0,$$slots:{default:[za]},$$scope:{ctx:T}}}),{c(){s=r("p"),s.innerHTML=w,o=i(),u(h.$$.fragment),j=i(),f=r("p"),f.innerHTML=$,G=i(),u(Z.$$.fragment),D=i(),W=r("p"),W.textContent=C,K=i(),u(I.$$.fragment),_=i(),u(v.$$.fragment)},l(m){s=p(m,"P",{"data-svelte-h":!0}),d(s)!=="svelte-1xg7vsu"&&(s.innerHTML=w),o=n(m),c(h.$$.fragment,m),j=n(m),f=p(m,"P",{"data-svelte-h":!0}),d(f)!=="svelte-qn8j3g"&&(f.innerHTML=$),G=n(m),c(Z.$$.fragment,m),D=n(m),W=p(m,"P",{"data-svelte-h":!0}),d(W)!=="svelte-1un5bjt"&&(W.textContent=C),K=n(m),c(I.$$.fragment,m),_=n(m),c(v.$$.fragment,m)},m(m,J){a(m,s,J),a(m,o,J),M(h,m,J),a(m,j,J),a(m,f,J),a(m,G,J),M(Z,m,J),a(m,D,J),a(m,W,J),a(m,K,J),M(I,m,J),a(m,_,J),M(v,m,J),B=!0},p(m,J){const rt={};J&2&&(rt.$$scope={dirty:J,ctx:m}),v.$set(rt)},i(m){B||(y(h.$$.fragment,m),y(Z.$$.fragment,m),y(I.$$.fragment,m),y(v.$$.fragment,m),B=!0)},o(m){g(h.$$.fragment,m),g(Z.$$.fragment,m),g(I.$$.fragment,m),g(v.$$.fragment,m),B=!1},d(m){m&&(l(s),l(o),l(j),l(f),l(G),l(D),l(W),l(K),l(_)),b(h,m),b(Z,m),b(I,m),b(v,m)}}}function Pa(T){let s,w='Loading a checkpoint from TheLastBen is very similar. For example, to load the <a href="https://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL" rel="nofollow">TheLastBen/William_Eggleston_Style_SDXL</a> checkpoint:',o,h,j;return h=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9yVGV4dDJJbWFnZS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmlsaXR5YWklMkZzdGFibGUtZGlmZnVzaW9uLXhsLWJhc2UtMS4wJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byglMjJjdWRhJTIyKSUwQXBpcGVsaW5lLmxvYWRfbG9yYV93ZWlnaHRzKCUyMlRoZUxhc3RCZW4lMkZXaWxsaWFtX0VnZ2xlc3Rvbl9TdHlsZV9TRFhMJTIyJTJDJTIwd2VpZ2h0X25hbWUlM0QlMjJ3ZWdnLnNhZmV0ZW5zb3JzJTIyKSUwQSUwQSUyMyUyMHVzZSUyMGJ5JTIwd2lsbGlhbSUyMGVnZ2xlc3RvbiUyMGluJTIwdGhlJTIwcHJvbXB0JTIwdG8lMjB0cmlnZ2VyJTIwdGhlJTIwTG9SQSUwQXByb21wdCUyMCUzRCUyMCUyMmElMjBob3VzZSUyMGJ5JTIwd2lsbGlhbSUyMGVnZ2xlc3RvbiUyQyUyMHN1bnJheXMlMkMlMjBiZWF1dGlmdWwlMkMlMjBzdW5saWdodCUyQyUyMHN1bnJheXMlMkMlMjBiZWF1dGlmdWwlMjIlMEFpbWFnZSUyMCUzRCUyMHBpcGVsaW5lKHByb21wdCUzRHByb21wdCkuaW1hZ2VzJTVCMCU1RCUwQWltYWdl",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)
pipeline.load_lora_weights(<span class="hljs-string">&quot;TheLastBen/William_Eggleston_Style_SDXL&quot;</span>, weight_name=<span class="hljs-string">&quot;wegg.safetensors&quot;</span>)

<span class="hljs-comment"># use by william eggleston in the prompt to trigger the LoRA</span>
prompt = <span class="hljs-string">&quot;a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful&quot;</span>
image = pipeline(prompt=prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),{c(){s=r("p"),s.innerHTML=w,o=i(),u(h.$$.fragment)},l(f){s=p(f,"P",{"data-svelte-h":!0}),d(s)!=="svelte-qdt5l0"&&(s.innerHTML=w),o=n(f),c(h.$$.fragment,f)},m(f,$){a(f,s,$),a(f,o,$),M(h,f,$),j=!0},p:P,i(f){j||(y(h.$$.fragment,f),j=!0)},o(f){g(h.$$.fragment,f),j=!1},d(f){f&&(l(s),l(o)),b(h,f)}}}function Da(T){let s,w,o,h;return s=new Ia({props:{id:"other-trainers",option:"Kohya",$$slots:{default:[Ea]},$$scope:{ctx:T}}}),o=new Ia({props:{id:"other-trainers",option:"TheLastBen",$$slots:{default:[Pa]},$$scope:{ctx:T}}}),{c(){u(s.$$.fragment),w=i(),u(o.$$.fragment)},l(j){c(s.$$.fragment,j),w=n(j),c(o.$$.fragment,j)},m(j,f){M(s,j,f),a(j,w,f),M(o,j,f),h=!0},p(j,f){const $={};f&2&&($.$$scope={dirty:f,ctx:j}),s.$set($);const G={};f&2&&(G.$$scope={dirty:f,ctx:j}),o.$set(G)},i(j){h||(y(s.$$.fragment,j),y(o.$$.fragment,j),h=!0)},o(j){g(s.$$.fragment,j),g(o.$$.fragment,j),h=!1},d(j){j&&l(w),b(s,j),b(o,j)}}}function Ka(T){let s,w=`Diffusers currently only supports IP-Adapter for some of the most popular pipelines. Feel free to open a feature request if you have a cool use case and want to integrate IP-Adapter with an unsupported pipeline!
Official IP-Adapter checkpoints are available from <a href="https://huggingface.co/h94/IP-Adapter" rel="nofollow">h94/IP-Adapter</a>.`;return{c(){s=r("p"),s.innerHTML=w},l(o){s=p(o,"P",{"data-svelte-h":!0}),d(s)!=="svelte-13d91fo"&&(s.innerHTML=w)},m(o,h){a(o,s,h)},p:P,d(o){o&&l(s)}}}function Oa(T){let s;return{c(){s=Ha("As InsightFace pretrained models are available for non-commercial research purposes, IP-Adapter-FaceID models are released exclusively for research purposes and are not intended for commercial use.")},l(w){s=Aa(w,"As InsightFace pretrained models are available for non-commercial research purposes, IP-Adapter-FaceID models are released exclusively for research purposes and are not intended for commercial use.")},m(w,o){a(w,s,o)},d(w){w&&l(s)}}}function es(T){let s,w,o,h,j,f,$,G,Z,D='There are several <a href="../training/overview">training</a> techniques for personalizing diffusion models to generate images of a specific subject or images in certain styles. Each of these training methods produces a different type of adapter. Some of the adapters generate an entirely new model, while other adapters only modify a smaller set of embeddings or weights. This means the loading process for each adapter is also different.',W,C,K="This guide will show you how to load DreamBooth, textual inversion, and LoRA weights.",I,_,v,B,m,J,rt='<a href="https://dreambooth.github.io/" rel="nofollow">DreamBooth</a> finetunes an <em>entire diffusion model</em> on just several images of a subject to generate images of that subject in new styles and settings. This method works by using a special word in the prompt that the model learns to associate with the subject image. Of all the training methods, DreamBooth produces the largest file size (usually a few GBs) because it is a full checkpoint model.',dt,O,xl='Let’s load the <a href="https://huggingface.co/sd-dreambooth-library/herge-style" rel="nofollow">herge_style</a> checkpoint, which is trained on just 10 images drawn by Hergé, to generate images in that style. For it to work, you need to include the special word <code>herge_style</code> in your prompt to trigger the checkpoint:',mt,ee,ft,L,Ll='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_dreambooth.png"/>',ht,te,ut,le,Rl='<a href="https://textual-inversion.github.io/" rel="nofollow">Textual inversion</a> is very similar to DreamBooth and it can also personalize a diffusion model to generate certain concepts (styles, objects) from just a few images. This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file.',ct,ae,kl="Because textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model.",Mt,se,yt,ie,Hl='Now you can load the textual inversion embeddings with the <a href="/docs/diffusers/v0.33.1/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion">load_textual_inversion()</a> method and generate some images. Let’s load the <a href="https://huggingface.co/sd-concepts-library/gta5-artwork" rel="nofollow">sd-concepts-library/gta5-artwork</a> embeddings and you’ll need to include the special word <code>&lt;gta5-artwork&gt;</code> in your prompt to trigger it:',gt,ne,bt,R,Al='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_txt_embed.png"/>',wt,oe,Xl='Textual inversion can also be trained on undesirable things to create <em>negative embeddings</em> to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand. This can be an easy way to quickly improve your prompt. You’ll also load the embeddings with <a href="/docs/diffusers/v0.33.1/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion">load_textual_inversion()</a>, but this time, you’ll need two more parameters:',jt,re,Vl="<li><code>weight_name</code>: specifies the weight file to load if the file was saved in the 🤗 Diffusers format with a specific name or if the file is stored in the A1111 format</li> <li><code>token</code>: specifies the special word to use in the prompt to trigger the embeddings</li>",Tt,pe,Fl='Let’s load the <a href="https://huggingface.co/sayakpaul/EasyNegative-test" rel="nofollow">sayakpaul/EasyNegative-test</a> embeddings:',Jt,de,Ut,me,Ql="Now you can use the <code>token</code> to generate an image with the negative embeddings:",$t,fe,Zt,k,ql='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png"/>',_t,he,vt,ue,Sl='<a href="https://huggingface.co/papers/2106.09685" rel="nofollow">Low-Rank Adaptation (LoRA)</a> is a popular training technique because it is fast and generates smaller file sizes (a couple hundred MBs). Like the other methods in this guide, LoRA can train a model to learn new styles from just a few images. It works by inserting new weights into the diffusion model and then only the new weights are trained instead of the entire model. This makes LoRAs faster to train and easier to store.',Gt,H,Wt,ce,Yl="LoRAs also need to be used with another model:",It,Me,Bt,ye,Nl='Then use the <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a> method to load the <a href="https://huggingface.co/ostris/super-cereal-sdxl-lora" rel="nofollow">ostris/super-cereal-sdxl-lora</a> weights and specify the weights filename from the repository:',Ct,ge,xt,A,zl='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_lora.png"/>',Lt,be,El='The <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a> method loads LoRA weights into both the UNet and text encoder. It is the preferred way for loading LoRAs because it can handle cases where:',Rt,we,Pl="<li>the LoRA weights don’t have separate identifiers for the UNet and text encoder</li> <li>the LoRA weights have separate identifiers for the UNet and text encoder</li>",kt,je,Dl='To directly load (and save) a LoRA adapter at the <em>model-level</em>, use <code>~PeftAdapterMixin.load_lora_adapter</code>, which builds and prepares the necessary model configuration for the adapter. Like <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a>, <code>PeftAdapterMixin.load_lora_adapter</code> can load LoRAs for both the UNet and text encoder. For example, if you’re loading a LoRA for the UNet, <code>PeftAdapterMixin.load_lora_adapter</code> ignores the keys for the text encoder.',Ht,Te,Kl="Use the <code>weight_name</code> parameter to specify the specific weight file and the <code>prefix</code> parameter to filter for the appropriate state dicts (<code>&quot;unet&quot;</code> in this case) to load.",At,Je,Xt,X,Ol='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_attn_proc.png"/>',Vt,Ue,ea="Save an adapter with <code>~PeftAdapterMixin.save_lora_adapter</code>.",Ft,$e,ta='To unload the LoRA weights, use the <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights">unload_lora_weights()</a> method to discard the LoRA weights and restore the model to its original weights:',Qt,Ze,qt,_e,St,ve,la='For both <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a> and <a href="/docs/diffusers/v0.33.1/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs">load_attn_procs()</a>, you can pass the <code>cross_attention_kwargs={&quot;scale&quot;: 0.5}</code> parameter to adjust how much of the LoRA weights to use. A value of <code>0</code> is the same as only using the base model weights, and a value of <code>1</code> is equivalent to using the fully finetuned LoRA.',Yt,Ge,aa="For more granular control on the amount of LoRA weights used per layer, you can use <code>set_adapters()</code> and pass a dictionary specifying by how much to scale the weights in each layer by.",Nt,We,zt,Ie,sa='This also works with multiple adapters - see <a href="https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#customize-adapters-strength" rel="nofollow">this guide</a> for how to do it.',Et,V,Pt,Be,Dt,Ce,ia='A common use case when serving multiple adapters is to load one adapter first, generate images, load another adapter, generate more images, load another adapter, etc. This workflow normally requires calling <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights">load_lora_weights()</a>, <code>set_adapters()</code>, and possibly <a href="/docs/diffusers/v0.33.1/en/api/loaders/peft#diffusers.loaders.PeftAdapterMixin.delete_adapters">delete_adapters()</a> to save memory. Moreover, if the model is compiled using <code>torch.compile</code>, performing these steps requires recompilation, which takes time.',Kt,xe,na="To better support this common workflow, you can “hotswap” a LoRA adapter, to avoid accumulating memory and in some cases, recompilation. It requires an adapter to already be loaded, and the new adapter weights are swapped in-place for the existing adapter.",Ot,Le,oa="Pass <code>hotswap=True</code> when loading a LoRA adapter to enable this feature. It is important to indicate the name of the existing adapter, (<code>default_0</code> is the default adapter name), to be swapped. If you loaded the first adapter with a different name, use that name instead.",el,Re,tl,F,ll,ke,ra='For compiled models, it is often (though not always if the second adapter targets identical LoRA ranks and scales) necessary to call <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.lora_base.LoraBaseMixin.enable_lora_hotswap">enable_lora_hotswap()</a> to avoid recompilation. Use <a href="/docs/diffusers/v0.33.1/en/api/loaders/lora#diffusers.loaders.lora_base.LoraBaseMixin.enable_lora_hotswap">enable_lora_hotswap()</a> <em>before</em> loading the first adapter, and <code>torch.compile</code> should be called <em>after</em> loading the first adapter.',al,He,sl,Ae,pa="The <code>target_rank=max_rank</code> argument is important for setting the maximum rank among all LoRA adapters that will be loaded. If you have one adapter with rank 8 and another with rank 16, pass <code>target_rank=16</code>. You should use a higher value if in doubt. By default, this value is 128.",il,Xe,da='However, there can be situations where recompilation is unavoidable. For example, if the hotswapped adapter targets more layers than the initial adapter, then recompilation is triggered. Try to load the adapter that targets the most layers first. Refer to the PEFT docs on <a href="https://huggingface.co/docs/peft/main/en/package_reference/hotswap#peft.utils.hotswap.hotswap_adapter" rel="nofollow">hotswapping</a> for more details about the limitations of this feature.',nl,Q,ol,Ve,rl,Fe,ma='Other popular LoRA trainers from the community include those by <a href="https://github.com/kohya-ss/sd-scripts/" rel="nofollow">Kohya</a> and <a href="https://github.com/TheLastBen/fast-stable-diffusion" rel="nofollow">TheLastBen</a>. These trainers create different LoRA checkpoints than those trained by 🤗 Diffusers, but they can still be loaded in the same way.',pl,q,dl,Qe,ml,qe,fa='<a href="https://ip-adapter.github.io/" rel="nofollow">IP-Adapter</a> is a lightweight adapter that enables image prompting for any diffusion model. This adapter works by decoupling the cross-attention layers of the image and text features. All the other model components are frozen and only the embedded image features in the UNet are trained. As a result, IP-Adapter files are typically only ~100MBs.',fl,Se,ha='You can learn more about how to use IP-Adapter for different tasks and specific use cases in the <a href="../using-diffusers/ip_adapter">IP-Adapter</a> guide.',hl,S,ul,Ye,ua="To start, load a Stable Diffusion checkpoint.",cl,Ne,Ml,ze,ca='Then load the IP-Adapter weights and add it to the pipeline with the <a href="/docs/diffusers/v0.33.1/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter">load_ip_adapter()</a> method.',yl,Ee,gl,Pe,Ma="Once loaded, you can use the pipeline with an image and text prompt to guide the image generation process.",bl,De,wl,Y,ya='    <img src="https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ip-bear.png"/>',jl,Ke,Tl,Oe,ga='IP-Adapter relies on an image encoder to generate image features. If the IP-Adapter repository contains an <code>image_encoder</code> subfolder, the image encoder is automatically loaded and registered to the pipeline. Otherwise, you’ll need to explicitly load the image encoder with a <a href="https://huggingface.co/docs/transformers/v4.51.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection" rel="nofollow">CLIPVisionModelWithProjection</a> model and pass it to the pipeline.',Jl,et,ba="This is the case for <em>IP-Adapter Plus</em> checkpoints which use the ViT-H image encoder.",Ul,tt,$l,lt,Zl,at,wa=`The IP-Adapter FaceID models are experimental IP Adapters that use image embeddings generated by <code>insightface</code> instead of CLIP image embeddings. Some of these models also use LoRA to improve ID consistency.
You need to install <code>insightface</code> and all its requirements to use these models.`,_l,N,vl,st,Gl,it,ja="If you want to use one of the two IP-Adapter FaceID Plus models, you must also load the CLIP image encoder, as this models use both <code>insightface</code> and CLIP image embeddings to achieve better photorealism.",Wl,nt,Il,ot,Bl,pt,Cl;return j=new x({props:{title:"Load adapters",local:"load-adapters",headingTag:"h1"}}),$=new Xa({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/loading_adapters.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/loading_adapters.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/tensorflow/loading_adapters.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/diffusers_doc/en/loading_adapters.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/loading_adapters.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/diffusers_doc/en/tensorflow/loading_adapters.ipynb"}]}}),_=new E({props:{$$slots:{default:[Qa]},$$scope:{ctx:T}}}),B=new x({props:{title:"DreamBooth",local:"dreambooth",headingTag:"h2"}}),ee=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9yVGV4dDJJbWFnZS5mcm9tX3ByZXRyYWluZWQoJTIyc2QtZHJlYW1ib290aC1saWJyYXJ5JTJGaGVyZ2Utc3R5bGUlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYpLnRvKCUyMmN1ZGElMjIpJTBBcHJvbXB0JTIwJTNEJTIwJTIyQSUyMGN1dGUlMjBoZXJnZV9zdHlsZSUyMGJyb3duJTIwYmVhciUyMGVhdGluZyUyMGElMjBzbGljZSUyMG9mJTIwcGl6emElMkMlMjBzdHVubmluZyUyMGNvbG9yJTIwc2NoZW1lJTJDJTIwbWFzdGVycGllY2UlMkMlMjBpbGx1c3RyYXRpb24lMjIlMEFpbWFnZSUyMCUzRCUyMHBpcGVsaW5lKHByb21wdCkuaW1hZ2VzJTVCMCU1RCUwQWltYWdl",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;sd-dreambooth-library/herge-style&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)
prompt = <span class="hljs-string">&quot;A cute herge_style brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration&quot;</span>
image = pipeline(prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),te=new x({props:{title:"Textual inversion",local:"textual-inversion",headingTag:"h2"}}),se=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9yVGV4dDJJbWFnZS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmxlLWRpZmZ1c2lvbi12MS01JTJGc3RhYmxlLWRpZmZ1c2lvbi12MS01JTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byglMjJjdWRhJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)`,wrap:!1}}),ne=new U({props:{code:"cGlwZWxpbmUubG9hZF90ZXh0dWFsX2ludmVyc2lvbiglMjJzZC1jb25jZXB0cy1saWJyYXJ5JTJGZ3RhNS1hcnR3b3JrJTIyKSUwQXByb21wdCUyMCUzRCUyMCUyMkElMjBjdXRlJTIwYnJvd24lMjBiZWFyJTIwZWF0aW5nJTIwYSUyMHNsaWNlJTIwb2YlMjBwaXp6YSUyQyUyMHN0dW5uaW5nJTIwY29sb3IlMjBzY2hlbWUlMkMlMjBtYXN0ZXJwaWVjZSUyQyUyMGlsbHVzdHJhdGlvbiUyQyUyMCUzQ2d0YTUtYXJ0d29yayUzRSUyMHN0eWxlJTIyJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZShwcm9tcHQpLmltYWdlcyU1QjAlNUQlMEFpbWFnZQ==",highlighted:`pipeline.load_textual_inversion(<span class="hljs-string">&quot;sd-concepts-library/gta5-artwork&quot;</span>)
prompt = <span class="hljs-string">&quot;A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, &lt;gta5-artwork&gt; style&quot;</span>
image = pipeline(prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),de=new U({props:{code:"cGlwZWxpbmUubG9hZF90ZXh0dWFsX2ludmVyc2lvbiglMEElMjAlMjAlMjAlMjAlMjJzYXlha3BhdWwlMkZFYXN5TmVnYXRpdmUtdGVzdCUyMiUyQyUyMHdlaWdodF9uYW1lJTNEJTIyRWFzeU5lZ2F0aXZlLnNhZmV0ZW5zb3JzJTIyJTJDJTIwdG9rZW4lM0QlMjJFYXN5TmVnYXRpdmUlMjIlMEEp",highlighted:`pipeline.load_textual_inversion(
    <span class="hljs-string">&quot;sayakpaul/EasyNegative-test&quot;</span>, weight_name=<span class="hljs-string">&quot;EasyNegative.safetensors&quot;</span>, token=<span class="hljs-string">&quot;EasyNegative&quot;</span>
)`,wrap:!1}}),fe=new U({props:{code:"cHJvbXB0JTIwJTNEJTIwJTIyQSUyMGN1dGUlMjBicm93biUyMGJlYXIlMjBlYXRpbmclMjBhJTIwc2xpY2UlMjBvZiUyMHBpenphJTJDJTIwc3R1bm5pbmclMjBjb2xvciUyMHNjaGVtZSUyQyUyMG1hc3RlcnBpZWNlJTJDJTIwaWxsdXN0cmF0aW9uJTJDJTIwRWFzeU5lZ2F0aXZlJTIyJTBBbmVnYXRpdmVfcHJvbXB0JTIwJTNEJTIwJTIyRWFzeU5lZ2F0aXZlJTIyJTBBJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZShwcm9tcHQlMkMlMjBuZWdhdGl2ZV9wcm9tcHQlM0RuZWdhdGl2ZV9wcm9tcHQlMkMlMjBudW1faW5mZXJlbmNlX3N0ZXBzJTNENTApLmltYWdlcyU1QjAlNUQlMEFpbWFnZQ==",highlighted:`prompt = <span class="hljs-string">&quot;A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative&quot;</span>
negative_prompt = <span class="hljs-string">&quot;EasyNegative&quot;</span>

image = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=<span class="hljs-number">50</span>).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),he=new x({props:{title:"LoRA",local:"lora",headingTag:"h2"}}),H=new E({props:{$$slots:{default:[qa]},$$scope:{ctx:T}}}),Me=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9yVGV4dDJJbWFnZS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmlsaXR5YWklMkZzdGFibGUtZGlmZnVzaW9uLXhsLWJhc2UtMS4wJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byglMjJjdWRhJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)`,wrap:!1}}),ge=new U({props:{code:"cGlwZWxpbmUubG9hZF9sb3JhX3dlaWdodHMoJTIyb3N0cmlzJTJGc3VwZXItY2VyZWFsLXNkeGwtbG9yYSUyMiUyQyUyMHdlaWdodF9uYW1lJTNEJTIyY2VyZWFsX2JveF9zZHhsX3YxLnNhZmV0ZW5zb3JzJTIyKSUwQXByb21wdCUyMCUzRCUyMCUyMmJlYXJzJTJDJTIwcGl6emElMjBiaXRlcyUyMiUwQWltYWdlJTIwJTNEJTIwcGlwZWxpbmUocHJvbXB0KS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2U=",highlighted:`pipeline.load_lora_weights(<span class="hljs-string">&quot;ostris/super-cereal-sdxl-lora&quot;</span>, weight_name=<span class="hljs-string">&quot;cereal_box_sdxl_v1.safetensors&quot;</span>)
prompt = <span class="hljs-string">&quot;bears, pizza bites&quot;</span>
image = pipeline(prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),Je=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwQXV0b1BpcGVsaW5lRm9yVGV4dDJJbWFnZS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmlsaXR5YWklMkZzdGFibGUtZGlmZnVzaW9uLXhsLWJhc2UtMS4wJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byglMjJjdWRhJTIyKSUwQXBpcGVsaW5lLnVuZXQubG9hZF9sb3JhX2FkYXB0ZXIoJTIyamJpbGNrZS1oZiUyRnNkeGwtY2luZW1hdGljLTElMjIlMkMlMjB3ZWlnaHRfbmFtZSUzRCUyMnB5dG9yY2hfbG9yYV93ZWlnaHRzLnNhZmV0ZW5zb3JzJTIyJTJDJTIwcHJlZml4JTNEJTIydW5ldCUyMiklMEElMEElMjMlMjB1c2UlMjBjbm10JTIwaW4lMjB0aGUlMjBwcm9tcHQlMjB0byUyMHRyaWdnZXIlMjB0aGUlMjBMb1JBJTBBcHJvbXB0JTIwJTNEJTIwJTIyQSUyMGN1dGUlMjBjbm10JTIwZWF0aW5nJTIwYSUyMHNsaWNlJTIwb2YlMjBwaXp6YSUyQyUyMHN0dW5uaW5nJTIwY29sb3IlMjBzY2hlbWUlMkMlMjBtYXN0ZXJwaWVjZSUyQyUyMGlsbHVzdHJhdGlvbiUyMiUwQWltYWdlJTIwJTNEJTIwcGlwZWxpbmUocHJvbXB0KS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2U=",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)
pipeline.unet.load_lora_adapter(<span class="hljs-string">&quot;jbilcke-hf/sdxl-cinematic-1&quot;</span>, weight_name=<span class="hljs-string">&quot;pytorch_lora_weights.safetensors&quot;</span>, prefix=<span class="hljs-string">&quot;unet&quot;</span>)

<span class="hljs-comment"># use cnmt in the prompt to trigger the LoRA</span>
prompt = <span class="hljs-string">&quot;A cute cnmt eating a slice of pizza, stunning color scheme, masterpiece, illustration&quot;</span>
image = pipeline(prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),Ze=new U({props:{code:"cGlwZWxpbmUudW5sb2FkX2xvcmFfd2VpZ2h0cygp",highlighted:"pipeline.unload_lora_weights()",wrap:!1}}),_e=new x({props:{title:"Adjust LoRA weight scale",local:"adjust-lora-weight-scale",headingTag:"h3"}}),We=new U({props:{code:"cGlwZSUyMCUzRCUyMC4uLiUyMCUyMyUyMGNyZWF0ZSUyMHBpcGVsaW5lJTBBcGlwZS5sb2FkX2xvcmFfd2VpZ2h0cyguLi4lMkMlMjBhZGFwdGVyX25hbWUlM0QlMjJteV9hZGFwdGVyJTIyKSUwQXNjYWxlcyUyMCUzRCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMnRleHRfZW5jb2RlciUyMiUzQSUyMDAuNSUyQyUwQSUyMCUyMCUyMCUyMCUyMnRleHRfZW5jb2Rlcl8yJTIyJTNBJTIwMC41JTJDJTIwJTIwJTIzJTIwb25seSUyMHVzYWJsZSUyMGlmJTIwcGlwZSUyMGhhcyUyMGElMjAybmQlMjB0ZXh0JTIwZW5jb2RlciUwQSUyMCUyMCUyMCUyMCUyMnVuZXQlMjIlM0ElMjAlN0IlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJkb3duJTIyJTNBJTIwMC45JTJDJTIwJTIwJTIzJTIwYWxsJTIwdHJhbnNmb3JtZXJzJTIwaW4lMjB0aGUlMjBkb3duLXBhcnQlMjB3aWxsJTIwdXNlJTIwc2NhbGUlMjAwLjklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjMlMjAlMjJtaWQlMjIlMjAlMjAlMjMlMjBpbiUyMHRoaXMlMjBleGFtcGxlJTIwJTIybWlkJTIyJTIwaXMlMjBub3QlMjBnaXZlbiUyQyUyMHRoZXJlZm9yZSUyMGFsbCUyMHRyYW5zZm9ybWVycyUyMGluJTIwdGhlJTIwbWlkJTIwcGFydCUyMHdpbGwlMjB1c2UlMjB0aGUlMjBkZWZhdWx0JTIwc2NhbGUlMjAxLjAlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJ1cCUyMiUzQSUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmJsb2NrXzAlMjIlM0ElMjAwLjYlMkMlMjAlMjAlMjMlMjBhbGwlMjAzJTIwdHJhbnNmb3JtZXJzJTIwaW4lMjB0aGUlMjAwdGglMjBibG9jayUyMGluJTIwdGhlJTIwdXAtcGFydCUyMHdpbGwlMjB1c2UlMjBzY2FsZSUyMDAuNiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmJsb2NrXzElMjIlM0ElMjAlNUIwLjQlMkMlMjAwLjglMkMlMjAxLjAlNUQlMkMlMjAlMjAlMjMlMjB0aGUlMjAzJTIwdHJhbnNmb3JtZXJzJTIwaW4lMjB0aGUlMjAxc3QlMjBibG9jayUyMGluJTIwdGhlJTIwdXAtcGFydCUyMHdpbGwlMjB1c2UlMjBzY2FsZXMlMjAwLjQlMkMlMjAwLjglMjBhbmQlMjAxLjAlMjByZXNwZWN0aXZlbHklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlN0QlMEElMjAlMjAlMjAlMjAlN0QlMEElN0QlMEFwaXBlLnNldF9hZGFwdGVycyglMjJteV9hZGFwdGVyJTIyJTJDJTIwc2NhbGVzKQ==",highlighted:`pipe = ... <span class="hljs-comment"># create pipeline</span>
pipe.load_lora_weights(..., adapter_name=<span class="hljs-string">&quot;my_adapter&quot;</span>)
scales = {
    <span class="hljs-string">&quot;text_encoder&quot;</span>: <span class="hljs-number">0.5</span>,
    <span class="hljs-string">&quot;text_encoder_2&quot;</span>: <span class="hljs-number">0.5</span>,  <span class="hljs-comment"># only usable if pipe has a 2nd text encoder</span>
    <span class="hljs-string">&quot;unet&quot;</span>: {
        <span class="hljs-string">&quot;down&quot;</span>: <span class="hljs-number">0.9</span>,  <span class="hljs-comment"># all transformers in the down-part will use scale 0.9</span>
        <span class="hljs-comment"># &quot;mid&quot;  # in this example &quot;mid&quot; is not given, therefore all transformers in the mid part will use the default scale 1.0</span>
        <span class="hljs-string">&quot;up&quot;</span>: {
            <span class="hljs-string">&quot;block_0&quot;</span>: <span class="hljs-number">0.6</span>,  <span class="hljs-comment"># all 3 transformers in the 0th block in the up-part will use scale 0.6</span>
            <span class="hljs-string">&quot;block_1&quot;</span>: [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">1.0</span>],  <span class="hljs-comment"># the 3 transformers in the 1st block in the up-part will use scales 0.4, 0.8 and 1.0 respectively</span>
        }
    }
}
pipe.set_adapters(<span class="hljs-string">&quot;my_adapter&quot;</span>, scales)`,wrap:!1}}),V=new E({props:{warning:!0,$$slots:{default:[Sa]},$$scope:{ctx:T}}}),Be=new x({props:{title:"Hotswapping LoRA adapters",local:"hotswapping-lora-adapters",headingTag:"h3"}}),Re=new U({props:{code:"",highlighted:`pipe = ...
<span class="hljs-comment"># load adapter 1 as normal</span>
pipeline.load_lora_weights(file_name_adapter_1)
<span class="hljs-comment"># generate some images with adapter 1</span>
...
<span class="hljs-comment"># now hot swap the 2nd adapter</span>
pipeline.load_lora_weights(file_name_adapter_2, hotswap=<span class="hljs-literal">True</span>, adapter_name=<span class="hljs-string">&quot;default_0&quot;</span>)
<span class="hljs-comment"># generate images with adapter 2</span>`,wrap:!1}}),F=new E({props:{warning:!0,$$slots:{default:[Ya]},$$scope:{ctx:T}}}),He=new U({props:{code:"",highlighted:`pipe = ...
<span class="hljs-comment"># call this extra method</span>
pipe.enable_lora_hotswap(target_rank=max_rank)
<span class="hljs-comment"># now load adapter 1</span>
pipe.load_lora_weights(file_name_adapter_1)
<span class="hljs-comment"># now compile the unet of the pipeline</span>
pipe.unet = torch.<span class="hljs-built_in">compile</span>(pipeline.unet, ...)
<span class="hljs-comment"># generate some images with adapter 1</span>
...
<span class="hljs-comment"># now hot swap adapter 2</span>
pipeline.load_lora_weights(file_name_adapter_2, hotswap=<span class="hljs-literal">True</span>, adapter_name=<span class="hljs-string">&quot;default_0&quot;</span>)
<span class="hljs-comment"># generate images with adapter 2</span>`,wrap:!1}}),Q=new E({props:{$$slots:{default:[Na]},$$scope:{ctx:T}}}),Ve=new x({props:{title:"Kohya and TheLastBen",local:"kohya-and-thelastben",headingTag:"h3"}}),q=new Fa({props:{id:"other-trainers",options:["Kohya","TheLastBen"],$$slots:{default:[Da]},$$scope:{ctx:T}}}),Qe=new x({props:{title:"IP-Adapter",local:"ip-adapter",headingTag:"h2"}}),S=new E({props:{warning:!1,$$slots:{default:[Ka]},$$scope:{ctx:T}}}),Ne=new U({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkaWZmdXNlcnMudXRpbHMlMjBpbXBvcnQlMjBsb2FkX2ltYWdlJTBBJTBBcGlwZWxpbmUlMjAlM0QlMjBBdXRvUGlwZWxpbmVGb3JUZXh0MkltYWdlLmZyb21fcHJldHJhaW5lZCglMjJzdGFibGUtZGlmZnVzaW9uLXYxLTUlMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYpLnRvKCUyMmN1ZGElMjIp",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoPipelineForText2Image
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image

pipeline = AutoPipelineForText2Image.from_pretrained(<span class="hljs-string">&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;</span>, torch_dtype=torch.float16).to(<span class="hljs-string">&quot;cuda&quot;</span>)`,wrap:!1}}),Ee=new U({props:{code:"cGlwZWxpbmUubG9hZF9pcF9hZGFwdGVyKCUyMmg5NCUyRklQLUFkYXB0ZXIlMjIlMkMlMjBzdWJmb2xkZXIlM0QlMjJtb2RlbHMlMjIlMkMlMjB3ZWlnaHRfbmFtZSUzRCUyMmlwLWFkYXB0ZXJfc2QxNS5iaW4lMjIp",highlighted:'pipeline.load_ip_adapter(<span class="hljs-string">&quot;h94/IP-Adapter&quot;</span>, subfolder=<span class="hljs-string">&quot;models&quot;</span>, weight_name=<span class="hljs-string">&quot;ip-adapter_sd15.bin&quot;</span>)',wrap:!1}}),De=new U({props:{code:"aW1hZ2UlMjAlM0QlMjBsb2FkX2ltYWdlKCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRmRpZmZ1c2VycyUyRmxvYWRfbmVnX2VtYmVkLnBuZyUyMiklMEFnZW5lcmF0b3IlMjAlM0QlMjB0b3JjaC5HZW5lcmF0b3IoZGV2aWNlJTNEJTIyY3B1JTIyKS5tYW51YWxfc2VlZCgzMyklMEFpbWFnZXMlMjAlM0QlMjBwaXBlbGluZSglMEElQzIlQTAlMjAlQzIlQTAlMjBwcm9tcHQlM0QnYmVzdCUyMHF1YWxpdHklMkMlMjBoaWdoJTIwcXVhbGl0eSUyQyUyMHdlYXJpbmclMjBzdW5nbGFzc2VzJyUyQyUwQSVDMiVBMCUyMCVDMiVBMCUyMGlwX2FkYXB0ZXJfaW1hZ2UlM0RpbWFnZSUyQyUwQSVDMiVBMCUyMCVDMiVBMCUyMG5lZ2F0aXZlX3Byb21wdCUzRCUyMm1vbm9jaHJvbWUlMkMlMjBsb3dyZXMlMkMlMjBiYWQlMjBhbmF0b215JTJDJTIwd29yc3QlMjBxdWFsaXR5JTJDJTIwbG93JTIwcXVhbGl0eSUyMiUyQyUwQSVDMiVBMCUyMCVDMiVBMCUyMG51bV9pbmZlcmVuY2Vfc3RlcHMlM0Q1MCUyQyUwQSVDMiVBMCUyMCVDMiVBMCUyMGdlbmVyYXRvciUzRGdlbmVyYXRvciUyQyUwQSkuaW1hZ2VzJTVCMCU1RCUwQWltYWdlcw==",highlighted:`image = load_image(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png&quot;</span>)
generator = torch.Generator(device=<span class="hljs-string">&quot;cpu&quot;</span>).manual_seed(<span class="hljs-number">33</span>)
images = pipeline(
    prompt=<span class="hljs-string">&#x27;best quality, high quality, wearing sunglasses&#x27;</span>,
    ip_adapter_image=image,
    negative_prompt=<span class="hljs-string">&quot;monochrome, lowres, bad anatomy, worst quality, low quality&quot;</span>,
    num_inference_steps=<span class="hljs-number">50</span>,
    generator=generator,
).images[<span class="hljs-number">0</span>]
images`,wrap:!1}}),Ke=new x({props:{title:"IP-Adapter Plus",local:"ip-adapter-plus",headingTag:"h3"}}),tt=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uJTBBJTBBaW1hZ2VfZW5jb2RlciUyMCUzRCUyMENMSVBWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJoOTQlMkZJUC1BZGFwdGVyJTIyJTJDJTBBJTIwJTIwJTIwJTIwc3ViZm9sZGVyJTNEJTIybW9kZWxzJTJGaW1hZ2VfZW5jb2RlciUyMiUyQyUwQSUyMCUyMCUyMCUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUwQSklMEElMEFwaXBlbGluZSUyMCUzRCUyMEF1dG9QaXBlbGluZUZvclRleHQySW1hZ2UuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi14bC1iYXNlLTEuMCUyMiUyQyUwQSUyMCUyMCUyMCUyMGltYWdlX2VuY29kZXIlM0RpbWFnZV9lbmNvZGVyJTJDJTBBJTIwJTIwJTIwJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTBBKS50byglMjJjdWRhJTIyKSUwQSUwQXBpcGVsaW5lLmxvYWRfaXBfYWRhcHRlciglMjJoOTQlMkZJUC1BZGFwdGVyJTIyJTJDJTIwc3ViZm9sZGVyJTNEJTIyc2R4bF9tb2RlbHMlMjIlMkMlMjB3ZWlnaHRfbmFtZSUzRCUyMmlwLWFkYXB0ZXItcGx1c19zZHhsX3ZpdC1oLnNhZmV0ZW5zb3JzJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPVisionModelWithProjection

image_encoder = CLIPVisionModelWithProjection.from_pretrained(
    <span class="hljs-string">&quot;h94/IP-Adapter&quot;</span>,
    subfolder=<span class="hljs-string">&quot;models/image_encoder&quot;</span>,
    torch_dtype=torch.float16
)

pipeline = AutoPipelineForText2Image.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>,
    image_encoder=image_encoder,
    torch_dtype=torch.float16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

pipeline.load_ip_adapter(<span class="hljs-string">&quot;h94/IP-Adapter&quot;</span>, subfolder=<span class="hljs-string">&quot;sdxl_models&quot;</span>, weight_name=<span class="hljs-string">&quot;ip-adapter-plus_sdxl_vit-h.safetensors&quot;</span>)`,wrap:!1}}),lt=new x({props:{title:"IP-Adapter Face ID models",local:"ip-adapter-face-id-models",headingTag:"h3"}}),N=new E({props:{warning:!0,$$slots:{default:[Oa]},$$scope:{ctx:T}}}),st=new U({props:{code:"cGlwZWxpbmUlMjAlM0QlMjBBdXRvUGlwZWxpbmVGb3JUZXh0MkltYWdlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtYmFzZS0xLjAlMjIlMkMlMEElMjAlMjAlMjAlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMEEpLnRvKCUyMmN1ZGElMjIpJTBBJTBBcGlwZWxpbmUubG9hZF9pcF9hZGFwdGVyKCUyMmg5NCUyRklQLUFkYXB0ZXItRmFjZUlEJTIyJTJDJTIwc3ViZm9sZGVyJTNETm9uZSUyQyUyMHdlaWdodF9uYW1lJTNEJTIyaXAtYWRhcHRlci1mYWNlaWRfc2R4bC5iaW4lMjIlMkMlMjBpbWFnZV9lbmNvZGVyX2ZvbGRlciUzRE5vbmUp",highlighted:`pipeline = AutoPipelineForText2Image.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>,
    torch_dtype=torch.float16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

pipeline.load_ip_adapter(<span class="hljs-string">&quot;h94/IP-Adapter-FaceID&quot;</span>, subfolder=<span class="hljs-literal">None</span>, weight_name=<span class="hljs-string">&quot;ip-adapter-faceid_sdxl.bin&quot;</span>, image_encoder_folder=<span class="hljs-literal">None</span>)`,wrap:!1}}),nt=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uJTBBJTBBaW1hZ2VfZW5jb2RlciUyMCUzRCUyMENMSVBWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJsYWlvbiUyRkNMSVAtVmlULUgtMTQtbGFpb24yQi1zMzJCLWI3OUslMjIlMkMlMEElMjAlMjAlMjAlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEEpJTBBJTBBcGlwZWxpbmUlMjAlM0QlMjBBdXRvUGlwZWxpbmVGb3JUZXh0MkltYWdlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFibGUtZGlmZnVzaW9uLXYxLTUlMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUlMjIlMkMlMEElMjAlMjAlMjAlMjBpbWFnZV9lbmNvZGVyJTNEaW1hZ2VfZW5jb2RlciUyQyUwQSUyMCUyMCUyMCUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUwQSkudG8oJTIyY3VkYSUyMiklMEElMEFwaXBlbGluZS5sb2FkX2lwX2FkYXB0ZXIoJTIyaDk0JTJGSVAtQWRhcHRlci1GYWNlSUQlMjIlMkMlMjBzdWJmb2xkZXIlM0ROb25lJTJDJTIwd2VpZ2h0X25hbWUlM0QlMjJpcC1hZGFwdGVyLWZhY2VpZC1wbHVzX3NkMTUuYmluJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPVisionModelWithProjection

image_encoder = CLIPVisionModelWithProjection.from_pretrained(
    <span class="hljs-string">&quot;laion/CLIP-ViT-H-14-laion2B-s32B-b79K&quot;</span>,
    torch_dtype=torch.float16,
)

pipeline = AutoPipelineForText2Image.from_pretrained(
    <span class="hljs-string">&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;</span>,
    image_encoder=image_encoder,
    torch_dtype=torch.float16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

pipeline.load_ip_adapter(<span class="hljs-string">&quot;h94/IP-Adapter-FaceID&quot;</span>, subfolder=<span class="hljs-literal">None</span>, weight_name=<span class="hljs-string">&quot;ip-adapter-faceid-plus_sd15.bin&quot;</span>)`,wrap:!1}}),ot=new Va({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md"}}),{c(){s=r("meta"),w=i(),o=r("p"),h=i(),u(j.$$.fragment),f=i(),u($.$$.fragment),G=i(),Z=r("p"),Z.innerHTML=D,W=i(),C=r("p"),C.textContent=K,I=i(),u(_.$$.fragment),v=i(),u(B.$$.fragment),m=i(),J=r("p"),J.innerHTML=rt,dt=i(),O=r("p"),O.innerHTML=xl,mt=i(),u(ee.$$.fragment),ft=i(),L=r("div"),L.innerHTML=Ll,ht=i(),u(te.$$.fragment),ut=i(),le=r("p"),le.innerHTML=Rl,ct=i(),ae=r("p"),ae.textContent=kl,Mt=i(),u(se.$$.fragment),yt=i(),ie=r("p"),ie.innerHTML=Hl,gt=i(),u(ne.$$.fragment),bt=i(),R=r("div"),R.innerHTML=Al,wt=i(),oe=r("p"),oe.innerHTML=Xl,jt=i(),re=r("ul"),re.innerHTML=Vl,Tt=i(),pe=r("p"),pe.innerHTML=Fl,Jt=i(),u(de.$$.fragment),Ut=i(),me=r("p"),me.innerHTML=Ql,$t=i(),u(fe.$$.fragment),Zt=i(),k=r("div"),k.innerHTML=ql,_t=i(),u(he.$$.fragment),vt=i(),ue=r("p"),ue.innerHTML=Sl,Gt=i(),u(H.$$.fragment),Wt=i(),ce=r("p"),ce.textContent=Yl,It=i(),u(Me.$$.fragment),Bt=i(),ye=r("p"),ye.innerHTML=Nl,Ct=i(),u(ge.$$.fragment),xt=i(),A=r("div"),A.innerHTML=zl,Lt=i(),be=r("p"),be.innerHTML=El,Rt=i(),we=r("ul"),we.innerHTML=Pl,kt=i(),je=r("p"),je.innerHTML=Dl,Ht=i(),Te=r("p"),Te.innerHTML=Kl,At=i(),u(Je.$$.fragment),Xt=i(),X=r("div"),X.innerHTML=Ol,Vt=i(),Ue=r("p"),Ue.innerHTML=ea,Ft=i(),$e=r("p"),$e.innerHTML=ta,Qt=i(),u(Ze.$$.fragment),qt=i(),u(_e.$$.fragment),St=i(),ve=r("p"),ve.innerHTML=la,Yt=i(),Ge=r("p"),Ge.innerHTML=aa,Nt=i(),u(We.$$.fragment),zt=i(),Ie=r("p"),Ie.innerHTML=sa,Et=i(),u(V.$$.fragment),Pt=i(),u(Be.$$.fragment),Dt=i(),Ce=r("p"),Ce.innerHTML=ia,Kt=i(),xe=r("p"),xe.textContent=na,Ot=i(),Le=r("p"),Le.innerHTML=oa,el=i(),u(Re.$$.fragment),tl=i(),u(F.$$.fragment),ll=i(),ke=r("p"),ke.innerHTML=ra,al=i(),u(He.$$.fragment),sl=i(),Ae=r("p"),Ae.innerHTML=pa,il=i(),Xe=r("p"),Xe.innerHTML=da,nl=i(),u(Q.$$.fragment),ol=i(),u(Ve.$$.fragment),rl=i(),Fe=r("p"),Fe.innerHTML=ma,pl=i(),u(q.$$.fragment),dl=i(),u(Qe.$$.fragment),ml=i(),qe=r("p"),qe.innerHTML=fa,fl=i(),Se=r("p"),Se.innerHTML=ha,hl=i(),u(S.$$.fragment),ul=i(),Ye=r("p"),Ye.textContent=ua,cl=i(),u(Ne.$$.fragment),Ml=i(),ze=r("p"),ze.innerHTML=ca,yl=i(),u(Ee.$$.fragment),gl=i(),Pe=r("p"),Pe.textContent=Ma,bl=i(),u(De.$$.fragment),wl=i(),Y=r("div"),Y.innerHTML=ya,jl=i(),u(Ke.$$.fragment),Tl=i(),Oe=r("p"),Oe.innerHTML=ga,Jl=i(),et=r("p"),et.innerHTML=ba,Ul=i(),u(tt.$$.fragment),$l=i(),u(lt.$$.fragment),Zl=i(),at=r("p"),at.innerHTML=wa,_l=i(),u(N.$$.fragment),vl=i(),u(st.$$.fragment),Gl=i(),it=r("p"),it.innerHTML=ja,Wl=i(),u(nt.$$.fragment),Il=i(),u(ot.$$.fragment),Bl=i(),pt=r("p"),this.h()},l(e){const t=Ra("svelte-u9bgzb",document.head);s=p(t,"META",{name:!0,content:!0}),t.forEach(l),w=n(e),o=p(e,"P",{}),Wa(o).forEach(l),h=n(e),c(j.$$.fragment,e),f=n(e),c($.$$.fragment,e),G=n(e),Z=p(e,"P",{"data-svelte-h":!0}),d(Z)!=="svelte-ax3rwx"&&(Z.innerHTML=D),W=n(e),C=p(e,"P",{"data-svelte-h":!0}),d(C)!=="svelte-h4deay"&&(C.textContent=K),I=n(e),c(_.$$.fragment,e),v=n(e),c(B.$$.fragment,e),m=n(e),J=p(e,"P",{"data-svelte-h":!0}),d(J)!=="svelte-yncm6r"&&(J.innerHTML=rt),dt=n(e),O=p(e,"P",{"data-svelte-h":!0}),d(O)!=="svelte-1ngpipq"&&(O.innerHTML=xl),mt=n(e),c(ee.$$.fragment,e),ft=n(e),L=p(e,"DIV",{class:!0,"data-svelte-h":!0}),d(L)!=="svelte-1qqjocv"&&(L.innerHTML=Ll),ht=n(e),c(te.$$.fragment,e),ut=n(e),le=p(e,"P",{"data-svelte-h":!0}),d(le)!=="svelte-z0bjzf"&&(le.innerHTML=Rl),ct=n(e),ae=p(e,"P",{"data-svelte-h":!0}),d(ae)!=="svelte-1bhtcph"&&(ae.textContent=kl),Mt=n(e),c(se.$$.fragment,e),yt=n(e),ie=p(e,"P",{"data-svelte-h":!0}),d(ie)!=="svelte-falo8x"&&(ie.innerHTML=Hl),gt=n(e),c(ne.$$.fragment,e),bt=n(e),R=p(e,"DIV",{class:!0,"data-svelte-h":!0}),d(R)!=="svelte-vwb4li"&&(R.innerHTML=Al),wt=n(e),oe=p(e,"P",{"data-svelte-h":!0}),d(oe)!=="svelte-1qesb3i"&&(oe.innerHTML=Xl),jt=n(e),re=p(e,"UL",{"data-svelte-h":!0}),d(re)!=="svelte-1v8nqu6"&&(re.innerHTML=Vl),Tt=n(e),pe=p(e,"P",{"data-svelte-h":!0}),d(pe)!=="svelte-1eujnzn"&&(pe.innerHTML=Fl),Jt=n(e),c(de.$$.fragment,e),Ut=n(e),me=p(e,"P",{"data-svelte-h":!0}),d(me)!=="svelte-5s5bn9"&&(me.innerHTML=Ql),$t=n(e),c(fe.$$.fragment,e),Zt=n(e),k=p(e,"DIV",{class:!0,"data-svelte-h":!0}),d(k)!=="svelte-j6euo"&&(k.innerHTML=ql),_t=n(e),c(he.$$.fragment,e),vt=n(e),ue=p(e,"P",{"data-svelte-h":!0}),d(ue)!=="svelte-10sp1c2"&&(ue.innerHTML=Sl),Gt=n(e),c(H.$$.fragment,e),Wt=n(e),ce=p(e,"P",{"data-svelte-h":!0}),d(ce)!=="svelte-l2hd9l"&&(ce.textContent=Yl),It=n(e),c(Me.$$.fragment,e),Bt=n(e),ye=p(e,"P",{"data-svelte-h":!0}),d(ye)!=="svelte-pgxof3"&&(ye.innerHTML=Nl),Ct=n(e),c(ge.$$.fragment,e),xt=n(e),A=p(e,"DIV",{class:!0,"data-svelte-h":!0}),d(A)!=="svelte-848z5s"&&(A.innerHTML=zl),Lt=n(e),be=p(e,"P",{"data-svelte-h":!0}),d(be)!=="svelte-1c53yqo"&&(be.innerHTML=El),Rt=n(e),we=p(e,"UL",{"data-svelte-h":!0}),d(we)!=="svelte-ddcy8o"&&(we.innerHTML=Pl),kt=n(e),je=p(e,"P",{"data-svelte-h":!0}),d(je)!=="svelte-1izorwr"&&(je.innerHTML=Dl),Ht=n(e),Te=p(e,"P",{"data-svelte-h":!0}),d(Te)!=="svelte-1f523mj"&&(Te.innerHTML=Kl),At=n(e),c(Je.$$.fragment,e),Xt=n(e),X=p(e,"DIV",{class:!0,"data-svelte-h":!0}),d(X)!=="svelte-1etahws"&&(X.innerHTML=Ol),Vt=n(e),Ue=p(e,"P",{"data-svelte-h":!0}),d(Ue)!=="svelte-hclwab"&&(Ue.innerHTML=ea),Ft=n(e),$e=p(e,"P",{"data-svelte-h":!0}),d($e)!=="svelte-hfpfs1"&&($e.innerHTML=ta),Qt=n(e),c(Ze.$$.fragment,e),qt=n(e),c(_e.$$.fragment,e),St=n(e),ve=p(e,"P",{"data-svelte-h":!0}),d(ve)!=="svelte-j75uuy"&&(ve.innerHTML=la),Yt=n(e),Ge=p(e,"P",{"data-svelte-h":!0}),d(Ge)!=="svelte-u1an6c"&&(Ge.innerHTML=aa),Nt=n(e),c(We.$$.fragment,e),zt=n(e),Ie=p(e,"P",{"data-svelte-h":!0}),d(Ie)!=="svelte-y7bfbz"&&(Ie.innerHTML=sa),Et=n(e),c(V.$$.fragment,e),Pt=n(e),c(Be.$$.fragment,e),Dt=n(e),Ce=p(e,"P",{"data-svelte-h":!0}),d(Ce)!=="svelte-apq3if"&&(Ce.innerHTML=ia),Kt=n(e),xe=p(e,"P",{"data-svelte-h":!0}),d(xe)!=="svelte-2pa1nu"&&(xe.textContent=na),Ot=n(e),Le=p(e,"P",{"data-svelte-h":!0}),d(Le)!=="svelte-4xpzmb"&&(Le.innerHTML=oa),el=n(e),c(Re.$$.fragment,e),tl=n(e),c(F.$$.fragment,e),ll=n(e),ke=p(e,"P",{"data-svelte-h":!0}),d(ke)!=="svelte-y0hv00"&&(ke.innerHTML=ra),al=n(e),c(He.$$.fragment,e),sl=n(e),Ae=p(e,"P",{"data-svelte-h":!0}),d(Ae)!=="svelte-1fob61h"&&(Ae.innerHTML=pa),il=n(e),Xe=p(e,"P",{"data-svelte-h":!0}),d(Xe)!=="svelte-pxw33x"&&(Xe.innerHTML=da),nl=n(e),c(Q.$$.fragment,e),ol=n(e),c(Ve.$$.fragment,e),rl=n(e),Fe=p(e,"P",{"data-svelte-h":!0}),d(Fe)!=="svelte-6zj1or"&&(Fe.innerHTML=ma),pl=n(e),c(q.$$.fragment,e),dl=n(e),c(Qe.$$.fragment,e),ml=n(e),qe=p(e,"P",{"data-svelte-h":!0}),d(qe)!=="svelte-1pu4ft1"&&(qe.innerHTML=fa),fl=n(e),Se=p(e,"P",{"data-svelte-h":!0}),d(Se)!=="svelte-18j3qom"&&(Se.innerHTML=ha),hl=n(e),c(S.$$.fragment,e),ul=n(e),Ye=p(e,"P",{"data-svelte-h":!0}),d(Ye)!=="svelte-w9hhyu"&&(Ye.textContent=ua),cl=n(e),c(Ne.$$.fragment,e),Ml=n(e),ze=p(e,"P",{"data-svelte-h":!0}),d(ze)!=="svelte-9a8ooa"&&(ze.innerHTML=ca),yl=n(e),c(Ee.$$.fragment,e),gl=n(e),Pe=p(e,"P",{"data-svelte-h":!0}),d(Pe)!=="svelte-7abk5j"&&(Pe.textContent=Ma),bl=n(e),c(De.$$.fragment,e),wl=n(e),Y=p(e,"DIV",{class:!0,"data-svelte-h":!0}),d(Y)!=="svelte-1vum0wo"&&(Y.innerHTML=ya),jl=n(e),c(Ke.$$.fragment,e),Tl=n(e),Oe=p(e,"P",{"data-svelte-h":!0}),d(Oe)!=="svelte-7lvggd"&&(Oe.innerHTML=ga),Jl=n(e),et=p(e,"P",{"data-svelte-h":!0}),d(et)!=="svelte-zfljo5"&&(et.innerHTML=ba),Ul=n(e),c(tt.$$.fragment,e),$l=n(e),c(lt.$$.fragment,e),Zl=n(e),at=p(e,"P",{"data-svelte-h":!0}),d(at)!=="svelte-1ah3evs"&&(at.innerHTML=wa),_l=n(e),c(N.$$.fragment,e),vl=n(e),c(st.$$.fragment,e),Gl=n(e),it=p(e,"P",{"data-svelte-h":!0}),d(it)!=="svelte-3ktslb"&&(it.innerHTML=ja),Wl=n(e),c(nt.$$.fragment,e),Il=n(e),c(ot.$$.fragment,e),Bl=n(e),pt=p(e,"P",{}),Wa(pt).forEach(l),this.h()},h(){z(s,"name","hf:doc:metadata"),z(s,"content",ts),z(L,"class","flex justify-center"),z(R,"class","flex justify-center"),z(k,"class","flex justify-center"),z(A,"class","flex justify-center"),z(X,"class","flex justify-center"),z(Y,"class","flex justify-center")},m(e,t){ka(document.head,s),a(e,w,t),a(e,o,t),a(e,h,t),M(j,e,t),a(e,f,t),M($,e,t),a(e,G,t),a(e,Z,t),a(e,W,t),a(e,C,t),a(e,I,t),M(_,e,t),a(e,v,t),M(B,e,t),a(e,m,t),a(e,J,t),a(e,dt,t),a(e,O,t),a(e,mt,t),M(ee,e,t),a(e,ft,t),a(e,L,t),a(e,ht,t),M(te,e,t),a(e,ut,t),a(e,le,t),a(e,ct,t),a(e,ae,t),a(e,Mt,t),M(se,e,t),a(e,yt,t),a(e,ie,t),a(e,gt,t),M(ne,e,t),a(e,bt,t),a(e,R,t),a(e,wt,t),a(e,oe,t),a(e,jt,t),a(e,re,t),a(e,Tt,t),a(e,pe,t),a(e,Jt,t),M(de,e,t),a(e,Ut,t),a(e,me,t),a(e,$t,t),M(fe,e,t),a(e,Zt,t),a(e,k,t),a(e,_t,t),M(he,e,t),a(e,vt,t),a(e,ue,t),a(e,Gt,t),M(H,e,t),a(e,Wt,t),a(e,ce,t),a(e,It,t),M(Me,e,t),a(e,Bt,t),a(e,ye,t),a(e,Ct,t),M(ge,e,t),a(e,xt,t),a(e,A,t),a(e,Lt,t),a(e,be,t),a(e,Rt,t),a(e,we,t),a(e,kt,t),a(e,je,t),a(e,Ht,t),a(e,Te,t),a(e,At,t),M(Je,e,t),a(e,Xt,t),a(e,X,t),a(e,Vt,t),a(e,Ue,t),a(e,Ft,t),a(e,$e,t),a(e,Qt,t),M(Ze,e,t),a(e,qt,t),M(_e,e,t),a(e,St,t),a(e,ve,t),a(e,Yt,t),a(e,Ge,t),a(e,Nt,t),M(We,e,t),a(e,zt,t),a(e,Ie,t),a(e,Et,t),M(V,e,t),a(e,Pt,t),M(Be,e,t),a(e,Dt,t),a(e,Ce,t),a(e,Kt,t),a(e,xe,t),a(e,Ot,t),a(e,Le,t),a(e,el,t),M(Re,e,t),a(e,tl,t),M(F,e,t),a(e,ll,t),a(e,ke,t),a(e,al,t),M(He,e,t),a(e,sl,t),a(e,Ae,t),a(e,il,t),a(e,Xe,t),a(e,nl,t),M(Q,e,t),a(e,ol,t),M(Ve,e,t),a(e,rl,t),a(e,Fe,t),a(e,pl,t),M(q,e,t),a(e,dl,t),M(Qe,e,t),a(e,ml,t),a(e,qe,t),a(e,fl,t),a(e,Se,t),a(e,hl,t),M(S,e,t),a(e,ul,t),a(e,Ye,t),a(e,cl,t),M(Ne,e,t),a(e,Ml,t),a(e,ze,t),a(e,yl,t),M(Ee,e,t),a(e,gl,t),a(e,Pe,t),a(e,bl,t),M(De,e,t),a(e,wl,t),a(e,Y,t),a(e,jl,t),M(Ke,e,t),a(e,Tl,t),a(e,Oe,t),a(e,Jl,t),a(e,et,t),a(e,Ul,t),M(tt,e,t),a(e,$l,t),M(lt,e,t),a(e,Zl,t),a(e,at,t),a(e,_l,t),M(N,e,t),a(e,vl,t),M(st,e,t),a(e,Gl,t),a(e,it,t),a(e,Wl,t),M(nt,e,t),a(e,Il,t),M(ot,e,t),a(e,Bl,t),a(e,pt,t),Cl=!0},p(e,[t]){const Ta={};t&2&&(Ta.$$scope={dirty:t,ctx:e}),_.$set(Ta);const Ja={};t&2&&(Ja.$$scope={dirty:t,ctx:e}),H.$set(Ja);const Ua={};t&2&&(Ua.$$scope={dirty:t,ctx:e}),V.$set(Ua);const $a={};t&2&&($a.$$scope={dirty:t,ctx:e}),F.$set($a);const Za={};t&2&&(Za.$$scope={dirty:t,ctx:e}),Q.$set(Za);const _a={};t&2&&(_a.$$scope={dirty:t,ctx:e}),q.$set(_a);const va={};t&2&&(va.$$scope={dirty:t,ctx:e}),S.$set(va);const Ga={};t&2&&(Ga.$$scope={dirty:t,ctx:e}),N.$set(Ga)},i(e){Cl||(y(j.$$.fragment,e),y($.$$.fragment,e),y(_.$$.fragment,e),y(B.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(se.$$.fragment,e),y(ne.$$.fragment,e),y(de.$$.fragment,e),y(fe.$$.fragment,e),y(he.$$.fragment,e),y(H.$$.fragment,e),y(Me.$$.fragment,e),y(ge.$$.fragment,e),y(Je.$$.fragment,e),y(Ze.$$.fragment,e),y(_e.$$.fragment,e),y(We.$$.fragment,e),y(V.$$.fragment,e),y(Be.$$.fragment,e),y(Re.$$.fragment,e),y(F.$$.fragment,e),y(He.$$.fragment,e),y(Q.$$.fragment,e),y(Ve.$$.fragment,e),y(q.$$.fragment,e),y(Qe.$$.fragment,e),y(S.$$.fragment,e),y(Ne.$$.fragment,e),y(Ee.$$.fragment,e),y(De.$$.fragment,e),y(Ke.$$.fragment,e),y(tt.$$.fragment,e),y(lt.$$.fragment,e),y(N.$$.fragment,e),y(st.$$.fragment,e),y(nt.$$.fragment,e),y(ot.$$.fragment,e),Cl=!0)},o(e){g(j.$$.fragment,e),g($.$$.fragment,e),g(_.$$.fragment,e),g(B.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(se.$$.fragment,e),g(ne.$$.fragment,e),g(de.$$.fragment,e),g(fe.$$.fragment,e),g(he.$$.fragment,e),g(H.$$.fragment,e),g(Me.$$.fragment,e),g(ge.$$.fragment,e),g(Je.$$.fragment,e),g(Ze.$$.fragment,e),g(_e.$$.fragment,e),g(We.$$.fragment,e),g(V.$$.fragment,e),g(Be.$$.fragment,e),g(Re.$$.fragment,e),g(F.$$.fragment,e),g(He.$$.fragment,e),g(Q.$$.fragment,e),g(Ve.$$.fragment,e),g(q.$$.fragment,e),g(Qe.$$.fragment,e),g(S.$$.fragment,e),g(Ne.$$.fragment,e),g(Ee.$$.fragment,e),g(De.$$.fragment,e),g(Ke.$$.fragment,e),g(tt.$$.fragment,e),g(lt.$$.fragment,e),g(N.$$.fragment,e),g(st.$$.fragment,e),g(nt.$$.fragment,e),g(ot.$$.fragment,e),Cl=!1},d(e){e&&(l(w),l(o),l(h),l(f),l(G),l(Z),l(W),l(C),l(I),l(v),l(m),l(J),l(dt),l(O),l(mt),l(ft),l(L),l(ht),l(ut),l(le),l(ct),l(ae),l(Mt),l(yt),l(ie),l(gt),l(bt),l(R),l(wt),l(oe),l(jt),l(re),l(Tt),l(pe),l(Jt),l(Ut),l(me),l($t),l(Zt),l(k),l(_t),l(vt),l(ue),l(Gt),l(Wt),l(ce),l(It),l(Bt),l(ye),l(Ct),l(xt),l(A),l(Lt),l(be),l(Rt),l(we),l(kt),l(je),l(Ht),l(Te),l(At),l(Xt),l(X),l(Vt),l(Ue),l(Ft),l($e),l(Qt),l(qt),l(St),l(ve),l(Yt),l(Ge),l(Nt),l(zt),l(Ie),l(Et),l(Pt),l(Dt),l(Ce),l(Kt),l(xe),l(Ot),l(Le),l(el),l(tl),l(ll),l(ke),l(al),l(sl),l(Ae),l(il),l(Xe),l(nl),l(ol),l(rl),l(Fe),l(pl),l(dl),l(ml),l(qe),l(fl),l(Se),l(hl),l(ul),l(Ye),l(cl),l(Ml),l(ze),l(yl),l(gl),l(Pe),l(bl),l(wl),l(Y),l(jl),l(Tl),l(Oe),l(Jl),l(et),l(Ul),l($l),l(Zl),l(at),l(_l),l(vl),l(Gl),l(it),l(Wl),l(Il),l(Bl),l(pt)),l(s),b(j,e),b($,e),b(_,e),b(B,e),b(ee,e),b(te,e),b(se,e),b(ne,e),b(de,e),b(fe,e),b(he,e),b(H,e),b(Me,e),b(ge,e),b(Je,e),b(Ze,e),b(_e,e),b(We,e),b(V,e),b(Be,e),b(Re,e),b(F,e),b(He,e),b(Q,e),b(Ve,e),b(q,e),b(Qe,e),b(S,e),b(Ne,e),b(Ee,e),b(De,e),b(Ke,e),b(tt,e),b(lt,e),b(N,e),b(st,e),b(nt,e),b(ot,e)}}}const ts='{"title":"Load adapters","local":"load-adapters","sections":[{"title":"DreamBooth","local":"dreambooth","sections":[],"depth":2},{"title":"Textual inversion","local":"textual-inversion","sections":[],"depth":2},{"title":"LoRA","local":"lora","sections":[{"title":"Adjust LoRA weight scale","local":"adjust-lora-weight-scale","sections":[],"depth":3},{"title":"Hotswapping LoRA adapters","local":"hotswapping-lora-adapters","sections":[],"depth":3},{"title":"Kohya and TheLastBen","local":"kohya-and-thelastben","sections":[],"depth":3}],"depth":2},{"title":"IP-Adapter","local":"ip-adapter","sections":[{"title":"IP-Adapter Plus","local":"ip-adapter-plus","sections":[],"depth":3},{"title":"IP-Adapter Face ID models","local":"ip-adapter-face-id-models","sections":[],"depth":3}],"depth":2}],"depth":1}';function ls(T){return Ca(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ds extends xa{constructor(s){super(),La(this,s,ls,es,Ba,{})}}export{ds as component};
