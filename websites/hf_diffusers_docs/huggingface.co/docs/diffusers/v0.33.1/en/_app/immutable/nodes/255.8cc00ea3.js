import{s as ct,o as ht,n as Ee}from"../chunks/scheduler.8c3d61f6.js";import{S as Mt,i as yt,g as o,s,r,A as bt,h as p,f as l,c as n,j as rt,u,x as f,k as ut,y as vt,a as i,v as d,d as c,t as h,w as M}from"../chunks/index.da70eac4.js";import{T as Pe}from"../chunks/Tip.1d9b8c37.js";import{C as T}from"../chunks/CodeBlock.a9c4becf.js";import{H as dt,E as wt}from"../chunks/index.5d4ab994.js";function _t(v){let a,y='Refer to the <a href="https://huggingface.co/docs/accelerate/main/en/concept_guides/big_model_inference" rel="nofollow">Handling big models for inference</a> guide for general guidance when working with big models that are hard to fit into memory.';return{c(){a=o("p"),a.innerHTML=y},l(m){a=p(m,"P",{"data-svelte-h":!0}),f(a)!=="svelte-6g79u0"&&(a.innerHTML=y)},m(m,b){i(m,a,b)},p:Ee,d(m){m&&l(a)}}}function Ut(v){let a,y="This feature is experimental and its APIs might change in the future.";return{c(){a=o("p"),a.textContent=y},l(m){a=p(m,"P",{"data-svelte-h":!0}),f(a)!=="svelte-jtjc2m"&&(a.textContent=y)},m(m,b){i(m,a,b)},p:Ee,d(m){m&&l(a)}}}function Tt(v){let a,y="Only the “balanced” strategy is supported at the moment, and we plan to support additional mapping strategies in the future.";return{c(){a=o("p"),a.textContent=y},l(m){a=p(m,"P",{"data-svelte-h":!0}),f(a)!=="svelte-qgg1n4"&&(a.textContent=y)},m(m,b){i(m,a,b)},p:Ee,d(m){m&&l(a)}}}function $t(v){let a,y,m,b,$,ie,J,Ve='A modern diffusion model, like <a href="../using-diffusers/sdxl">Stable Diffusion XL (SDXL)</a>, is not just a single model, but a collection of multiple models. SDXL has four different model-level components:',se,g,Xe="<li>A variational autoencoder (VAE)</li> <li>Two text encoders</li> <li>A UNet for denoising</li>",ne,Z,qe="Usually, the text encoders and the denoiser are much larger compared to the VAE.",ae,C,Ne="As models get bigger and better, it’s possible your model is so big that even a single copy won’t fit in memory. But that doesn’t mean it can’t be loaded. If you have more than one GPU, there is more memory available to store your model. In this case, it’s better to split your model checkpoint into several smaller <em>checkpoint shards</em>.",oe,x,Se='When a text encoder checkpoint has multiple shards, like <a href="https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/tree/main/text_encoder_3" rel="nofollow">T5-xxl for SD3</a>, it is automatically handled by the <a href="https://huggingface.co/docs/transformers/index" rel="nofollow">Transformers</a> library as it is a required dependency of Diffusers when using the <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/stable_diffusion_3#diffusers.StableDiffusion3Pipeline">StableDiffusion3Pipeline</a>. More specifically, Transformers will automatically handle the loading of multiple shards within the requested model class and get it ready so that inference can be performed.',pe,G,De='The denoiser checkpoint can also have multiple shards and supports inference thanks to the <a href="https://huggingface.co/docs/accelerate/index" rel="nofollow">Accelerate</a> library.',fe,w,me,j,Ye='For example, let’s save a sharded checkpoint for the <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main/unet" rel="nofollow">SDXL UNet</a>:',re,W,ue,k,Qe='The size of the fp32 variant of the SDXL UNet checkpoint is ~10.4GB. Set the <code>max_shard_size</code> parameter to 5GB to create 3 shards. After saving, you can load them in <a href="/docs/diffusers/v0.33.1/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline">StableDiffusionXLPipeline</a>:',de,B,ce,L,ze='If placing all the model-level components on the GPU at once is not feasible, use <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload">enable_model_cpu_offload()</a> to help you:',he,R,Me,I,Ae="In general, we recommend sharding when a checkpoint is more than 5GB (in fp32).",ye,H,be,P,Fe="On distributed setups, you can run inference across multiple GPUs with Accelerate.",ve,_,we,E,Ke="With Accelerate, you can use the <code>device_map</code> to determine how to distribute the models of a pipeline across multiple devices. This is useful in situations where you have more than one GPU.",_e,V,Oe='For example, if you have two 8GB GPUs, then using <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload">enable_model_cpu_offload()</a> may not work so well because:',Ue,X,et='<li>it only works on a single GPU</li> <li>a single model might not fit on a single GPU (<a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload">enable_sequential_cpu_offload()</a> might work but it will be extremely slow and it is also limited to a single GPU)</li>',Te,q,tt="To make use of both GPUs, you can use the “balanced” device placement strategy which splits the models across all available GPUs.",$e,U,Je,N,ge,S,lt="You can also pass a dictionary to enforce the maximum GPU memory that can be used on each device:",Ze,D,Ce,Y,it="If a device is not present in <code>max_memory</code>, then it will be completely ignored and will not participate in the device placement.",xe,Q,st='By default, Diffusers uses the maximum memory of all devices. If the models don’t fit on the GPUs, they are offloaded to the CPU. If the CPU doesn’t have enough memory, then you might see an error. In that case, you could defer to using <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload">enable_sequential_cpu_offload()</a> and <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload">enable_model_cpu_offload()</a>.',Ge,z,nt='Call <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.reset_device_map">reset_device_map()</a> to reset the <code>device_map</code> of a pipeline. This is also necessary if you want to use methods like <code>to()</code>, <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload">enable_sequential_cpu_offload()</a>, and <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload">enable_model_cpu_offload()</a> on a pipeline that was device-mapped.',je,A,We,F,at="Once a pipeline has been device-mapped, you can also access its device map via <code>hf_device_map</code>:",ke,K,Be,O,ot="An example device map would look like so:",Le,ee,Re,te,Ie,le,He;return $=new dt({props:{title:"Working with big models",local:"working-with-big-models",headingTag:"h1"}}),w=new Pe({props:{warning:!1,$$slots:{default:[_t]},$$scope:{ctx:v}}}),W=new T({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFVOZXQyRENvbmRpdGlvbk1vZGVsJTBBJTBBdW5ldCUyMCUzRCUyMFVOZXQyRENvbmRpdGlvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtYmFzZS0xLjAlMjIlMkMlMjBzdWJmb2xkZXIlM0QlMjJ1bmV0JTIyJTBBKSUwQXVuZXQuc2F2ZV9wcmV0cmFpbmVkKCUyMnNkeGwtdW5ldC1zaGFyZGVkJTIyJTJDJTIwbWF4X3NoYXJkX3NpemUlM0QlMjI1R0IlMjIp",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> UNet2DConditionModel

unet = UNet2DConditionModel.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, subfolder=<span class="hljs-string">&quot;unet&quot;</span>
)
unet.save_pretrained(<span class="hljs-string">&quot;sdxl-unet-sharded&quot;</span>, max_shard_size=<span class="hljs-string">&quot;5GB&quot;</span>)`,wrap:!1}}),B=new T({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFVOZXQyRENvbmRpdGlvbk1vZGVsJTJDJTIwU3RhYmxlRGlmZnVzaW9uWExQaXBlbGluZSUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdW5ldCUyMCUzRCUyMFVOZXQyRENvbmRpdGlvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzYXlha3BhdWwlMkZzZHhsLXVuZXQtc2hhcmRlZCUyMiUyQyUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUwQSklMEFwaXBlbGluZSUyMCUzRCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi14bC1iYXNlLTEuMCUyMiUyQyUyMHVuZXQlM0R1bmV0JTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTBBKS50byglMjJjdWRhJTIyKSUwQSUwQWltYWdlJTIwJTNEJTIwcGlwZWxpbmUoJTIyYSUyMGN1dGUlMjBkb2clMjBydW5uaW5nJTIwb24lMjB0aGUlMjBncmFzcyUyMiUyQyUyMG51bV9pbmZlcmVuY2Vfc3RlcHMlM0QzMCkuaW1hZ2VzJTVCMCU1RCUwQWltYWdlLnNhdmUoJTIyZG9nLnBuZyUyMik=",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> UNet2DConditionModel, StableDiffusionXLPipeline
<span class="hljs-keyword">import</span> torch

unet = UNet2DConditionModel.from_pretrained(
    <span class="hljs-string">&quot;sayakpaul/sdxl-unet-sharded&quot;</span>, torch_dtype=torch.float16
)
pipeline = StableDiffusionXLPipeline.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, unet=unet, torch_dtype=torch.float16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

image = pipeline(<span class="hljs-string">&quot;a cute dog running on the grass&quot;</span>, num_inference_steps=<span class="hljs-number">30</span>).images[<span class="hljs-number">0</span>]
image.save(<span class="hljs-string">&quot;dog.png&quot;</span>)`,wrap:!1}}),R=new T({props:{code:"LSUyMHBpcGVsaW5lLnRvKCUyMmN1ZGElMjIpJTBBJTJCJTIwcGlwZWxpbmUuZW5hYmxlX21vZGVsX2NwdV9vZmZsb2FkKCk=",highlighted:`<span class="hljs-deletion">- pipeline.to(&quot;cuda&quot;)</span>
<span class="hljs-addition">+ pipeline.enable_model_cpu_offload()</span>`,wrap:!1}}),H=new dt({props:{title:"Device placement",local:"device-placement",headingTag:"h2"}}),_=new Pe({props:{warning:!0,$$slots:{default:[Ut]},$$scope:{ctx:v}}}),U=new Pe({props:{warning:!0,$$slots:{default:[Tt]},$$scope:{ctx:v}}}),N=new T({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMERpZmZ1c2lvblBpcGVsaW5lJTBBaW1wb3J0JTIwdG9yY2glMEElMEFwaXBlbGluZSUyMCUzRCUyMERpZmZ1c2lvblBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMEEtJTIwJTIwJTIwJTIwJTIyc3RhYmxlLWRpZmZ1c2lvbi12MS01JTJGc3RhYmxlLWRpZmZ1c2lvbi12MS01JTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwdXNlX3NhZmV0ZW5zb3JzJTNEVHJ1ZSUyQyUwQSUyQiUyMCUyMCUyMCUyMCUyMnN0YWJsZS1kaWZmdXNpb24tdjEtNSUyRnN0YWJsZS1kaWZmdXNpb24tdjEtNSUyMiUyQyUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUyMHVzZV9zYWZldGVuc29ycyUzRFRydWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYmFsYW5jZWQlMjIlMEEpJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZSglMjJhJTIwZG9nJTIyKS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2U=",highlighted:`from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(
<span class="hljs-deletion">-    &quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;, torch_dtype=torch.float16, use_safetensors=True,</span>
<span class="hljs-addition">+    &quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;, torch_dtype=torch.float16, use_safetensors=True, device_map=&quot;balanced&quot;</span>
)
image = pipeline(&quot;a dog&quot;).images[0]
image`,wrap:!1}}),D=new T({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMERpZmZ1c2lvblBpcGVsaW5lJTBBaW1wb3J0JTIwdG9yY2glMEElMEFtYXhfbWVtb3J5JTIwJTNEJTIwJTdCMCUzQSUyMjFHQiUyMiUyQyUyMDElM0ElMjIxR0IlMjIlN0QlMEFwaXBlbGluZSUyMCUzRCUyMERpZmZ1c2lvblBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFibGUtZGlmZnVzaW9uLXYxLTUlMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUlMjIlMkMlMEElMjAlMjAlMjAlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjB1c2Vfc2FmZXRlbnNvcnMlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmJhbGFuY2VkJTIyJTJDJTBBJTJCJTIwJTIwJTIwbWF4X21lbW9yeSUzRG1heF9tZW1vcnklMEEpJTBBaW1hZ2UlMjAlM0QlMjBwaXBlbGluZSglMjJhJTIwZG9nJTIyKS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2U=",highlighted:`from diffusers import DiffusionPipeline
import torch

max_memory = {0:&quot;1GB&quot;, 1:&quot;1GB&quot;}
pipeline = DiffusionPipeline.from_pretrained(
    &quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;,
    torch_dtype=torch.float16,
    use_safetensors=True,
    device_map=&quot;balanced&quot;,
<span class="hljs-addition">+   max_memory=max_memory</span>
)
image = pipeline(&quot;a dog&quot;).images[0]
image`,wrap:!1}}),A=new T({props:{code:"cGlwZWxpbmUucmVzZXRfZGV2aWNlX21hcCgp",highlighted:"pipeline.reset_device_map()",wrap:!1}}),K=new T({props:{code:"cHJpbnQocGlwZWxpbmUuaGZfZGV2aWNlX21hcCk=",highlighted:'<span class="hljs-built_in">print</span>(pipeline.hf_device_map)',wrap:!1}}),ee=new T({props:{code:"JTdCJ3VuZXQnJTNBJTIwMSUyQyUyMCd2YWUnJTNBJTIwMSUyQyUyMCdzYWZldHlfY2hlY2tlciclM0ElMjAwJTJDJTIwJ3RleHRfZW5jb2RlciclM0ElMjAwJTdE",highlighted:'{<span class="hljs-string">&#x27;unet&#x27;</span>: 1, <span class="hljs-string">&#x27;vae&#x27;</span>: 1, <span class="hljs-string">&#x27;safety_checker&#x27;</span>: 0, <span class="hljs-string">&#x27;text_encoder&#x27;</span>: 0}',wrap:!1}}),te=new wt({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/inference_with_big_models.md"}}),{c(){a=o("meta"),y=s(),m=o("p"),b=s(),r($.$$.fragment),ie=s(),J=o("p"),J.innerHTML=Ve,se=s(),g=o("ul"),g.innerHTML=Xe,ne=s(),Z=o("p"),Z.textContent=qe,ae=s(),C=o("p"),C.innerHTML=Ne,oe=s(),x=o("p"),x.innerHTML=Se,pe=s(),G=o("p"),G.innerHTML=De,fe=s(),r(w.$$.fragment),me=s(),j=o("p"),j.innerHTML=Ye,re=s(),r(W.$$.fragment),ue=s(),k=o("p"),k.innerHTML=Qe,de=s(),r(B.$$.fragment),ce=s(),L=o("p"),L.innerHTML=ze,he=s(),r(R.$$.fragment),Me=s(),I=o("p"),I.textContent=Ae,ye=s(),r(H.$$.fragment),be=s(),P=o("p"),P.textContent=Fe,ve=s(),r(_.$$.fragment),we=s(),E=o("p"),E.innerHTML=Ke,_e=s(),V=o("p"),V.innerHTML=Oe,Ue=s(),X=o("ul"),X.innerHTML=et,Te=s(),q=o("p"),q.textContent=tt,$e=s(),r(U.$$.fragment),Je=s(),r(N.$$.fragment),ge=s(),S=o("p"),S.textContent=lt,Ze=s(),r(D.$$.fragment),Ce=s(),Y=o("p"),Y.innerHTML=it,xe=s(),Q=o("p"),Q.innerHTML=st,Ge=s(),z=o("p"),z.innerHTML=nt,je=s(),r(A.$$.fragment),We=s(),F=o("p"),F.innerHTML=at,ke=s(),r(K.$$.fragment),Be=s(),O=o("p"),O.textContent=ot,Le=s(),r(ee.$$.fragment),Re=s(),r(te.$$.fragment),Ie=s(),le=o("p"),this.h()},l(e){const t=bt("svelte-u9bgzb",document.head);a=p(t,"META",{name:!0,content:!0}),t.forEach(l),y=n(e),m=p(e,"P",{}),rt(m).forEach(l),b=n(e),u($.$$.fragment,e),ie=n(e),J=p(e,"P",{"data-svelte-h":!0}),f(J)!=="svelte-i5ki63"&&(J.innerHTML=Ve),se=n(e),g=p(e,"UL",{"data-svelte-h":!0}),f(g)!=="svelte-di8t2m"&&(g.innerHTML=Xe),ne=n(e),Z=p(e,"P",{"data-svelte-h":!0}),f(Z)!=="svelte-141shqw"&&(Z.textContent=qe),ae=n(e),C=p(e,"P",{"data-svelte-h":!0}),f(C)!=="svelte-n27w9x"&&(C.innerHTML=Ne),oe=n(e),x=p(e,"P",{"data-svelte-h":!0}),f(x)!=="svelte-17qfdzf"&&(x.innerHTML=Se),pe=n(e),G=p(e,"P",{"data-svelte-h":!0}),f(G)!=="svelte-igij6y"&&(G.innerHTML=De),fe=n(e),u(w.$$.fragment,e),me=n(e),j=p(e,"P",{"data-svelte-h":!0}),f(j)!=="svelte-bu4e6r"&&(j.innerHTML=Ye),re=n(e),u(W.$$.fragment,e),ue=n(e),k=p(e,"P",{"data-svelte-h":!0}),f(k)!=="svelte-1xxatoo"&&(k.innerHTML=Qe),de=n(e),u(B.$$.fragment,e),ce=n(e),L=p(e,"P",{"data-svelte-h":!0}),f(L)!=="svelte-e9sr1"&&(L.innerHTML=ze),he=n(e),u(R.$$.fragment,e),Me=n(e),I=p(e,"P",{"data-svelte-h":!0}),f(I)!=="svelte-110kwsj"&&(I.textContent=Ae),ye=n(e),u(H.$$.fragment,e),be=n(e),P=p(e,"P",{"data-svelte-h":!0}),f(P)!=="svelte-17ssrfq"&&(P.textContent=Fe),ve=n(e),u(_.$$.fragment,e),we=n(e),E=p(e,"P",{"data-svelte-h":!0}),f(E)!=="svelte-1y4i1fz"&&(E.innerHTML=Ke),_e=n(e),V=p(e,"P",{"data-svelte-h":!0}),f(V)!=="svelte-149dkvh"&&(V.innerHTML=Oe),Ue=n(e),X=p(e,"UL",{"data-svelte-h":!0}),f(X)!=="svelte-1ifotj5"&&(X.innerHTML=et),Te=n(e),q=p(e,"P",{"data-svelte-h":!0}),f(q)!=="svelte-awezto"&&(q.textContent=tt),$e=n(e),u(U.$$.fragment,e),Je=n(e),u(N.$$.fragment,e),ge=n(e),S=p(e,"P",{"data-svelte-h":!0}),f(S)!=="svelte-1y8b6l8"&&(S.textContent=lt),Ze=n(e),u(D.$$.fragment,e),Ce=n(e),Y=p(e,"P",{"data-svelte-h":!0}),f(Y)!=="svelte-fmsbbs"&&(Y.innerHTML=it),xe=n(e),Q=p(e,"P",{"data-svelte-h":!0}),f(Q)!=="svelte-1mlas4u"&&(Q.innerHTML=st),Ge=n(e),z=p(e,"P",{"data-svelte-h":!0}),f(z)!=="svelte-mlqepy"&&(z.innerHTML=nt),je=n(e),u(A.$$.fragment,e),We=n(e),F=p(e,"P",{"data-svelte-h":!0}),f(F)!=="svelte-17pbny5"&&(F.innerHTML=at),ke=n(e),u(K.$$.fragment,e),Be=n(e),O=p(e,"P",{"data-svelte-h":!0}),f(O)!=="svelte-18o6tiw"&&(O.textContent=ot),Le=n(e),u(ee.$$.fragment,e),Re=n(e),u(te.$$.fragment,e),Ie=n(e),le=p(e,"P",{}),rt(le).forEach(l),this.h()},h(){ut(a,"name","hf:doc:metadata"),ut(a,"content",Jt)},m(e,t){vt(document.head,a),i(e,y,t),i(e,m,t),i(e,b,t),d($,e,t),i(e,ie,t),i(e,J,t),i(e,se,t),i(e,g,t),i(e,ne,t),i(e,Z,t),i(e,ae,t),i(e,C,t),i(e,oe,t),i(e,x,t),i(e,pe,t),i(e,G,t),i(e,fe,t),d(w,e,t),i(e,me,t),i(e,j,t),i(e,re,t),d(W,e,t),i(e,ue,t),i(e,k,t),i(e,de,t),d(B,e,t),i(e,ce,t),i(e,L,t),i(e,he,t),d(R,e,t),i(e,Me,t),i(e,I,t),i(e,ye,t),d(H,e,t),i(e,be,t),i(e,P,t),i(e,ve,t),d(_,e,t),i(e,we,t),i(e,E,t),i(e,_e,t),i(e,V,t),i(e,Ue,t),i(e,X,t),i(e,Te,t),i(e,q,t),i(e,$e,t),d(U,e,t),i(e,Je,t),d(N,e,t),i(e,ge,t),i(e,S,t),i(e,Ze,t),d(D,e,t),i(e,Ce,t),i(e,Y,t),i(e,xe,t),i(e,Q,t),i(e,Ge,t),i(e,z,t),i(e,je,t),d(A,e,t),i(e,We,t),i(e,F,t),i(e,ke,t),d(K,e,t),i(e,Be,t),i(e,O,t),i(e,Le,t),d(ee,e,t),i(e,Re,t),d(te,e,t),i(e,Ie,t),i(e,le,t),He=!0},p(e,[t]){const pt={};t&2&&(pt.$$scope={dirty:t,ctx:e}),w.$set(pt);const ft={};t&2&&(ft.$$scope={dirty:t,ctx:e}),_.$set(ft);const mt={};t&2&&(mt.$$scope={dirty:t,ctx:e}),U.$set(mt)},i(e){He||(c($.$$.fragment,e),c(w.$$.fragment,e),c(W.$$.fragment,e),c(B.$$.fragment,e),c(R.$$.fragment,e),c(H.$$.fragment,e),c(_.$$.fragment,e),c(U.$$.fragment,e),c(N.$$.fragment,e),c(D.$$.fragment,e),c(A.$$.fragment,e),c(K.$$.fragment,e),c(ee.$$.fragment,e),c(te.$$.fragment,e),He=!0)},o(e){h($.$$.fragment,e),h(w.$$.fragment,e),h(W.$$.fragment,e),h(B.$$.fragment,e),h(R.$$.fragment,e),h(H.$$.fragment,e),h(_.$$.fragment,e),h(U.$$.fragment,e),h(N.$$.fragment,e),h(D.$$.fragment,e),h(A.$$.fragment,e),h(K.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),He=!1},d(e){e&&(l(y),l(m),l(b),l(ie),l(J),l(se),l(g),l(ne),l(Z),l(ae),l(C),l(oe),l(x),l(pe),l(G),l(fe),l(me),l(j),l(re),l(ue),l(k),l(de),l(ce),l(L),l(he),l(Me),l(I),l(ye),l(be),l(P),l(ve),l(we),l(E),l(_e),l(V),l(Ue),l(X),l(Te),l(q),l($e),l(Je),l(ge),l(S),l(Ze),l(Ce),l(Y),l(xe),l(Q),l(Ge),l(z),l(je),l(We),l(F),l(ke),l(Be),l(O),l(Le),l(Re),l(Ie),l(le)),l(a),M($,e),M(w,e),M(W,e),M(B,e),M(R,e),M(H,e),M(_,e),M(U,e),M(N,e),M(D,e),M(A,e),M(K,e),M(ee,e),M(te,e)}}}const Jt='{"title":"Working with big models","local":"working-with-big-models","sections":[{"title":"Device placement","local":"device-placement","sections":[],"depth":2}],"depth":1}';function gt(v){return ht(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Wt extends Mt{constructor(a){super(),yt(this,a,gt,$t,ct,{})}}export{Wt as component};
