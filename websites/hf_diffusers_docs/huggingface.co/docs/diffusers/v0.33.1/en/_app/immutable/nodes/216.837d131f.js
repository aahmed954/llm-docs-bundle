import{s as Te,o as ve,n as ge}from"../chunks/scheduler.8c3d61f6.js";import{S as Je,i as Ze,g as p,s as i,r as c,A as Ge,h as r,f as s,c as a,j as we,u,x as m,k as Ue,y as Be,a as l,v as y,d as M,t as h,w as d}from"../chunks/index.da70eac4.js";import{T as $e}from"../chunks/Tip.1d9b8c37.js";import{C as oe}from"../chunks/CodeBlock.a9c4becf.js";import{H as ae,E as je}from"../chunks/index.5d4ab994.js";function _e(V){let n,f='Pipelines with a <img alt="MPS" src="https://img.shields.io/badge/MPS-000000?style=flat&amp;logo=apple&amp;logoColor=white%22"/> badge indicate a model can take advantage of the MPS backend on Apple silicon devices for faster inference. Feel free to open a <a href="https://github.com/huggingface/diffusers/compare" rel="nofollow">Pull Request</a> to add this badge to pipelines that are missing it.';return{c(){n=p("p"),n.innerHTML=f},l(o){n=r(o,"P",{"data-svelte-h":!0}),m(n)!=="svelte-1mb04cv"&&(n.innerHTML=f)},m(o,b){l(o,n,b)},p:ge,d(o){o&&s(n)}}}function Ce(V){let n,f='The PyTorch <a href="https://pytorch.org/docs/stable/notes/mps.html" rel="nofollow">mps</a> backend does not support NDArray sizes greater than <code>2**32</code>. Please open an <a href="https://github.com/huggingface/diffusers/issues/new/choose" rel="nofollow">Issue</a> if you encounter this problem so we can investigate.';return{c(){n=p("p"),n.innerHTML=f},l(o){n=r(o,"P",{"data-svelte-h":!0}),m(n)!=="svelte-1054fmo"&&(n.innerHTML=f)},m(o,b){l(o,n,b)},p:ge,d(o){o&&s(n)}}}function He(V){let n,f,o,b,$,Y,w,z,g,pe='ü§ó Diffusers is compatible with Apple silicon (M1/M2 chips) using the PyTorch <a href="https://pytorch.org/docs/stable/notes/mps.html" rel="nofollow"><code>mps</code></a> device, which uses the Metal framework to leverage the GPU on MacOS devices. You‚Äôll need to have:',E,T,re='<li>macOS computer with Apple silicon (M1/M2) hardware</li> <li>macOS 12.6 or later (13.0 or later recommended)</li> <li>arm64 version of Python</li> <li><a href="https://pytorch.org/get-started/locally/" rel="nofollow">PyTorch 2.0</a> (recommended) or 1.13 (minimum version supported for <code>mps</code>)</li>',x,v,me="The <code>mps</code> backend uses PyTorch‚Äôs <code>.to()</code> interface to move the Stable Diffusion pipeline on to your M1 or M2 device:",F,J,k,U,X,Z,fe="If you‚Äôre using <strong>PyTorch 1.13</strong>, you need to ‚Äúprime‚Äù the pipeline with an additional one-time pass through it. This is a temporary workaround for an issue where the first inference pass produces slightly different results than subsequent ones. You only need to do this pass once, and after just one inference step you can discard the result.",q,G,A,B,Q,j,ce="This section lists some common issues with using the <code>mps</code> backend and how to solve them.",N,_,D,C,ue="M1/M2 performance is very sensitive to memory pressure. When this occurs, the system automatically swaps if it needs to which significantly degrades performance.",K,H,ye='To prevent this from happening, we recommend <em>attention slicing</em> to reduce memory pressure during inference and prevent swapping. This is especially relevant if your computer has less than 64GB of system RAM, or if you generate images at non-standard resolutions larger than 512√ó512 pixels. Call the <a href="/docs/diffusers/v0.33.1/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_attention_slicing">enable_attention_slicing()</a> function on your pipeline:',O,S,ee,I,Me="Attention slicing performs the costly attention operation in multiple steps instead of all at once. It usually improves performance by ~20% in computers without universal memory, but we‚Äôve observed <em>better performance</em> in most Apple silicon computers unless you have 64GB of RAM or more.",te,L,se,P,he="Generating multiple prompts in a batch can crash or fail to work reliably. If this is the case, try iterating instead of batching.",le,R,ne,W,ie;return $=new ae({props:{title:"Metal Performance Shaders (MPS)",local:"metal-performance-shaders-mps",headingTag:"h1"}}),w=new $e({props:{warning:!1,$$slots:{default:[_e]},$$scope:{ctx:V}}}),J=new oe({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMERpZmZ1c2lvblBpcGVsaW5lJTBBJTBBcGlwZSUyMCUzRCUyMERpZmZ1c2lvblBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMjJzdGFibGUtZGlmZnVzaW9uLXYxLTUlMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUlMjIpJTBBcGlwZSUyMCUzRCUyMHBpcGUudG8oJTIybXBzJTIyKSUwQSUwQSUyMyUyMFJlY29tbWVuZGVkJTIwaWYlMjB5b3VyJTIwY29tcHV0ZXIlMjBoYXMlMjAlM0MlMjA2NCUyMEdCJTIwb2YlMjBSQU0lMEFwaXBlLmVuYWJsZV9hdHRlbnRpb25fc2xpY2luZygpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhbiUyMGFzdHJvbmF1dCUyMHJpZGluZyUyMGElMjBob3JzZSUyMG9uJTIwbWFycyUyMiUwQWltYWdlJTIwJTNEJTIwcGlwZShwcm9tcHQpLmltYWdlcyU1QjAlNUQlMEFpbWFnZQ==",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;</span>)
pipe = pipe.to(<span class="hljs-string">&quot;mps&quot;</span>)

<span class="hljs-comment"># Recommended if your computer has &lt; 64 GB of RAM</span>
pipe.enable_attention_slicing()

prompt = <span class="hljs-string">&quot;a photo of an astronaut riding a horse on mars&quot;</span>
image = pipe(prompt).images[<span class="hljs-number">0</span>]
image`,wrap:!1}}),U=new $e({props:{warning:!0,$$slots:{default:[Ce]},$$scope:{ctx:V}}}),G=new oe({props:{code:"JTIwJTIwZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMERpZmZ1c2lvblBpcGVsaW5lJTBBJTBBJTIwJTIwcGlwZSUyMCUzRCUyMERpZmZ1c2lvblBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMjJzdGFibGUtZGlmZnVzaW9uLXYxLTUlMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUlMjIpLnRvKCUyMm1wcyUyMiklMEElMjAlMjBwaXBlLmVuYWJsZV9hdHRlbnRpb25fc2xpY2luZygpJTBBJTBBJTIwJTIwcHJvbXB0JTIwJTNEJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhbiUyMGFzdHJvbmF1dCUyMHJpZGluZyUyMGElMjBob3JzZSUyMG9uJTIwbWFycyUyMiUwQSUyMCUyMCUyMyUyMEZpcnN0LXRpbWUlMjAlMjJ3YXJtdXAlMjIlMjBwYXNzJTIwaWYlMjBQeVRvcmNoJTIwdmVyc2lvbiUyMGlzJTIwMS4xMyUwQSUyQiUyMF8lMjAlM0QlMjBwaXBlKHByb21wdCUyQyUyMG51bV9pbmZlcmVuY2Vfc3RlcHMlM0QxKSUwQSUwQSUyMCUyMCUyMyUyMFJlc3VsdHMlMjBtYXRjaCUyMHRob3NlJTIwZnJvbSUyMHRoZSUyMENQVSUyMGRldmljZSUyMGFmdGVyJTIwdGhlJTIwd2FybXVwJTIwcGFzcy4lMEElMjAlMjBpbWFnZSUyMCUzRCUyMHBpcGUocHJvbXB0KS5pbWFnZXMlNUIwJTVE",highlighted:`  from diffusers import DiffusionPipeline

  pipe = DiffusionPipeline.from_pretrained(&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;).to(&quot;mps&quot;)
  pipe.enable_attention_slicing()

  prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
  # First-time &quot;warmup&quot; pass if PyTorch version is 1.13
<span class="hljs-addition">+ _ = pipe(prompt, num_inference_steps=1)</span>

  # Results match those from the CPU device after the warmup pass.
  image = pipe(prompt).images[0]`,wrap:!1}}),B=new ae({props:{title:"Troubleshoot",local:"troubleshoot",headingTag:"h2"}}),_=new ae({props:{title:"Attention slicing",local:"attention-slicing",headingTag:"h3"}}),S=new oe({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMERpZmZ1c2lvblBpcGVsaW5lJTBBaW1wb3J0JTIwdG9yY2glMEElMEFwaXBlbGluZSUyMCUzRCUyMERpZmZ1c2lvblBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMjJzdGFibGUtZGlmZnVzaW9uLXYxLTUlMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMjB2YXJpYW50JTNEJTIyZnAxNiUyMiUyQyUyMHVzZV9zYWZldGVuc29ycyUzRFRydWUpLnRvKCUyMm1wcyUyMiklMEFwaXBlbGluZS5lbmFibGVfYXR0ZW50aW9uX3NsaWNpbmcoKQ==",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DiffusionPipeline
<span class="hljs-keyword">import</span> torch

pipeline = DiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;</span>, torch_dtype=torch.float16, variant=<span class="hljs-string">&quot;fp16&quot;</span>, use_safetensors=<span class="hljs-literal">True</span>).to(<span class="hljs-string">&quot;mps&quot;</span>)
pipeline.enable_attention_slicing()`,wrap:!1}}),L=new ae({props:{title:"Batch inference",local:"batch-inference",headingTag:"h3"}}),R=new je({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/mps.md"}}),{c(){n=p("meta"),f=i(),o=p("p"),b=i(),c($.$$.fragment),Y=i(),c(w.$$.fragment),z=i(),g=p("p"),g.innerHTML=pe,E=i(),T=p("ul"),T.innerHTML=re,x=i(),v=p("p"),v.innerHTML=me,F=i(),c(J.$$.fragment),k=i(),c(U.$$.fragment),X=i(),Z=p("p"),Z.innerHTML=fe,q=i(),c(G.$$.fragment),A=i(),c(B.$$.fragment),Q=i(),j=p("p"),j.innerHTML=ce,N=i(),c(_.$$.fragment),D=i(),C=p("p"),C.textContent=ue,K=i(),H=p("p"),H.innerHTML=ye,O=i(),c(S.$$.fragment),ee=i(),I=p("p"),I.innerHTML=Me,te=i(),c(L.$$.fragment),se=i(),P=p("p"),P.textContent=he,le=i(),c(R.$$.fragment),ne=i(),W=p("p"),this.h()},l(e){const t=Ge("svelte-u9bgzb",document.head);n=r(t,"META",{name:!0,content:!0}),t.forEach(s),f=a(e),o=r(e,"P",{}),we(o).forEach(s),b=a(e),u($.$$.fragment,e),Y=a(e),u(w.$$.fragment,e),z=a(e),g=r(e,"P",{"data-svelte-h":!0}),m(g)!=="svelte-j79ol3"&&(g.innerHTML=pe),E=a(e),T=r(e,"UL",{"data-svelte-h":!0}),m(T)!=="svelte-1rb0nsv"&&(T.innerHTML=re),x=a(e),v=r(e,"P",{"data-svelte-h":!0}),m(v)!=="svelte-n7utwp"&&(v.innerHTML=me),F=a(e),u(J.$$.fragment,e),k=a(e),u(U.$$.fragment,e),X=a(e),Z=r(e,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-1sliuep"&&(Z.innerHTML=fe),q=a(e),u(G.$$.fragment,e),A=a(e),u(B.$$.fragment,e),Q=a(e),j=r(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-161cphk"&&(j.innerHTML=ce),N=a(e),u(_.$$.fragment,e),D=a(e),C=r(e,"P",{"data-svelte-h":!0}),m(C)!=="svelte-1oew7wr"&&(C.textContent=ue),K=a(e),H=r(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-151bcz2"&&(H.innerHTML=ye),O=a(e),u(S.$$.fragment,e),ee=a(e),I=r(e,"P",{"data-svelte-h":!0}),m(I)!=="svelte-xihi2j"&&(I.innerHTML=Me),te=a(e),u(L.$$.fragment,e),se=a(e),P=r(e,"P",{"data-svelte-h":!0}),m(P)!=="svelte-lu9cjo"&&(P.textContent=he),le=a(e),u(R.$$.fragment,e),ne=a(e),W=r(e,"P",{}),we(W).forEach(s),this.h()},h(){Ue(n,"name","hf:doc:metadata"),Ue(n,"content",Se)},m(e,t){Be(document.head,n),l(e,f,t),l(e,o,t),l(e,b,t),y($,e,t),l(e,Y,t),y(w,e,t),l(e,z,t),l(e,g,t),l(e,E,t),l(e,T,t),l(e,x,t),l(e,v,t),l(e,F,t),y(J,e,t),l(e,k,t),y(U,e,t),l(e,X,t),l(e,Z,t),l(e,q,t),y(G,e,t),l(e,A,t),y(B,e,t),l(e,Q,t),l(e,j,t),l(e,N,t),y(_,e,t),l(e,D,t),l(e,C,t),l(e,K,t),l(e,H,t),l(e,O,t),y(S,e,t),l(e,ee,t),l(e,I,t),l(e,te,t),y(L,e,t),l(e,se,t),l(e,P,t),l(e,le,t),y(R,e,t),l(e,ne,t),l(e,W,t),ie=!0},p(e,[t]){const de={};t&2&&(de.$$scope={dirty:t,ctx:e}),w.$set(de);const be={};t&2&&(be.$$scope={dirty:t,ctx:e}),U.$set(be)},i(e){ie||(M($.$$.fragment,e),M(w.$$.fragment,e),M(J.$$.fragment,e),M(U.$$.fragment,e),M(G.$$.fragment,e),M(B.$$.fragment,e),M(_.$$.fragment,e),M(S.$$.fragment,e),M(L.$$.fragment,e),M(R.$$.fragment,e),ie=!0)},o(e){h($.$$.fragment,e),h(w.$$.fragment,e),h(J.$$.fragment,e),h(U.$$.fragment,e),h(G.$$.fragment,e),h(B.$$.fragment,e),h(_.$$.fragment,e),h(S.$$.fragment,e),h(L.$$.fragment,e),h(R.$$.fragment,e),ie=!1},d(e){e&&(s(f),s(o),s(b),s(Y),s(z),s(g),s(E),s(T),s(x),s(v),s(F),s(k),s(X),s(Z),s(q),s(A),s(Q),s(j),s(N),s(D),s(C),s(K),s(H),s(O),s(ee),s(I),s(te),s(se),s(P),s(le),s(ne),s(W)),s(n),d($,e),d(w,e),d(J,e),d(U,e),d(G,e),d(B,e),d(_,e),d(S,e),d(L,e),d(R,e)}}}const Se='{"title":"Metal Performance Shaders (MPS)","local":"metal-performance-shaders-mps","sections":[{"title":"Troubleshoot","local":"troubleshoot","sections":[{"title":"Attention slicing","local":"attention-slicing","sections":[],"depth":3},{"title":"Batch inference","local":"batch-inference","sections":[],"depth":3}],"depth":2}],"depth":1}';function Ie(V){return ve(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ye extends Je{constructor(n){super(),Ze(this,n,Ie,He,Te,{})}}export{Ye as component};
