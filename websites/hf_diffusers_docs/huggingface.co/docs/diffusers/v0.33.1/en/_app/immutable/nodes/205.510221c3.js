import{s as J,n as K,o as Q}from"../chunks/scheduler.8c3d61f6.js";import{S as Y,i as Z,g,s as r,r as b,A as ee,h as _,f as t,c as s,j as A,u as q,x as G,k as O,y as D,a as n,v,d as x,t as $,w}from"../chunks/index.da70eac4.js";import{D as B}from"../chunks/Docstring.567bc132.js";import{H as z,E as oe}from"../chunks/index.5d4ab994.js";function te(W){let i,E,I,F,a,P,p,R,d,l,k,h,M="Hugging Face Hybrid Inference that allow running VAE decode remotely.",C,u,L,c,f,U,y,X="Hugging Face Hybrid Inference that allow running VAE encode remotely.",S,m,N,T,V;return a=new z({props:{title:"Hybrid Inference API Reference",local:"hybrid-inference-api-reference",headingTag:"h1"}}),p=new z({props:{title:"Remote Decode",local:"diffusers.utils.remote_decode",headingTag:"h2"}}),l=new B({props:{name:"diffusers.utils.remote_decode",anchor:"diffusers.utils.remote_decode",parameters:[{name:"endpoint",val:": str"},{name:"tensor",val:": torch.Tensor"},{name:"processor",val:": typing.Union[ForwardRef('VaeImageProcessor'), ForwardRef('VideoProcessor'), NoneType] = None"},{name:"do_scaling",val:": bool = True"},{name:"scaling_factor",val:": typing.Optional[float] = None"},{name:"shift_factor",val:": typing.Optional[float] = None"},{name:"output_type",val:": typing.Literal['mp4', 'pil', 'pt'] = 'pil'"},{name:"return_type",val:": typing.Literal['mp4', 'pil', 'pt'] = 'pil'"},{name:"image_format",val:": typing.Literal['png', 'jpg'] = 'jpg'"},{name:"partial_postprocess",val:": bool = False"},{name:"input_tensor_type",val:": typing.Literal['binary'] = 'binary'"},{name:"output_tensor_type",val:": typing.Literal['binary'] = 'binary'"},{name:"height",val:": typing.Optional[int] = None"},{name:"width",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.utils.remote_decode.endpoint",description:`<strong>endpoint</strong> (<code>str</code>) &#x2014;
Endpoint for Remote Decode.`,name:"endpoint"},{anchor:"diffusers.utils.remote_decode.tensor",description:`<strong>tensor</strong> (<code>torch.Tensor</code>) &#x2014;
Tensor to be decoded.`,name:"tensor"},{anchor:"diffusers.utils.remote_decode.processor",description:`<strong>processor</strong> (<code>VaeImageProcessor</code> or <code>VideoProcessor</code>, <em>optional</em>) &#x2014;
Used with <code>return_type=&quot;pt&quot;</code>, and <code>return_type=&quot;pil&quot;</code> for Video models.`,name:"processor"},{anchor:"diffusers.utils.remote_decode.do_scaling",description:`<strong>do_scaling</strong> (<code>bool</code>, default <code>True</code>, <em>optional</em>) &#x2014;
<strong>DEPRECATED</strong>. <strong>pass <code>scaling_factor</code>/<code>shift_factor</code> instead.</strong> <strong>still set
do_scaling=None/do_scaling=False for no scaling until option is removed</strong> When <code>True</code> scaling e.g. <code>latents / self.vae.config.scaling_factor</code> is applied remotely. If <code>False</code>, input must be passed with scaling
applied.`,name:"do_scaling"},{anchor:"diffusers.utils.remote_decode.scaling_factor",description:`<strong>scaling_factor</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Scaling is applied when passed e.g. <a href="https://github.com/huggingface/diffusers/blob/7007febae5cff000d4df9059d9cf35133e8b2ca9/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L1083C37-L1083C77" rel="nofollow"><code>latents / self.vae.config.scaling_factor</code></a>.</p>
<ul>
<li>SD v1: 0.18215</li>
<li>SD XL: 0.13025</li>
<li>Flux: 0.3611
If <code>None</code>, input must be passed with scaling applied.</li>
</ul>`,name:"scaling_factor"},{anchor:"diffusers.utils.remote_decode.shift_factor",description:`<strong>shift_factor</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Shift is applied when passed e.g. <code>latents + self.vae.config.shift_factor</code>.</p>
<ul>
<li>Flux: 0.1159
If <code>None</code>, input must be passed with scaling applied.</li>
</ul>`,name:"shift_factor"},{anchor:"diffusers.utils.remote_decode.output_type",description:`<strong>output_type</strong> (<code>&quot;mp4&quot;</code> or <code>&quot;pil&quot;</code> or <code>&quot;pt&quot;, default </code>&#x201C;pil&#x201D;) &#x2014;
<strong>Endpoint</strong> output type. Subject to change. Report feedback on preferred type.</p>
<p><code>&quot;mp4&quot;: Supported by video models. Endpoint returns </code>bytes<code>of video.</code>&#x201C;pil&#x201D;<code>: Supported by image and video models. Image models: Endpoint returns </code>bytes<code>of an image in</code>image_format<code>. Video models: Endpoint returns </code>torch.Tensor<code>with partial</code>postprocessing<code>applied. Requires</code>processor<code>as a flag (any</code>None<code>value will work).</code>&#x201C;pt&#x201D;<code>: Support by image and video models. Endpoint returns </code>torch.Tensor<code>. With </code>partial_postprocess=True<code>the tensor is postprocessed</code>uint8\` image tensor.</p>
<p>Recommendations:
<code>&quot;pt&quot;</code> with <code>partial_postprocess=True</code> is the smallest transfer for full quality. <code>&quot;pt&quot;</code> with
<code>partial_postprocess=False</code> is the most compatible with third party code. <code>&quot;pil&quot;</code> with
<code>image_format=&quot;jpg&quot;</code> is the smallest transfer overall.`,name:"output_type"},{anchor:"diffusers.utils.remote_decode.return_type",description:`<strong>return_type</strong> (<code>&quot;mp4&quot;</code> or <code>&quot;pil&quot;</code> or <code>&quot;pt&quot;, default </code>&#x201C;pil&#x201D;) &#x2014;
<strong>Function</strong> return type.</p>
<p><code>&quot;mp4&quot;: Function returns </code>bytes<code>of video.</code>&#x201C;pil&#x201D;<code>: Function returns </code>PIL.Image.Image<code>. With </code>output_type=&#x201C;pil&#x201D; no further processing is applied. With <code>output_type=&quot;pt&quot; a </code>PIL.Image.Image<code>is created.</code>partial_postprocess=False<code> </code>processor<code>is required.</code>partial_postprocess=True<code> </code>processor<code>is **not** required.</code>&#x201C;pt&#x201D;<code>: Function returns </code>torch.Tensor<code>. </code>processor<code>is **not** required.</code>partial_postprocess=False<code>tensor is</code>float16<code>or</code>bfloat16<code>, without denormalization. </code>partial_postprocess=True<code>tensor is</code>uint8\`, denormalized.`,name:"return_type"},{anchor:"diffusers.utils.remote_decode.image_format",description:`<strong>image_format</strong> (<code>&quot;png&quot;</code> or <code>&quot;jpg&quot;</code>, default <code>jpg</code>) &#x2014;
Used with <code>output_type=&quot;pil&quot;</code>. Endpoint returns <code>jpg</code> or <code>png</code>.`,name:"image_format"},{anchor:"diffusers.utils.remote_decode.partial_postprocess",description:`<strong>partial_postprocess</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
Used with <code>output_type=&quot;pt&quot;</code>. <code>partial_postprocess=False</code> tensor is <code>float16</code> or <code>bfloat16</code>, without
denormalization. <code>partial_postprocess=True</code> tensor is <code>uint8</code>, denormalized.`,name:"partial_postprocess"},{anchor:"diffusers.utils.remote_decode.input_tensor_type",description:`<strong>input_tensor_type</strong> (<code>&quot;binary&quot;</code>, default <code>&quot;binary&quot;</code>) &#x2014;
Tensor transfer type.`,name:"input_tensor_type"},{anchor:"diffusers.utils.remote_decode.output_tensor_type",description:`<strong>output_tensor_type</strong> (<code>&quot;binary&quot;</code>, default <code>&quot;binary&quot;</code>) &#x2014;
Tensor transfer type.`,name:"output_tensor_type"},{anchor:"diffusers.utils.remote_decode.height",description:`<strong>height</strong> (<code>int</code>, <strong>optional</strong>) &#x2014;
Required for <code>&quot;packed&quot;</code> latents.`,name:"height"},{anchor:"diffusers.utils.remote_decode.width",description:`<strong>width</strong> (<code>int</code>, <strong>optional</strong>) &#x2014;
Required for <code>&quot;packed&quot;</code> latents.`,name:"width"}],source:"https://github.com/huggingface/diffusers/blob/v0.33.1/src/diffusers/utils/remote_utils.py#L188",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>output (<code>Image.Image</code> or <code>List[Image.Image]</code> or <code>bytes</code> or <code>torch.Tensor</code>).</p>
`}}),u=new z({props:{title:"Remote Encode",local:"diffusers.utils.remote_utils.remote_encode",headingTag:"h2"}}),f=new B({props:{name:"diffusers.utils.remote_utils.remote_encode",anchor:"diffusers.utils.remote_utils.remote_encode",parameters:[{name:"endpoint",val:": str"},{name:"image",val:": typing.Union[ForwardRef('torch.Tensor'), PIL.Image.Image]"},{name:"scaling_factor",val:": typing.Optional[float] = None"},{name:"shift_factor",val:": typing.Optional[float] = None"}],parametersDescription:[{anchor:"diffusers.utils.remote_utils.remote_encode.endpoint",description:`<strong>endpoint</strong> (<code>str</code>) &#x2014;
Endpoint for Remote Decode.`,name:"endpoint"},{anchor:"diffusers.utils.remote_utils.remote_encode.image",description:`<strong>image</strong> (<code>torch.Tensor</code> or <code>PIL.Image.Image</code>) &#x2014;
Image to be encoded.`,name:"image"},{anchor:"diffusers.utils.remote_utils.remote_encode.scaling_factor",description:`<strong>scaling_factor</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Scaling is applied when passed e.g. <code>latents * self.vae.config.scaling_factor</code>.<ul>
<li>SD v1: 0.18215</li>
<li>SD XL: 0.13025</li>
<li>Flux: 0.3611
If <code>None</code>, input must be passed with scaling applied.</li>
</ul>`,name:"scaling_factor"},{anchor:"diffusers.utils.remote_utils.remote_encode.shift_factor",description:`<strong>shift_factor</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Shift is applied when passed e.g. <code>latents - self.vae.config.shift_factor</code>.<ul>
<li>Flux: 0.1159
If <code>None</code>, input must be passed with scaling applied.</li>
</ul>`,name:"shift_factor"}],source:"https://github.com/huggingface/diffusers/blob/v0.33.1/src/diffusers/utils/remote_utils.py#L380",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>output (<code>torch.Tensor</code>).</p>
`}}),m=new oe({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/api_reference.md"}}),{c(){i=g("meta"),E=r(),I=g("p"),F=r(),b(a.$$.fragment),P=r(),b(p.$$.fragment),R=r(),d=g("div"),b(l.$$.fragment),k=r(),h=g("p"),h.textContent=M,C=r(),b(u.$$.fragment),L=r(),c=g("div"),b(f.$$.fragment),U=r(),y=g("p"),y.textContent=X,S=r(),b(m.$$.fragment),N=r(),T=g("p"),this.h()},l(e){const o=ee("svelte-u9bgzb",document.head);i=_(o,"META",{name:!0,content:!0}),o.forEach(t),E=s(e),I=_(e,"P",{}),A(I).forEach(t),F=s(e),q(a.$$.fragment,e),P=s(e),q(p.$$.fragment,e),R=s(e),d=_(e,"DIV",{class:!0});var j=A(d);q(l.$$.fragment,j),k=s(j),h=_(j,"P",{"data-svelte-h":!0}),G(h)!=="svelte-1102kvv"&&(h.textContent=M),j.forEach(t),C=s(e),q(u.$$.fragment,e),L=s(e),c=_(e,"DIV",{class:!0});var H=A(c);q(f.$$.fragment,H),U=s(H),y=_(H,"P",{"data-svelte-h":!0}),G(y)!=="svelte-ju55g7"&&(y.textContent=X),H.forEach(t),S=s(e),q(m.$$.fragment,e),N=s(e),T=_(e,"P",{}),A(T).forEach(t),this.h()},h(){O(i,"name","hf:doc:metadata"),O(i,"content",ne),O(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){D(document.head,i),n(e,E,o),n(e,I,o),n(e,F,o),v(a,e,o),n(e,P,o),v(p,e,o),n(e,R,o),n(e,d,o),v(l,d,null),D(d,k),D(d,h),n(e,C,o),v(u,e,o),n(e,L,o),n(e,c,o),v(f,c,null),D(c,U),D(c,y),n(e,S,o),v(m,e,o),n(e,N,o),n(e,T,o),V=!0},p:K,i(e){V||(x(a.$$.fragment,e),x(p.$$.fragment,e),x(l.$$.fragment,e),x(u.$$.fragment,e),x(f.$$.fragment,e),x(m.$$.fragment,e),V=!0)},o(e){$(a.$$.fragment,e),$(p.$$.fragment,e),$(l.$$.fragment,e),$(u.$$.fragment,e),$(f.$$.fragment,e),$(m.$$.fragment,e),V=!1},d(e){e&&(t(E),t(I),t(F),t(P),t(R),t(d),t(C),t(L),t(c),t(S),t(N),t(T)),t(i),w(a,e),w(p,e),w(l),w(u,e),w(f),w(m,e)}}}const ne='{"title":"Hybrid Inference API Reference","local":"hybrid-inference-api-reference","sections":[{"title":"Remote Decode","local":"diffusers.utils.remote_decode","sections":[],"depth":2},{"title":"Remote Encode","local":"diffusers.utils.remote_utils.remote_encode","sections":[],"depth":2}],"depth":1}';function re(W){return Q(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ae extends Y{constructor(i){super(),Z(this,i,re,te,J,{})}}export{ae as component};
