import{s as Dl,o as Pl,n as nl}from"../chunks/scheduler.8c3d61f6.js";import{S as ql,i as Kl,g as i,s as n,r as c,A as Ol,h as p,f as l,c as a,j as Yl,u as m,x as o,k as B,y as es,a as s,v as u,d as M,t as f,w as d}from"../chunks/index.da70eac4.js";import{T as sl}from"../chunks/Tip.1d9b8c37.js";import{C as h}from"../chunks/CodeBlock.a9c4becf.js";import{H as J,E as ts}from"../chunks/index.5d4ab994.js";function ls(w){let r,b=`The results reported below are from a 80GB 400W A100 with its clock rate set to the maximum.
If you’re interested in the full benchmarking code, take a look at <a href="https://github.com/huggingface/diffusion-fast" rel="nofollow">huggingface/diffusion-fast</a>.`;return{c(){r=i("p"),r.innerHTML=b},l(y){r=p(y,"P",{"data-svelte-h":!0}),o(r)!=="svelte-17hqnyr"&&(r.innerHTML=b)},m(y,j){s(y,r,j)},p:nl,d(y){y&&l(r)}}}function ss(w){let r,b="In our later experiments with float16, recent versions of torchao do not incur numerical problems from float16.";return{c(){r=i("p"),r.textContent=b},l(y){r=p(y,"P",{"data-svelte-h":!0}),o(r)!=="svelte-17qv8f9"&&(r.textContent=b)},m(y,j){s(y,r,j)},p:nl,d(y){y&&l(r)}}}function ns(w){let r,b='From PyTorch 2.3.1, you can control the caching behavior of <code>torch.compile()</code>. This is particularly beneficial for compilation modes like <code>&quot;max-autotune&quot;</code> which performs a grid-search over several compilation flags to find the optimal configuration. Learn more in the <a href="https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html" rel="nofollow">Compile Time Caching in torch.compile</a> tutorial.';return{c(){r=i("p"),r.innerHTML=b},l(y){r=p(y,"P",{"data-svelte-h":!0}),o(r)!=="svelte-y3dot9"&&(r.innerHTML=b)},m(y,j){s(y,r,j)},p:nl,d(y){y&&l(r)}}}function as(w){let r,b='Support for <code>fuse_qkv_projections()</code> is limited and experimental. It’s not available for many non-Stable Diffusion pipelines such as <a href="../using-diffusers/kandinsky">Kandinsky</a>. You can refer to this <a href="https://github.com/huggingface/diffusers/pull/6179" rel="nofollow">PR</a> to get an idea about how to enable this for the other pipelines.';return{c(){r=i("p"),r.innerHTML=b},l(y){r=p(y,"P",{"data-svelte-h":!0}),o(r)!=="svelte-onvios"&&(r.innerHTML=b)},m(y,j){s(y,r,j)},p:nl,d(y){y&&l(r)}}}function is(w){let r,b,y,j,A,Fe,k,al='Diffusion models are slower than their GAN counterparts because of the iterative and sequential reverse diffusion process. There are several techniques that can address this limitation such as progressive timestep distillation (<a href="../using-diffusers/inference_with_lcm_lora">LCM LoRA</a>), model compression (<a href="https://huggingface.co/segmind/SSD-1B" rel="nofollow">SSD-1B</a>), and reusing adjacent features of the denoiser (<a href="../optimization/deepcache">DeepCache</a>).',Ve,x,il='However, you don’t necessarily need to use these techniques to speed up inference. With PyTorch 2 alone, you can accelerate the inference latency of text-to-image diffusion pipelines by up to 3x. This tutorial will show you how to progressively apply the optimizations found in PyTorch 2 to reduce inference latency. You’ll use the <a href="../using-diffusers/sdxl">Stable Diffusion XL (SDXL)</a> pipeline in this tutorial, but these techniques are applicable to other text-to-image diffusion pipelines too.',He,W,pl="Make sure you’re using the latest version of Diffusers:",Qe,S,Ne,X,ol="Then upgrade the other required libraries too:",Ee,R,ze,F,rl='Install <a href="https://pytorch.org/" rel="nofollow">PyTorch nightly</a> to benefit from the latest and fastest kernels:',Le,V,Ye,U,De,H,Pe,Q,cl='Let’s start with a baseline. Disable reduced precision and the <a href="../optimization/torch2.0#scaled-dot-product-attention"><code>scaled_dot_product_attention</code> (SDPA)</a> function which is automatically used by Diffusers:',qe,N,Ke,E,ml="This default setup takes 7.36 seconds.",Oe,T,ul='<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/progressive-acceleration-sdxl/SDXL%2C_Batch_Size%3A_1%2C_Steps%3A_30_0.png" width="500"/>',et,z,tt,L,Ml="Enable the first optimization, reduced precision or more specifically bfloat16. There are several benefits of using reduced precision:",lt,Y,fl='<li>Using a reduced numerical precision (such as float16 or bfloat16) for inference doesn’t affect the generation quality but significantly improves latency.</li> <li>The benefits of using bfloat16 compared to float16 are hardware dependent, but modern GPUs tend to favor bfloat16.</li> <li>bfloat16 is much more resilient when used with quantization compared to float16, but more recent versions of the quantization library (<a href="https://github.com/pytorch-labs/ao" rel="nofollow">torchao</a>) we used don’t have numerical issues with float16.</li>',st,D,nt,P,dl="bfloat16 reduces the latency from 7.36 seconds to 4.63 seconds.",at,C,yl='<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/progressive-acceleration-sdxl/SDXL%2C_Batch_Size%3A_1%2C_Steps%3A_30_1.png" width="500"/>',it,v,pt,q,hl='Take a look at the <a href="../optimization/fp16">Speed up inference</a> guide to learn more about running inference with reduced precision.',ot,K,rt,O,bl='Attention blocks are intensive to run. But with PyTorch’s <a href="../optimization/torch2.0#scaled-dot-product-attention"><code>scaled_dot_product_attention</code></a> function, it is a lot more efficient. This function is used by default in Diffusers so you don’t need to make any changes to the code.',ct,ee,mt,te,jl="Scaled dot product attention improves the latency from 4.63 seconds to 3.31 seconds.",ut,$,wl='<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/progressive-acceleration-sdxl/SDXL%2C_Batch_Size%3A_1%2C_Steps%3A_30_2.png" width="500"/>',Mt,le,ft,se,Jl='PyTorch 2 includes <code>torch.compile</code> which uses fast and optimized kernels. In Diffusers, the UNet and VAE are usually compiled because these are the most compute-intensive modules. First, configure a few compiler flags (refer to the <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py" rel="nofollow">full list</a> for more options):',dt,ne,yt,ae,Ul="It is also important to change the UNet and VAE’s memory layout to “channels_last” when compiling them to ensure maximum speed.",ht,ie,bt,pe,Tl="Now compile and perform inference:",jt,oe,wt,re,Cl="<code>torch.compile</code> offers different backends and modes. For maximum inference speed, use “max-autotune” for the inductor backend. “max-autotune” uses CUDA graphs and optimizes the compilation graph specifically for latency. CUDA graphs greatly reduces the overhead of launching GPU operations by using a mechanism to launch multiple GPU operations through a single CPU operation.",Jt,ce,vl="Using SDPA attention and compiling both the UNet and VAE cuts the latency from 3.31 seconds to 2.54 seconds.",Ut,g,$l='<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/progressive-acceleration-sdxl/SDXL%2C_Batch_Size%3A_1%2C_Steps%3A_30_3.png" width="500"/>',Tt,_,Ct,me,vt,ue,gl="Specifying <code>fullgraph=True</code> ensures there are no graph breaks in the underlying model to take full advantage of <code>torch.compile</code> without any performance degradation. For the UNet and VAE, this means changing how you access the return variables.",$t,Me,gt,fe,_t,de,_l='During the iterative reverse diffusion process, the <code>step()</code> function is <a href="https://github.com/huggingface/diffusers/blob/1d686bac8146037e97f3fd8c56e4063230f71751/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L1228" rel="nofollow">called</a> on the scheduler each time after the denoiser predicts the less noisy latent embeddings. Inside <code>step()</code>, the <code>sigmas</code> variable is <a href="https://github.com/huggingface/diffusers/blob/1d686bac8146037e97f3fd8c56e4063230f71751/src/diffusers/schedulers/scheduling_euler_discrete.py#L476" rel="nofollow">indexed</a> which when placed on the GPU, causes a communication sync between the CPU and GPU. This introduces latency and it becomes more evident when the denoiser has already been compiled.',Zt,ye,Zl='But if the <code>sigmas</code> array always <a href="https://github.com/huggingface/diffusers/blob/35a969d297cba69110d175ee79c59312b9f49e1e/src/diffusers/schedulers/scheduling_euler_discrete.py#L240" rel="nofollow">stays on the CPU</a>, the CPU and GPU sync doesn’t occur and you don’t get any latency. In general, any CPU and GPU communication sync should be none or be kept to a bare minimum because it can impact inference latency.',Gt,he,It,be,Gl="The UNet and VAE in SDXL use Transformer-like blocks which consists of attention blocks and feed-forward blocks.",Bt,je,Il="In an attention block, the input is projected into three sub-spaces using three different projection matrices – Q, K, and V. These projections are performed separately on the input. But we can horizontally combine the projection matrices into a single matrix and perform the projection in one step. This increases the size of the matrix multiplications of the input projections and improves the impact of quantization.",At,we,Bl="You can combine the projection matrices with just a single line of code:",kt,Je,xt,Ue,Al="This provides a minor improvement from 2.54 seconds to 2.52 seconds.",Wt,Z,kl='<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/progressive-acceleration-sdxl/SDXL%2C_Batch_Size%3A_1%2C_Steps%3A_30_4.png" width="500"/>',St,G,Xt,Te,Rt,Ce,xl='You can also use the ultra-lightweight PyTorch quantization library, <a href="https://github.com/pytorch-labs/ao" rel="nofollow">torchao</a> (commit SHA <code>54bcd5a10d0abbe7b0c045052029257099f83fd9</code>), to apply <a href="https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html" rel="nofollow">dynamic int8 quantization</a> to the UNet and VAE. Quantization adds additional conversion overhead to the model that is hopefully made up for by faster matmuls (dynamic quantization). If the matmuls are too small, these techniques may degrade performance.',Ft,ve,Wl="First, configure all the compiler tags:",Vt,$e,Ht,ge,Sl='Certain linear layers in the UNet and VAE don’t benefit from dynamic int8 quantization. You can filter out those layers with the <a href="https://github.com/huggingface/diffusion-fast/blob/0f169640b1db106fe6a479f78c1ed3bfaeba3386/utils/pipeline_utils.py#L16" rel="nofollow"><code>dynamic_quant_filter_fn</code></a> shown below.',Qt,_e,Nt,Ze,Xl="Finally, apply all the optimizations discussed so far:",Et,Ge,zt,Ie,Rl="Since dynamic quantization is only limited to the linear layers, convert the appropriate pointwise convolution layers into linear layers to maximize its benefit.",Lt,Be,Yt,Ae,Fl="Apply dynamic quantization:",Dt,ke,Pt,xe,Vl="Finally, compile and perform inference:",qt,We,Kt,Se,Hl="Applying dynamic quantization improves the latency from 2.52 seconds to 2.43 seconds.",Ot,I,Ql='<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/progressive-acceleration-sdxl/SDXL%2C_Batch_Size%3A_1%2C_Steps%3A_30_5.png" width="500"/>',el,Xe,tl,Re,ll;return A=new J({props:{title:"Accelerate inference of text-to-image diffusion models",local:"accelerate-inference-of-text-to-image-diffusion-models",headingTag:"h1"}}),S=new h({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZGlmZnVzZXJz",highlighted:"pip install -U diffusers",wrap:!1}}),R=new h({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwdHJhbnNmb3JtZXJzJTIwYWNjZWxlcmF0ZSUyMHBlZnQ=",highlighted:"pip install -U transformers accelerate peft",wrap:!1}}),V=new h({props:{code:"cGlwMyUyMGluc3RhbGwlMjAtLXByZSUyMHRvcmNoJTIwLS1pbmRleC11cmwlMjBodHRwcyUzQSUyRiUyRmRvd25sb2FkLnB5dG9yY2gub3JnJTJGd2hsJTJGbmlnaHRseSUyRmN1MTIx",highlighted:"pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121",wrap:!1}}),U=new sl({props:{warning:!1,$$slots:{default:[ls]},$$scope:{ctx:w}}}),H=new J({props:{title:"Baseline",local:"baseline",headingTag:"h2"}}),N=new h({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEElMEElMjMlMjBMb2FkJTIwdGhlJTIwcGlwZWxpbmUlMjBpbiUyMGZ1bGwtcHJlY2lzaW9uJTIwYW5kJTIwcGxhY2UlMjBpdHMlMjBtb2RlbCUyMGNvbXBvbmVudHMlMjBvbiUyMENVREEuJTBBcGlwZSUyMCUzRCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi14bC1iYXNlLTEuMCUyMiUwQSkudG8oJTIyY3VkYSUyMiklMEElMEElMjMlMjBSdW4lMjB0aGUlMjBhdHRlbnRpb24lMjBvcHMlMjB3aXRob3V0JTIwU0RQQS4lMEFwaXBlLnVuZXQuc2V0X2RlZmF1bHRfYXR0bl9wcm9jZXNzb3IoKSUwQXBpcGUudmFlLnNldF9kZWZhdWx0X2F0dG5fcHJvY2Vzc29yKCklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJBc3Ryb25hdXQlMjBpbiUyMGElMjBqdW5nbGUlMkMlMjBjb2xkJTIwY29sb3IlMjBwYWxldHRlJTJDJTIwbXV0ZWQlMjBjb2xvcnMlMkMlMjBkZXRhaWxlZCUyQyUyMDhrJTIyJTBBaW1hZ2UlMjAlM0QlMjBwaXBlKHByb21wdCUyQyUyMG51bV9pbmZlcmVuY2Vfc3RlcHMlM0QzMCkuaW1hZ2VzJTVCMCU1RA==",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionXLPipeline

<span class="hljs-comment"># Load the pipeline in full-precision and place its model components on CUDA.</span>
pipe = StableDiffusionXLPipeline.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-comment"># Run the attention ops without SDPA.</span>
pipe.unet.set_default_attn_processor()
pipe.vae.set_default_attn_processor()

prompt = <span class="hljs-string">&quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;</span>
image = pipe(prompt, num_inference_steps=<span class="hljs-number">30</span>).images[<span class="hljs-number">0</span>]`,wrap:!1}}),z=new J({props:{title:"bfloat16",local:"bfloat16",headingTag:"h2"}}),D=new h({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGUlMjAlM0QlMjBTdGFibGVEaWZmdXNpb25YTFBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtYmFzZS0xLjAlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTBBKS50byglMjJjdWRhJTIyKSUwQSUwQSUyMyUyMFJ1biUyMHRoZSUyMGF0dGVudGlvbiUyMG9wcyUyMHdpdGhvdXQlMjBTRFBBLiUwQXBpcGUudW5ldC5zZXRfZGVmYXVsdF9hdHRuX3Byb2Nlc3NvcigpJTBBcGlwZS52YWUuc2V0X2RlZmF1bHRfYXR0bl9wcm9jZXNzb3IoKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMkFzdHJvbmF1dCUyMGluJTIwYSUyMGp1bmdsZSUyQyUyMGNvbGQlMjBjb2xvciUyMHBhbGV0dGUlMkMlMjBtdXRlZCUyMGNvbG9ycyUyQyUyMGRldGFpbGVkJTJDJTIwOGslMjIlMEFpbWFnZSUyMCUzRCUyMHBpcGUocHJvbXB0JTJDJTIwbnVtX2luZmVyZW5jZV9zdGVwcyUzRDMwKS5pbWFnZXMlNUIwJTVE",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionXLPipeline
<span class="hljs-keyword">import</span> torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.bfloat16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-comment"># Run the attention ops without SDPA.</span>
pipe.unet.set_default_attn_processor()
pipe.vae.set_default_attn_processor()

prompt = <span class="hljs-string">&quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;</span>
image = pipe(prompt, num_inference_steps=<span class="hljs-number">30</span>).images[<span class="hljs-number">0</span>]`,wrap:!1}}),v=new sl({props:{$$slots:{default:[ss]},$$scope:{ctx:w}}}),K=new J({props:{title:"SDPA",local:"sdpa",headingTag:"h2"}}),ee=new h({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXBpcGUlMjAlM0QlMjBTdGFibGVEaWZmdXNpb25YTFBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtYmFzZS0xLjAlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTBBKS50byglMjJjdWRhJTIyKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMkFzdHJvbmF1dCUyMGluJTIwYSUyMGp1bmdsZSUyQyUyMGNvbGQlMjBjb2xvciUyMHBhbGV0dGUlMkMlMjBtdXRlZCUyMGNvbG9ycyUyQyUyMGRldGFpbGVkJTJDJTIwOGslMjIlMEFpbWFnZSUyMCUzRCUyMHBpcGUocHJvbXB0JTJDJTIwbnVtX2luZmVyZW5jZV9zdGVwcyUzRDMwKS5pbWFnZXMlNUIwJTVE",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionXLPipeline
<span class="hljs-keyword">import</span> torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.bfloat16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

prompt = <span class="hljs-string">&quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;</span>
image = pipe(prompt, num_inference_steps=<span class="hljs-number">30</span>).images[<span class="hljs-number">0</span>]`,wrap:!1}}),le=new J({props:{title:"torch.compile",local:"torchcompile",headingTag:"h2"}}),ne=new h({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXRvcmNoLl9pbmR1Y3Rvci5jb25maWcuY29udl8xeDFfYXNfbW0lMjAlM0QlMjBUcnVlJTBBdG9yY2guX2luZHVjdG9yLmNvbmZpZy5jb29yZGluYXRlX2Rlc2NlbnRfdHVuaW5nJTIwJTNEJTIwVHJ1ZSUwQXRvcmNoLl9pbmR1Y3Rvci5jb25maWcuZXBpbG9ndWVfZnVzaW9uJTIwJTNEJTIwRmFsc2UlMEF0b3JjaC5faW5kdWN0b3IuY29uZmlnLmNvb3JkaW5hdGVfZGVzY2VudF9jaGVja19hbGxfZGlyZWN0aW9ucyUyMCUzRCUyMFRydWU=",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionXLPipeline
<span class="hljs-keyword">import</span> torch

torch._inductor.config.conv_1x1_as_mm = <span class="hljs-literal">True</span>
torch._inductor.config.coordinate_descent_tuning = <span class="hljs-literal">True</span>
torch._inductor.config.epilogue_fusion = <span class="hljs-literal">False</span>
torch._inductor.config.coordinate_descent_check_all_directions = <span class="hljs-literal">True</span>`,wrap:!1}}),ie=new h({props:{code:"cGlwZS51bmV0LnRvKG1lbW9yeV9mb3JtYXQlM0R0b3JjaC5jaGFubmVsc19sYXN0KSUwQXBpcGUudmFlLnRvKG1lbW9yeV9mb3JtYXQlM0R0b3JjaC5jaGFubmVsc19sYXN0KQ==",highlighted:`pipe.unet.to(memory_format=torch.channels_last)
pipe.vae.to(memory_format=torch.channels_last)`,wrap:!1}}),oe=new h({props:{code:"JTIzJTIwQ29tcGlsZSUyMHRoZSUyMFVOZXQlMjBhbmQlMjBWQUUuJTBBcGlwZS51bmV0JTIwJTNEJTIwdG9yY2guY29tcGlsZShwaXBlLnVuZXQlMkMlMjBtb2RlJTNEJTIybWF4LWF1dG90dW5lJTIyJTJDJTIwZnVsbGdyYXBoJTNEVHJ1ZSklMEFwaXBlLnZhZS5kZWNvZGUlMjAlM0QlMjB0b3JjaC5jb21waWxlKHBpcGUudmFlLmRlY29kZSUyQyUyMG1vZGUlM0QlMjJtYXgtYXV0b3R1bmUlMjIlMkMlMjBmdWxsZ3JhcGglM0RUcnVlKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMkFzdHJvbmF1dCUyMGluJTIwYSUyMGp1bmdsZSUyQyUyMGNvbGQlMjBjb2xvciUyMHBhbGV0dGUlMkMlMjBtdXRlZCUyMGNvbG9ycyUyQyUyMGRldGFpbGVkJTJDJTIwOGslMjIlMEElMEElMjMlMjBGaXJzdCUyMGNhbGwlMjB0byUyMCU2MHBpcGUlNjAlMjBpcyUyMHNsb3clMkMlMjBzdWJzZXF1ZW50JTIwb25lcyUyMGFyZSUyMGZhc3Rlci4lMEFpbWFnZSUyMCUzRCUyMHBpcGUocHJvbXB0JTJDJTIwbnVtX2luZmVyZW5jZV9zdGVwcyUzRDMwKS5pbWFnZXMlNUIwJTVE",highlighted:`<span class="hljs-comment"># Compile the UNet and VAE.</span>
pipe.unet = torch.<span class="hljs-built_in">compile</span>(pipe.unet, mode=<span class="hljs-string">&quot;max-autotune&quot;</span>, fullgraph=<span class="hljs-literal">True</span>)
pipe.vae.decode = torch.<span class="hljs-built_in">compile</span>(pipe.vae.decode, mode=<span class="hljs-string">&quot;max-autotune&quot;</span>, fullgraph=<span class="hljs-literal">True</span>)

prompt = <span class="hljs-string">&quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;</span>

<span class="hljs-comment"># First call to \`pipe\` is slow, subsequent ones are faster.</span>
image = pipe(prompt, num_inference_steps=<span class="hljs-number">30</span>).images[<span class="hljs-number">0</span>]`,wrap:!1}}),_=new sl({props:{warning:!1,$$slots:{default:[ns]},$$scope:{ctx:w}}}),me=new J({props:{title:"Prevent graph breaks",local:"prevent-graph-breaks",headingTag:"h3"}}),Me=new h({props:{code:"LSUyMGxhdGVudHMlMjAlM0QlMjB1bmV0KCUwQS0lMjAlMjAlMjBsYXRlbnRzJTJDJTIwdGltZXN0ZXAlM0R0aW1lc3RlcCUyQyUyMGVuY29kZXJfaGlkZGVuX3N0YXRlcyUzRHByb21wdF9lbWJlZHMlMEEtKS5zYW1wbGUlMEElMEElMkIlMjBsYXRlbnRzJTIwJTNEJTIwdW5ldCglMEElMkIlMjAlMjAlMjBsYXRlbnRzJTJDJTIwdGltZXN0ZXAlM0R0aW1lc3RlcCUyQyUyMGVuY29kZXJfaGlkZGVuX3N0YXRlcyUzRHByb21wdF9lbWJlZHMlMkMlMjByZXR1cm5fZGljdCUzREZhbHNlJTBBJTJCKSU1QjAlNUQ=",highlighted:`<span class="hljs-deletion">- latents = unet(</span>
<span class="hljs-deletion">-   latents, timestep=timestep, encoder_hidden_states=prompt_embeds</span>
<span class="hljs-deletion">-).sample</span>

<span class="hljs-addition">+ latents = unet(</span>
<span class="hljs-addition">+   latents, timestep=timestep, encoder_hidden_states=prompt_embeds, return_dict=False</span>
<span class="hljs-addition">+)[0]</span>`,wrap:!1}}),fe=new J({props:{title:"Remove GPU sync after compilation",local:"remove-gpu-sync-after-compilation",headingTag:"h3"}}),he=new J({props:{title:"Combine the attention block’s projection matrices",local:"combine-the-attention-blocks-projection-matrices",headingTag:"h2"}}),Je=new h({props:{code:"cGlwZS5mdXNlX3Frdl9wcm9qZWN0aW9ucygp",highlighted:"pipe.fuse_qkv_projections()",wrap:!1}}),G=new sl({props:{warning:!0,$$slots:{default:[as]},$$scope:{ctx:w}}}),Te=new J({props:{title:"Dynamic quantization",local:"dynamic-quantization",headingTag:"h2"}}),$e=new h({props:{code:"ZnJvbSUyMGRpZmZ1c2VycyUyMGltcG9ydCUyMFN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQSUyMyUyME5vdGljZSUyMHRoZSUyMHR3byUyMG5ldyUyMGZsYWdzJTIwYXQlMjB0aGUlMjBlbmQuJTBBdG9yY2guX2luZHVjdG9yLmNvbmZpZy5jb252XzF4MV9hc19tbSUyMCUzRCUyMFRydWUlMEF0b3JjaC5faW5kdWN0b3IuY29uZmlnLmNvb3JkaW5hdGVfZGVzY2VudF90dW5pbmclMjAlM0QlMjBUcnVlJTBBdG9yY2guX2luZHVjdG9yLmNvbmZpZy5lcGlsb2d1ZV9mdXNpb24lMjAlM0QlMjBGYWxzZSUwQXRvcmNoLl9pbmR1Y3Rvci5jb25maWcuY29vcmRpbmF0ZV9kZXNjZW50X2NoZWNrX2FsbF9kaXJlY3Rpb25zJTIwJTNEJTIwVHJ1ZSUwQXRvcmNoLl9pbmR1Y3Rvci5jb25maWcuZm9yY2VfZnVzZV9pbnRfbW1fd2l0aF9tdWwlMjAlM0QlMjBUcnVlJTBBdG9yY2guX2luZHVjdG9yLmNvbmZpZy51c2VfbWl4ZWRfbW0lMjAlM0QlMjBUcnVl",highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionXLPipeline
<span class="hljs-keyword">import</span> torch

<span class="hljs-comment"># Notice the two new flags at the end.</span>
torch._inductor.config.conv_1x1_as_mm = <span class="hljs-literal">True</span>
torch._inductor.config.coordinate_descent_tuning = <span class="hljs-literal">True</span>
torch._inductor.config.epilogue_fusion = <span class="hljs-literal">False</span>
torch._inductor.config.coordinate_descent_check_all_directions = <span class="hljs-literal">True</span>
torch._inductor.config.force_fuse_int_mm_with_mul = <span class="hljs-literal">True</span>
torch._inductor.config.use_mixed_mm = <span class="hljs-literal">True</span>`,wrap:!1}}),_e=new h({props:{code:"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",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">dynamic_quant_filter_fn</span>(<span class="hljs-params">mod, *args</span>):
    <span class="hljs-keyword">return</span> (
        <span class="hljs-built_in">isinstance</span>(mod, torch.nn.Linear)
        <span class="hljs-keyword">and</span> mod.in_features &gt; <span class="hljs-number">16</span>
        <span class="hljs-keyword">and</span> (mod.in_features, mod.out_features)
        <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [
            (<span class="hljs-number">1280</span>, <span class="hljs-number">640</span>),
            (<span class="hljs-number">1920</span>, <span class="hljs-number">1280</span>),
            (<span class="hljs-number">1920</span>, <span class="hljs-number">640</span>),
            (<span class="hljs-number">2048</span>, <span class="hljs-number">1280</span>),
            (<span class="hljs-number">2048</span>, <span class="hljs-number">2560</span>),
            (<span class="hljs-number">2560</span>, <span class="hljs-number">1280</span>),
            (<span class="hljs-number">256</span>, <span class="hljs-number">128</span>),
            (<span class="hljs-number">2816</span>, <span class="hljs-number">1280</span>),
            (<span class="hljs-number">320</span>, <span class="hljs-number">640</span>),
            (<span class="hljs-number">512</span>, <span class="hljs-number">1536</span>),
            (<span class="hljs-number">512</span>, <span class="hljs-number">256</span>),
            (<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),
            (<span class="hljs-number">640</span>, <span class="hljs-number">1280</span>),
            (<span class="hljs-number">640</span>, <span class="hljs-number">1920</span>),
            (<span class="hljs-number">640</span>, <span class="hljs-number">320</span>),
            (<span class="hljs-number">640</span>, <span class="hljs-number">5120</span>),
            (<span class="hljs-number">640</span>, <span class="hljs-number">640</span>),
            (<span class="hljs-number">960</span>, <span class="hljs-number">320</span>),
            (<span class="hljs-number">960</span>, <span class="hljs-number">640</span>),
        ]
    )


<span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_filter_fn</span>(<span class="hljs-params">mod, *args</span>):
    <span class="hljs-keyword">return</span> (
        <span class="hljs-built_in">isinstance</span>(mod, torch.nn.Conv2d) <span class="hljs-keyword">and</span> mod.kernel_size == (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-keyword">and</span> <span class="hljs-number">128</span> <span class="hljs-keyword">in</span> [mod.in_channels, mod.out_channels]
    )`,wrap:!1}}),Ge=new h({props:{code:"JTIzJTIwU0RQQSUyMCUyQiUyMGJmbG9hdDE2LiUwQXBpcGUlMjAlM0QlMjBTdGFibGVEaWZmdXNpb25YTFBpcGVsaW5lLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtYmFzZS0xLjAlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTBBKS50byglMjJjdWRhJTIyKSUwQSUwQSUyMyUyMENvbWJpbmUlMjBhdHRlbnRpb24lMjBwcm9qZWN0aW9uJTIwbWF0cmljZXMuJTBBcGlwZS5mdXNlX3Frdl9wcm9qZWN0aW9ucygpJTBBJTBBJTIzJTIwQ2hhbmdlJTIwdGhlJTIwbWVtb3J5JTIwbGF5b3V0LiUwQXBpcGUudW5ldC50byhtZW1vcnlfZm9ybWF0JTNEdG9yY2guY2hhbm5lbHNfbGFzdCklMEFwaXBlLnZhZS50byhtZW1vcnlfZm9ybWF0JTNEdG9yY2guY2hhbm5lbHNfbGFzdCk=",highlighted:`<span class="hljs-comment"># SDPA + bfloat16.</span>
pipe = StableDiffusionXLPipeline.from_pretrained(
    <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>, torch_dtype=torch.bfloat16
).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-comment"># Combine attention projection matrices.</span>
pipe.fuse_qkv_projections()

<span class="hljs-comment"># Change the memory layout.</span>
pipe.unet.to(memory_format=torch.channels_last)
pipe.vae.to(memory_format=torch.channels_last)`,wrap:!1}}),Be=new h({props:{code:"ZnJvbSUyMHRvcmNoYW8lMjBpbXBvcnQlMjBzd2FwX2NvbnYyZF8xeDFfdG9fbGluZWFyJTBBJTBBc3dhcF9jb252MmRfMXgxX3RvX2xpbmVhcihwaXBlLnVuZXQlMkMlMjBjb252X2ZpbHRlcl9mbiklMEFzd2FwX2NvbnYyZF8xeDFfdG9fbGluZWFyKHBpcGUudmFlJTJDJTIwY29udl9maWx0ZXJfZm4p",highlighted:`<span class="hljs-keyword">from</span> torchao <span class="hljs-keyword">import</span> swap_conv2d_1x1_to_linear

swap_conv2d_1x1_to_linear(pipe.unet, conv_filter_fn)
swap_conv2d_1x1_to_linear(pipe.vae, conv_filter_fn)`,wrap:!1}}),ke=new h({props:{code:"ZnJvbSUyMHRvcmNoYW8lMjBpbXBvcnQlMjBhcHBseV9keW5hbWljX3F1YW50JTBBJTBBYXBwbHlfZHluYW1pY19xdWFudChwaXBlLnVuZXQlMkMlMjBkeW5hbWljX3F1YW50X2ZpbHRlcl9mbiklMEFhcHBseV9keW5hbWljX3F1YW50KHBpcGUudmFlJTJDJTIwZHluYW1pY19xdWFudF9maWx0ZXJfZm4p",highlighted:`<span class="hljs-keyword">from</span> torchao <span class="hljs-keyword">import</span> apply_dynamic_quant

apply_dynamic_quant(pipe.unet, dynamic_quant_filter_fn)
apply_dynamic_quant(pipe.vae, dynamic_quant_filter_fn)`,wrap:!1}}),We=new h({props:{code:"cGlwZS51bmV0JTIwJTNEJTIwdG9yY2guY29tcGlsZShwaXBlLnVuZXQlMkMlMjBtb2RlJTNEJTIybWF4LWF1dG90dW5lJTIyJTJDJTIwZnVsbGdyYXBoJTNEVHJ1ZSklMEFwaXBlLnZhZS5kZWNvZGUlMjAlM0QlMjB0b3JjaC5jb21waWxlKHBpcGUudmFlLmRlY29kZSUyQyUyMG1vZGUlM0QlMjJtYXgtYXV0b3R1bmUlMjIlMkMlMjBmdWxsZ3JhcGglM0RUcnVlKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMkFzdHJvbmF1dCUyMGluJTIwYSUyMGp1bmdsZSUyQyUyMGNvbGQlMjBjb2xvciUyMHBhbGV0dGUlMkMlMjBtdXRlZCUyMGNvbG9ycyUyQyUyMGRldGFpbGVkJTJDJTIwOGslMjIlMEFpbWFnZSUyMCUzRCUyMHBpcGUocHJvbXB0JTJDJTIwbnVtX2luZmVyZW5jZV9zdGVwcyUzRDMwKS5pbWFnZXMlNUIwJTVE",highlighted:`pipe.unet = torch.<span class="hljs-built_in">compile</span>(pipe.unet, mode=<span class="hljs-string">&quot;max-autotune&quot;</span>, fullgraph=<span class="hljs-literal">True</span>)
pipe.vae.decode = torch.<span class="hljs-built_in">compile</span>(pipe.vae.decode, mode=<span class="hljs-string">&quot;max-autotune&quot;</span>, fullgraph=<span class="hljs-literal">True</span>)

prompt = <span class="hljs-string">&quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;</span>
image = pipe(prompt, num_inference_steps=<span class="hljs-number">30</span>).images[<span class="hljs-number">0</span>]`,wrap:!1}}),Xe=new ts({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md"}}),{c(){r=i("meta"),b=n(),y=i("p"),j=n(),c(A.$$.fragment),Fe=n(),k=i("p"),k.innerHTML=al,Ve=n(),x=i("p"),x.innerHTML=il,He=n(),W=i("p"),W.textContent=pl,Qe=n(),c(S.$$.fragment),Ne=n(),X=i("p"),X.textContent=ol,Ee=n(),c(R.$$.fragment),ze=n(),F=i("p"),F.innerHTML=rl,Le=n(),c(V.$$.fragment),Ye=n(),c(U.$$.fragment),De=n(),c(H.$$.fragment),Pe=n(),Q=i("p"),Q.innerHTML=cl,qe=n(),c(N.$$.fragment),Ke=n(),E=i("p"),E.textContent=ml,Oe=n(),T=i("div"),T.innerHTML=ul,et=n(),c(z.$$.fragment),tt=n(),L=i("p"),L.textContent=Ml,lt=n(),Y=i("ul"),Y.innerHTML=fl,st=n(),c(D.$$.fragment),nt=n(),P=i("p"),P.textContent=dl,at=n(),C=i("div"),C.innerHTML=yl,it=n(),c(v.$$.fragment),pt=n(),q=i("p"),q.innerHTML=hl,ot=n(),c(K.$$.fragment),rt=n(),O=i("p"),O.innerHTML=bl,ct=n(),c(ee.$$.fragment),mt=n(),te=i("p"),te.textContent=jl,ut=n(),$=i("div"),$.innerHTML=wl,Mt=n(),c(le.$$.fragment),ft=n(),se=i("p"),se.innerHTML=Jl,dt=n(),c(ne.$$.fragment),yt=n(),ae=i("p"),ae.textContent=Ul,ht=n(),c(ie.$$.fragment),bt=n(),pe=i("p"),pe.textContent=Tl,jt=n(),c(oe.$$.fragment),wt=n(),re=i("p"),re.innerHTML=Cl,Jt=n(),ce=i("p"),ce.textContent=vl,Ut=n(),g=i("div"),g.innerHTML=$l,Tt=n(),c(_.$$.fragment),Ct=n(),c(me.$$.fragment),vt=n(),ue=i("p"),ue.innerHTML=gl,$t=n(),c(Me.$$.fragment),gt=n(),c(fe.$$.fragment),_t=n(),de=i("p"),de.innerHTML=_l,Zt=n(),ye=i("p"),ye.innerHTML=Zl,Gt=n(),c(he.$$.fragment),It=n(),be=i("p"),be.textContent=Gl,Bt=n(),je=i("p"),je.textContent=Il,At=n(),we=i("p"),we.textContent=Bl,kt=n(),c(Je.$$.fragment),xt=n(),Ue=i("p"),Ue.textContent=Al,Wt=n(),Z=i("div"),Z.innerHTML=kl,St=n(),c(G.$$.fragment),Xt=n(),c(Te.$$.fragment),Rt=n(),Ce=i("p"),Ce.innerHTML=xl,Ft=n(),ve=i("p"),ve.textContent=Wl,Vt=n(),c($e.$$.fragment),Ht=n(),ge=i("p"),ge.innerHTML=Sl,Qt=n(),c(_e.$$.fragment),Nt=n(),Ze=i("p"),Ze.textContent=Xl,Et=n(),c(Ge.$$.fragment),zt=n(),Ie=i("p"),Ie.textContent=Rl,Lt=n(),c(Be.$$.fragment),Yt=n(),Ae=i("p"),Ae.textContent=Fl,Dt=n(),c(ke.$$.fragment),Pt=n(),xe=i("p"),xe.textContent=Vl,qt=n(),c(We.$$.fragment),Kt=n(),Se=i("p"),Se.textContent=Hl,Ot=n(),I=i("div"),I.innerHTML=Ql,el=n(),c(Xe.$$.fragment),tl=n(),Re=i("p"),this.h()},l(e){const t=Ol("svelte-u9bgzb",document.head);r=p(t,"META",{name:!0,content:!0}),t.forEach(l),b=a(e),y=p(e,"P",{}),Yl(y).forEach(l),j=a(e),m(A.$$.fragment,e),Fe=a(e),k=p(e,"P",{"data-svelte-h":!0}),o(k)!=="svelte-6v913i"&&(k.innerHTML=al),Ve=a(e),x=p(e,"P",{"data-svelte-h":!0}),o(x)!=="svelte-x8vtym"&&(x.innerHTML=il),He=a(e),W=p(e,"P",{"data-svelte-h":!0}),o(W)!=="svelte-1scvl0y"&&(W.textContent=pl),Qe=a(e),m(S.$$.fragment,e),Ne=a(e),X=p(e,"P",{"data-svelte-h":!0}),o(X)!=="svelte-1s5viqc"&&(X.textContent=ol),Ee=a(e),m(R.$$.fragment,e),ze=a(e),F=p(e,"P",{"data-svelte-h":!0}),o(F)!=="svelte-22cx8u"&&(F.innerHTML=rl),Le=a(e),m(V.$$.fragment,e),Ye=a(e),m(U.$$.fragment,e),De=a(e),m(H.$$.fragment,e),Pe=a(e),Q=p(e,"P",{"data-svelte-h":!0}),o(Q)!=="svelte-8fcr93"&&(Q.innerHTML=cl),qe=a(e),m(N.$$.fragment,e),Ke=a(e),E=p(e,"P",{"data-svelte-h":!0}),o(E)!=="svelte-1owd7rf"&&(E.textContent=ml),Oe=a(e),T=p(e,"DIV",{class:!0,"data-svelte-h":!0}),o(T)!=="svelte-soduuj"&&(T.innerHTML=ul),et=a(e),m(z.$$.fragment,e),tt=a(e),L=p(e,"P",{"data-svelte-h":!0}),o(L)!=="svelte-h6fwbn"&&(L.textContent=Ml),lt=a(e),Y=p(e,"UL",{"data-svelte-h":!0}),o(Y)!=="svelte-n003gd"&&(Y.innerHTML=fl),st=a(e),m(D.$$.fragment,e),nt=a(e),P=p(e,"P",{"data-svelte-h":!0}),o(P)!=="svelte-1r8kz3j"&&(P.textContent=dl),at=a(e),C=p(e,"DIV",{class:!0,"data-svelte-h":!0}),o(C)!=="svelte-18j4rd2"&&(C.innerHTML=yl),it=a(e),m(v.$$.fragment,e),pt=a(e),q=p(e,"P",{"data-svelte-h":!0}),o(q)!=="svelte-1mvuf9k"&&(q.innerHTML=hl),ot=a(e),m(K.$$.fragment,e),rt=a(e),O=p(e,"P",{"data-svelte-h":!0}),o(O)!=="svelte-xg8cbh"&&(O.innerHTML=bl),ct=a(e),m(ee.$$.fragment,e),mt=a(e),te=p(e,"P",{"data-svelte-h":!0}),o(te)!=="svelte-ys3vnp"&&(te.textContent=jl),ut=a(e),$=p(e,"DIV",{class:!0,"data-svelte-h":!0}),o($)!=="svelte-17d4anp"&&($.innerHTML=wl),Mt=a(e),m(le.$$.fragment,e),ft=a(e),se=p(e,"P",{"data-svelte-h":!0}),o(se)!=="svelte-5l299y"&&(se.innerHTML=Jl),dt=a(e),m(ne.$$.fragment,e),yt=a(e),ae=p(e,"P",{"data-svelte-h":!0}),o(ae)!=="svelte-pjywod"&&(ae.textContent=Ul),ht=a(e),m(ie.$$.fragment,e),bt=a(e),pe=p(e,"P",{"data-svelte-h":!0}),o(pe)!=="svelte-o941ig"&&(pe.textContent=Tl),jt=a(e),m(oe.$$.fragment,e),wt=a(e),re=p(e,"P",{"data-svelte-h":!0}),o(re)!=="svelte-djy428"&&(re.innerHTML=Cl),Jt=a(e),ce=p(e,"P",{"data-svelte-h":!0}),o(ce)!=="svelte-5oajk3"&&(ce.textContent=vl),Ut=a(e),g=p(e,"DIV",{class:!0,"data-svelte-h":!0}),o(g)!=="svelte-149zdtk"&&(g.innerHTML=$l),Tt=a(e),m(_.$$.fragment,e),Ct=a(e),m(me.$$.fragment,e),vt=a(e),ue=p(e,"P",{"data-svelte-h":!0}),o(ue)!=="svelte-1m1ded6"&&(ue.innerHTML=gl),$t=a(e),m(Me.$$.fragment,e),gt=a(e),m(fe.$$.fragment,e),_t=a(e),de=p(e,"P",{"data-svelte-h":!0}),o(de)!=="svelte-16jpy2z"&&(de.innerHTML=_l),Zt=a(e),ye=p(e,"P",{"data-svelte-h":!0}),o(ye)!=="svelte-8sfwtq"&&(ye.innerHTML=Zl),Gt=a(e),m(he.$$.fragment,e),It=a(e),be=p(e,"P",{"data-svelte-h":!0}),o(be)!=="svelte-ewv31z"&&(be.textContent=Gl),Bt=a(e),je=p(e,"P",{"data-svelte-h":!0}),o(je)!=="svelte-1j310cj"&&(je.textContent=Il),At=a(e),we=p(e,"P",{"data-svelte-h":!0}),o(we)!=="svelte-jomo87"&&(we.textContent=Bl),kt=a(e),m(Je.$$.fragment,e),xt=a(e),Ue=p(e,"P",{"data-svelte-h":!0}),o(Ue)!=="svelte-1d7kejb"&&(Ue.textContent=Al),Wt=a(e),Z=p(e,"DIV",{class:!0,"data-svelte-h":!0}),o(Z)!=="svelte-468rrb"&&(Z.innerHTML=kl),St=a(e),m(G.$$.fragment,e),Xt=a(e),m(Te.$$.fragment,e),Rt=a(e),Ce=p(e,"P",{"data-svelte-h":!0}),o(Ce)!=="svelte-1wvr34e"&&(Ce.innerHTML=xl),Ft=a(e),ve=p(e,"P",{"data-svelte-h":!0}),o(ve)!=="svelte-gvi9jq"&&(ve.textContent=Wl),Vt=a(e),m($e.$$.fragment,e),Ht=a(e),ge=p(e,"P",{"data-svelte-h":!0}),o(ge)!=="svelte-ha2qym"&&(ge.innerHTML=Sl),Qt=a(e),m(_e.$$.fragment,e),Nt=a(e),Ze=p(e,"P",{"data-svelte-h":!0}),o(Ze)!=="svelte-1mxk5md"&&(Ze.textContent=Xl),Et=a(e),m(Ge.$$.fragment,e),zt=a(e),Ie=p(e,"P",{"data-svelte-h":!0}),o(Ie)!=="svelte-1e1ng4n"&&(Ie.textContent=Rl),Lt=a(e),m(Be.$$.fragment,e),Yt=a(e),Ae=p(e,"P",{"data-svelte-h":!0}),o(Ae)!=="svelte-13vdbpw"&&(Ae.textContent=Fl),Dt=a(e),m(ke.$$.fragment,e),Pt=a(e),xe=p(e,"P",{"data-svelte-h":!0}),o(xe)!=="svelte-145l8jb"&&(xe.textContent=Vl),qt=a(e),m(We.$$.fragment,e),Kt=a(e),Se=p(e,"P",{"data-svelte-h":!0}),o(Se)!=="svelte-od9x3v"&&(Se.textContent=Hl),Ot=a(e),I=p(e,"DIV",{class:!0,"data-svelte-h":!0}),o(I)!=="svelte-czihiq"&&(I.innerHTML=Ql),el=a(e),m(Xe.$$.fragment,e),tl=a(e),Re=p(e,"P",{}),Yl(Re).forEach(l),this.h()},h(){B(r,"name","hf:doc:metadata"),B(r,"content",ps),B(T,"class","flex justify-center"),B(C,"class","flex justify-center"),B($,"class","flex justify-center"),B(g,"class","flex justify-center"),B(Z,"class","flex justify-center"),B(I,"class","flex justify-center")},m(e,t){es(document.head,r),s(e,b,t),s(e,y,t),s(e,j,t),u(A,e,t),s(e,Fe,t),s(e,k,t),s(e,Ve,t),s(e,x,t),s(e,He,t),s(e,W,t),s(e,Qe,t),u(S,e,t),s(e,Ne,t),s(e,X,t),s(e,Ee,t),u(R,e,t),s(e,ze,t),s(e,F,t),s(e,Le,t),u(V,e,t),s(e,Ye,t),u(U,e,t),s(e,De,t),u(H,e,t),s(e,Pe,t),s(e,Q,t),s(e,qe,t),u(N,e,t),s(e,Ke,t),s(e,E,t),s(e,Oe,t),s(e,T,t),s(e,et,t),u(z,e,t),s(e,tt,t),s(e,L,t),s(e,lt,t),s(e,Y,t),s(e,st,t),u(D,e,t),s(e,nt,t),s(e,P,t),s(e,at,t),s(e,C,t),s(e,it,t),u(v,e,t),s(e,pt,t),s(e,q,t),s(e,ot,t),u(K,e,t),s(e,rt,t),s(e,O,t),s(e,ct,t),u(ee,e,t),s(e,mt,t),s(e,te,t),s(e,ut,t),s(e,$,t),s(e,Mt,t),u(le,e,t),s(e,ft,t),s(e,se,t),s(e,dt,t),u(ne,e,t),s(e,yt,t),s(e,ae,t),s(e,ht,t),u(ie,e,t),s(e,bt,t),s(e,pe,t),s(e,jt,t),u(oe,e,t),s(e,wt,t),s(e,re,t),s(e,Jt,t),s(e,ce,t),s(e,Ut,t),s(e,g,t),s(e,Tt,t),u(_,e,t),s(e,Ct,t),u(me,e,t),s(e,vt,t),s(e,ue,t),s(e,$t,t),u(Me,e,t),s(e,gt,t),u(fe,e,t),s(e,_t,t),s(e,de,t),s(e,Zt,t),s(e,ye,t),s(e,Gt,t),u(he,e,t),s(e,It,t),s(e,be,t),s(e,Bt,t),s(e,je,t),s(e,At,t),s(e,we,t),s(e,kt,t),u(Je,e,t),s(e,xt,t),s(e,Ue,t),s(e,Wt,t),s(e,Z,t),s(e,St,t),u(G,e,t),s(e,Xt,t),u(Te,e,t),s(e,Rt,t),s(e,Ce,t),s(e,Ft,t),s(e,ve,t),s(e,Vt,t),u($e,e,t),s(e,Ht,t),s(e,ge,t),s(e,Qt,t),u(_e,e,t),s(e,Nt,t),s(e,Ze,t),s(e,Et,t),u(Ge,e,t),s(e,zt,t),s(e,Ie,t),s(e,Lt,t),u(Be,e,t),s(e,Yt,t),s(e,Ae,t),s(e,Dt,t),u(ke,e,t),s(e,Pt,t),s(e,xe,t),s(e,qt,t),u(We,e,t),s(e,Kt,t),s(e,Se,t),s(e,Ot,t),s(e,I,t),s(e,el,t),u(Xe,e,t),s(e,tl,t),s(e,Re,t),ll=!0},p(e,[t]){const Nl={};t&2&&(Nl.$$scope={dirty:t,ctx:e}),U.$set(Nl);const El={};t&2&&(El.$$scope={dirty:t,ctx:e}),v.$set(El);const zl={};t&2&&(zl.$$scope={dirty:t,ctx:e}),_.$set(zl);const Ll={};t&2&&(Ll.$$scope={dirty:t,ctx:e}),G.$set(Ll)},i(e){ll||(M(A.$$.fragment,e),M(S.$$.fragment,e),M(R.$$.fragment,e),M(V.$$.fragment,e),M(U.$$.fragment,e),M(H.$$.fragment,e),M(N.$$.fragment,e),M(z.$$.fragment,e),M(D.$$.fragment,e),M(v.$$.fragment,e),M(K.$$.fragment,e),M(ee.$$.fragment,e),M(le.$$.fragment,e),M(ne.$$.fragment,e),M(ie.$$.fragment,e),M(oe.$$.fragment,e),M(_.$$.fragment,e),M(me.$$.fragment,e),M(Me.$$.fragment,e),M(fe.$$.fragment,e),M(he.$$.fragment,e),M(Je.$$.fragment,e),M(G.$$.fragment,e),M(Te.$$.fragment,e),M($e.$$.fragment,e),M(_e.$$.fragment,e),M(Ge.$$.fragment,e),M(Be.$$.fragment,e),M(ke.$$.fragment,e),M(We.$$.fragment,e),M(Xe.$$.fragment,e),ll=!0)},o(e){f(A.$$.fragment,e),f(S.$$.fragment,e),f(R.$$.fragment,e),f(V.$$.fragment,e),f(U.$$.fragment,e),f(H.$$.fragment,e),f(N.$$.fragment,e),f(z.$$.fragment,e),f(D.$$.fragment,e),f(v.$$.fragment,e),f(K.$$.fragment,e),f(ee.$$.fragment,e),f(le.$$.fragment,e),f(ne.$$.fragment,e),f(ie.$$.fragment,e),f(oe.$$.fragment,e),f(_.$$.fragment,e),f(me.$$.fragment,e),f(Me.$$.fragment,e),f(fe.$$.fragment,e),f(he.$$.fragment,e),f(Je.$$.fragment,e),f(G.$$.fragment,e),f(Te.$$.fragment,e),f($e.$$.fragment,e),f(_e.$$.fragment,e),f(Ge.$$.fragment,e),f(Be.$$.fragment,e),f(ke.$$.fragment,e),f(We.$$.fragment,e),f(Xe.$$.fragment,e),ll=!1},d(e){e&&(l(b),l(y),l(j),l(Fe),l(k),l(Ve),l(x),l(He),l(W),l(Qe),l(Ne),l(X),l(Ee),l(ze),l(F),l(Le),l(Ye),l(De),l(Pe),l(Q),l(qe),l(Ke),l(E),l(Oe),l(T),l(et),l(tt),l(L),l(lt),l(Y),l(st),l(nt),l(P),l(at),l(C),l(it),l(pt),l(q),l(ot),l(rt),l(O),l(ct),l(mt),l(te),l(ut),l($),l(Mt),l(ft),l(se),l(dt),l(yt),l(ae),l(ht),l(bt),l(pe),l(jt),l(wt),l(re),l(Jt),l(ce),l(Ut),l(g),l(Tt),l(Ct),l(vt),l(ue),l($t),l(gt),l(_t),l(de),l(Zt),l(ye),l(Gt),l(It),l(be),l(Bt),l(je),l(At),l(we),l(kt),l(xt),l(Ue),l(Wt),l(Z),l(St),l(Xt),l(Rt),l(Ce),l(Ft),l(ve),l(Vt),l(Ht),l(ge),l(Qt),l(Nt),l(Ze),l(Et),l(zt),l(Ie),l(Lt),l(Yt),l(Ae),l(Dt),l(Pt),l(xe),l(qt),l(Kt),l(Se),l(Ot),l(I),l(el),l(tl),l(Re)),l(r),d(A,e),d(S,e),d(R,e),d(V,e),d(U,e),d(H,e),d(N,e),d(z,e),d(D,e),d(v,e),d(K,e),d(ee,e),d(le,e),d(ne,e),d(ie,e),d(oe,e),d(_,e),d(me,e),d(Me,e),d(fe,e),d(he,e),d(Je,e),d(G,e),d(Te,e),d($e,e),d(_e,e),d(Ge,e),d(Be,e),d(ke,e),d(We,e),d(Xe,e)}}}const ps='{"title":"Accelerate inference of text-to-image diffusion models","local":"accelerate-inference-of-text-to-image-diffusion-models","sections":[{"title":"Baseline","local":"baseline","sections":[],"depth":2},{"title":"bfloat16","local":"bfloat16","sections":[],"depth":2},{"title":"SDPA","local":"sdpa","sections":[],"depth":2},{"title":"torch.compile","local":"torchcompile","sections":[{"title":"Prevent graph breaks","local":"prevent-graph-breaks","sections":[],"depth":3},{"title":"Remove GPU sync after compilation","local":"remove-gpu-sync-after-compilation","sections":[],"depth":3}],"depth":2},{"title":"Combine the attention block’s projection matrices","local":"combine-the-attention-blocks-projection-matrices","sections":[],"depth":2},{"title":"Dynamic quantization","local":"dynamic-quantization","sections":[],"depth":2}],"depth":1}';function os(w){return Pl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fs extends ql{constructor(r){super(),Kl(this,r,os,is,Dl,{})}}export{fs as component};
