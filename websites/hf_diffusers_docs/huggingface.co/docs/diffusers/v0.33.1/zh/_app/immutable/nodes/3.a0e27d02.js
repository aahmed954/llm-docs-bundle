import{s as K,n as X,o as tt}from"../chunks/scheduler.412302f6.js";import{S as et,i as it,g as d,s as n,r as H,A as at,h as r,f as i,c as o,j as Z,x,u as U,k as A,y as nt,a,v as R,d as V,t as q,w as z}from"../chunks/index.f36f02f5.js";import{H as J,E as ot}from"../chunks/index.df723ca6.js";function dt(B){let l,I,v,y,s,O='<br/> <img src="https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg" width="400"/> <br/>',G,p,D,u,F=`ğŸ¤— Diffusers æ˜¯ä¸€ä¸ªå€¼å¾—é¦–é€‰ç”¨äºç”Ÿæˆå›¾åƒã€éŸ³é¢‘ç”šè‡³ 3D åˆ†å­ç»“æ„çš„ï¼Œæœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åº“ã€‚
æ— è®ºæ‚¨æ˜¯åœ¨å¯»æ‰¾ç®€å•çš„æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œè¿˜æ˜¯æƒ³è®­ç»ƒè‡ªå·±çš„æ‰©æ•£æ¨¡å‹ï¼ŒğŸ¤— Diffusers è¿™ä¸€æ¨¡å—åŒ–å·¥å…·ç®±éƒ½èƒ½å¯¹å…¶æä¾›æ”¯æŒã€‚
æœ¬åº“çš„è®¾è®¡æ›´åé‡äº<a href="conceptual/philosophy#usability-over-performance">å¯ç”¨è€Œéé«˜æ€§èƒ½</a>ã€<a href="conceptual/philosophy#simple-over-easy">ç®€æ˜è€Œéç®€å•</a>ä»¥åŠ<a href="conceptual/philosophy#tweakable-contributorfriendly-over-abstraction">æ˜“ç”¨è€ŒéæŠ½è±¡</a>ã€‚`,T,g,j="æœ¬åº“åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š",M,h,Q='<li>æœ€å…ˆè¿›çš„æ‰©æ•£ç®¡é“ <a href="api/pipelines/overview">diffusion pipelines</a>ï¼Œåªéœ€å‡ è¡Œä»£ç å³å¯è¿›è¡Œæ¨ç†ã€‚</li> <li>å¯äº¤æ›¿ä½¿ç”¨çš„å„ç§å™ªå£°è°ƒåº¦å™¨ <a href="api/schedulers/overview">noise schedulers</a>ï¼Œç”¨äºå¹³è¡¡ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ã€‚</li> <li>é¢„è®­ç»ƒæ¨¡å‹ <a href="api/models">models</a>ï¼Œå¯ä½œä¸ºæ„å»ºæ¨¡å—ï¼Œå¹¶ä¸è°ƒåº¦ç¨‹åºç»“åˆä½¿ç”¨ï¼Œæ¥åˆ›å»ºæ‚¨è‡ªå·±çš„ç«¯åˆ°ç«¯æ‰©æ•£ç³»ç»Ÿã€‚</li>',S,f,N='<div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5"><a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/tutorial_overview"><div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Tutorials</div> <p class="text-gray-700">Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you&#39;re using ğŸ¤— Diffusers for the first time!</p></a> <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./using-diffusers/loading_overview"><div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">How-to guides</div> <p class="text-gray-700">Practical guides for helping you load pipelines, models, and schedulers. You&#39;ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p></a> <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./conceptual/philosophy"><div class="w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Conceptual guides</div> <p class="text-gray-700">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p></a> <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./api/models"><div class="w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Reference</div> <p class="text-gray-700">Technical descriptions of how ğŸ¤— Diffusers classes and methods work.</p></a></div>',L,c,$,m,W="ä¸‹è¡¨æ±‡æ€»äº†å½“å‰æ‰€æœ‰å®˜æ–¹æ”¯æŒçš„pipelinesåŠå…¶å¯¹åº”çš„è®ºæ–‡.",C,b,Y='<thead><tr><th>ç®¡é“</th> <th>è®ºæ–‡/ä»“åº“</th> <th align="center">ä»»åŠ¡</th></tr></thead> <tbody><tr><td><a href="./api/pipelines/alt_diffusion">alt_diffusion</a></td> <td><a href="https://arxiv.org/abs/2211.06679" rel="nofollow">AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</a></td> <td align="center">Image-to-Image Text-Guided Generation</td></tr> <tr><td><a href="./api/pipelines/audio_diffusion">audio_diffusion</a></td> <td><a href="https://github.com/teticio/audio-diffusion.git" rel="nofollow">Audio Diffusion</a></td> <td align="center">Unconditional Audio Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/controlnet">controlnet</a></td> <td><a href="https://arxiv.org/abs/2302.05543" rel="nofollow">Adding Conditional Control to Text-to-Image Diffusion Models</a></td> <td align="center">Image-to-Image Text-Guided Generation</td></tr> <tr><td><a href="./api/pipelines/cycle_diffusion">cycle_diffusion</a></td> <td><a href="https://arxiv.org/abs/2210.05559" rel="nofollow">Unifying Diffusion Modelsâ€™ Latent Space, with Applications to CycleDiffusion and Guidance</a></td> <td align="center">Image-to-Image Text-Guided Generation</td></tr> <tr><td><a href="./api/pipelines/dance_diffusion">dance_diffusion</a></td> <td><a href="https://github.com/williamberman/diffusers.git" rel="nofollow">Dance Diffusion</a></td> <td align="center">Unconditional Audio Generation</td></tr> <tr><td><a href="./api/pipelines/ddpm">ddpm</a></td> <td><a href="https://arxiv.org/abs/2006.11239" rel="nofollow">Denoising Diffusion Probabilistic Models</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./api/pipelines/ddim">ddim</a></td> <td><a href="https://arxiv.org/abs/2010.02502" rel="nofollow">Denoising Diffusion Implicit Models</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./if">if</a></td> <td><a href="./api/pipelines/if"><strong>IF</strong></a></td> <td align="center">Image Generation</td></tr> <tr><td><a href="./if">if_img2img</a></td> <td><a href="./api/pipelines/if"><strong>IF</strong></a></td> <td align="center">Image-to-Image Generation</td></tr> <tr><td><a href="./if">if_inpainting</a></td> <td><a href="./api/pipelines/if"><strong>IF</strong></a></td> <td align="center">Image-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/latent_diffusion">latent_diffusion</a></td> <td><a href="https://arxiv.org/abs/2112.10752" rel="nofollow">High-Resolution Image Synthesis with Latent Diffusion Models</a></td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/latent_diffusion">latent_diffusion</a></td> <td><a href="https://arxiv.org/abs/2112.10752" rel="nofollow">High-Resolution Image Synthesis with Latent Diffusion Models</a></td> <td align="center">Super Resolution Image-to-Image</td></tr> <tr><td><a href="./api/pipelines/latent_diffusion_uncond">latent_diffusion_uncond</a></td> <td><a href="https://arxiv.org/abs/2112.10752" rel="nofollow">High-Resolution Image Synthesis with Latent Diffusion Models</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./api/pipelines/paint_by_example">paint_by_example</a></td> <td><a href="https://arxiv.org/abs/2211.13227" rel="nofollow">Paint by Example: Exemplar-based Image Editing with Diffusion Models</a></td> <td align="center">Image-Guided Image Inpainting</td></tr> <tr><td><a href="./api/pipelines/pndm">pndm</a></td> <td><a href="https://arxiv.org/abs/2202.09778" rel="nofollow">Pseudo Numerical Methods for Diffusion Models on Manifolds</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./api/pipelines/score_sde_ve">score_sde_ve</a></td> <td><a href="https://openreview.net/forum?id=PxTIG12RRHS" rel="nofollow">Score-Based Generative Modeling through Stochastic Differential Equations</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./api/pipelines/score_sde_vp">score_sde_vp</a></td> <td><a href="https://openreview.net/forum?id=PxTIG12RRHS" rel="nofollow">Score-Based Generative Modeling through Stochastic Differential Equations</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./api/pipelines/semantic_stable_diffusion">semantic_stable_diffusion</a></td> <td><a href="https://arxiv.org/abs/2301.12247" rel="nofollow">Semantic Guidance</a></td> <td align="center">Text-Guided Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/text2img">stable_diffusion_text2img</a></td> <td><a href="https://stability.ai/blog/stable-diffusion-public-release" rel="nofollow">Stable Diffusion</a></td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/img2img">stable_diffusion_img2img</a></td> <td><a href="https://stability.ai/blog/stable-diffusion-public-release" rel="nofollow">Stable Diffusion</a></td> <td align="center">Image-to-Image Text-Guided Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/inpaint">stable_diffusion_inpaint</a></td> <td><a href="https://stability.ai/blog/stable-diffusion-public-release" rel="nofollow">Stable Diffusion</a></td> <td align="center">Text-Guided Image Inpainting</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/panorama">stable_diffusion_panorama</a></td> <td><a href="https://multidiffusion.github.io/" rel="nofollow">MultiDiffusion</a></td> <td align="center">Text-to-Panorama Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/pix2pix">stable_diffusion_pix2pix</a></td> <td><a href="https://arxiv.org/abs/2211.09800" rel="nofollow">InstructPix2Pix: Learning to Follow Image Editing Instructions</a></td> <td align="center">Text-Guided Image Editing</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/pix2pix_zero">stable_diffusion_pix2pix_zero</a></td> <td><a href="https://pix2pixzero.github.io/" rel="nofollow">Zero-shot Image-to-Image Translation</a></td> <td align="center">Text-Guided Image Editing</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/attend_and_excite">stable_diffusion_attend_and_excite</a></td> <td><a href="https://arxiv.org/abs/2301.13826" rel="nofollow">Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</a></td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/self_attention_guidance">stable_diffusion_self_attention_guidance</a></td> <td><a href="https://arxiv.org/abs/2210.00939" rel="nofollow">Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</a></td> <td align="center">Text-to-Image Generation Unconditional Image Generation</td></tr> <tr><td><a href="./stable_diffusion/image_variation">stable_diffusion_image_variation</a></td> <td><a href="https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations" rel="nofollow">Stable Diffusion Image Variations</a></td> <td align="center">Image-to-Image Generation</td></tr> <tr><td><a href="./stable_diffusion/latent_upscale">stable_diffusion_latent_upscale</a></td> <td><a href="https://twitter.com/StabilityAI/status/1590531958815064065" rel="nofollow">Stable Diffusion Latent Upscaler</a></td> <td align="center">Text-Guided Super Resolution Image-to-Image</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion/model_editing">stable_diffusion_model_editing</a></td> <td><a href="https://time-diffusion.github.io/" rel="nofollow">Editing Implicit Assumptions in Text-to-Image Diffusion Models</a></td> <td align="center">Text-to-Image Model Editing</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion_2">stable_diffusion_2</a></td> <td><a href="https://stability.ai/blog/stable-diffusion-v2-release" rel="nofollow">Stable Diffusion 2</a></td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion_2">stable_diffusion_2</a></td> <td><a href="https://stability.ai/blog/stable-diffusion-v2-release" rel="nofollow">Stable Diffusion 2</a></td> <td align="center">Text-Guided Image Inpainting</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion_2">stable_diffusion_2</a></td> <td><a href="https://github.com/Stability-AI/stablediffusion#depth-conditional-stable-diffusion" rel="nofollow">Depth-Conditional Stable Diffusion</a></td> <td align="center">Depth-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion_2">stable_diffusion_2</a></td> <td><a href="https://stability.ai/blog/stable-diffusion-v2-release" rel="nofollow">Stable Diffusion 2</a></td> <td align="center">Text-Guided Super Resolution Image-to-Image</td></tr> <tr><td><a href="./api/pipelines/stable_diffusion_safe">stable_diffusion_safe</a></td> <td><a href="https://arxiv.org/abs/2211.05105" rel="nofollow">Safe Stable Diffusion</a></td> <td align="center">Text-Guided Generation</td></tr> <tr><td><a href="./stable_unclip">stable_unclip</a></td> <td>Stable unCLIP</td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./stable_unclip">stable_unclip</a></td> <td>Stable unCLIP</td> <td align="center">Image-to-Image Text-Guided Generation</td></tr> <tr><td><a href="./api/pipelines/stochastic_karras_ve">stochastic_karras_ve</a></td> <td><a href="https://arxiv.org/abs/2206.00364" rel="nofollow">Elucidating the Design Space of Diffusion-Based Generative Models</a></td> <td align="center">Unconditional Image Generation</td></tr> <tr><td><a href="./api/pipelines/text_to_video">text_to_video_sd</a></td> <td><a href="https://modelscope.cn/models/damo/text-to-video-synthesis/summary" rel="nofollow">Modelscopeâ€™s Text-to-video-synthesis Model in Open Domain</a></td> <td align="center">Text-to-Video Generation</td></tr> <tr><td><a href="./api/pipelines/unclip">unclip</a></td> <td><a href="https://arxiv.org/abs/2204.06125" rel="nofollow">Hierarchical Text-Conditional Image Generation with CLIP Latents</a>(implementation by <a href="https://github.com/kakaobrain/karlo" rel="nofollow">kakaobrain</a>)</td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/versatile_diffusion">versatile_diffusion</a></td> <td><a href="https://arxiv.org/abs/2211.08332" rel="nofollow">Versatile Diffusion: Text, Images and Variations All in One Diffusion Model</a></td> <td align="center">Text-to-Image Generation</td></tr> <tr><td><a href="./api/pipelines/versatile_diffusion">versatile_diffusion</a></td> <td><a href="https://arxiv.org/abs/2211.08332" rel="nofollow">Versatile Diffusion: Text, Images and Variations All in One Diffusion Model</a></td> <td align="center">Image Variations Generation</td></tr> <tr><td><a href="./api/pipelines/versatile_diffusion">versatile_diffusion</a></td> <td><a href="https://arxiv.org/abs/2211.08332" rel="nofollow">Versatile Diffusion: Text, Images and Variations All in One Diffusion Model</a></td> <td align="center">Dual Image and Text Guided Generation</td></tr> <tr><td><a href="./api/pipelines/vq_diffusion">vq_diffusion</a></td> <td><a href="https://arxiv.org/abs/2111.14822" rel="nofollow">Vector Quantized Diffusion Model for Text-to-Image Synthesis</a></td> <td align="center">Text-to-Image Generation</td></tr></tbody>',P,_,E,w,k;return p=new J({props:{title:"ğŸ§¨ Diffusers",local:"-diffusers",headingTag:"h1"}}),c=new J({props:{title:"ğŸ§¨ Diffusers pipelines",local:"-diffusers-pipelines",headingTag:"h2"}}),_=new ot({props:{source:"https://github.com/huggingface/diffusers/blob/main/docs/source/zh/index.md"}}),{c(){l=d("meta"),I=n(),v=d("p"),y=n(),s=d("p"),s.innerHTML=O,G=n(),H(p.$$.fragment),D=n(),u=d("p"),u.innerHTML=F,T=n(),g=d("p"),g.textContent=j,M=n(),h=d("ul"),h.innerHTML=Q,S=n(),f=d("div"),f.innerHTML=N,L=n(),H(c.$$.fragment),$=n(),m=d("p"),m.textContent=W,C=n(),b=d("table"),b.innerHTML=Y,P=n(),H(_.$$.fragment),E=n(),w=d("p"),this.h()},l(t){const e=at("svelte-u9bgzb",document.head);l=r(e,"META",{name:!0,content:!0}),e.forEach(i),I=o(t),v=r(t,"P",{}),Z(v).forEach(i),y=o(t),s=r(t,"P",{align:!0,"data-svelte-h":!0}),x(s)!=="svelte-aksdn0"&&(s.innerHTML=O),G=o(t),U(p.$$.fragment,t),D=o(t),u=r(t,"P",{"data-svelte-h":!0}),x(u)!=="svelte-7h3gty"&&(u.innerHTML=F),T=o(t),g=r(t,"P",{"data-svelte-h":!0}),x(g)!=="svelte-f0h4hq"&&(g.textContent=j),M=o(t),h=r(t,"UL",{"data-svelte-h":!0}),x(h)!=="svelte-qatawh"&&(h.innerHTML=Q),S=o(t),f=r(t,"DIV",{class:!0,"data-svelte-h":!0}),x(f)!=="svelte-1kh56sb"&&(f.innerHTML=N),L=o(t),U(c.$$.fragment,t),$=o(t),m=r(t,"P",{"data-svelte-h":!0}),x(m)!=="svelte-q0dceg"&&(m.textContent=W),C=o(t),b=r(t,"TABLE",{"data-svelte-h":!0}),x(b)!=="svelte-roh3l2"&&(b.innerHTML=Y),P=o(t),U(_.$$.fragment,t),E=o(t),w=r(t,"P",{}),Z(w).forEach(i),this.h()},h(){A(l,"name","hf:doc:metadata"),A(l,"content",rt),A(s,"align","center"),A(f,"class","mt-10")},m(t,e){nt(document.head,l),a(t,I,e),a(t,v,e),a(t,y,e),a(t,s,e),a(t,G,e),R(p,t,e),a(t,D,e),a(t,u,e),a(t,T,e),a(t,g,e),a(t,M,e),a(t,h,e),a(t,S,e),a(t,f,e),a(t,L,e),R(c,t,e),a(t,$,e),a(t,m,e),a(t,C,e),a(t,b,e),a(t,P,e),R(_,t,e),a(t,E,e),a(t,w,e),k=!0},p:X,i(t){k||(V(p.$$.fragment,t),V(c.$$.fragment,t),V(_.$$.fragment,t),k=!0)},o(t){q(p.$$.fragment,t),q(c.$$.fragment,t),q(_.$$.fragment,t),k=!1},d(t){t&&(i(I),i(v),i(y),i(s),i(G),i(D),i(u),i(T),i(g),i(M),i(h),i(S),i(f),i(L),i($),i(m),i(C),i(b),i(P),i(E),i(w)),i(l),z(p,t),z(c,t),z(_,t)}}}const rt='{"title":"ğŸ§¨ Diffusers","local":"-diffusers","sections":[{"title":"ğŸ§¨ Diffusers pipelines","local":"-diffusers-pipelines","sections":[],"depth":2}],"depth":1}';function lt(B){return tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ut extends et{constructor(l){super(),it(this,l,lt,dt,K,{})}}export{ut as component};
